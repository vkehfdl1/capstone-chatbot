question,answer,passage_id_1,passage_id_2,passage_id_3,passage_id_4,passage_id_5,passage_content_1,passage_content_2,passage_content_3,passage_content_4,passage_content_5,passage_scores_1,passage_scores_2,passage_scores_3,passage_scores_4,passage_scores_5,F1_score,Recall,Precision
What evaluation metric is used?,Sample Answer,1908.06083-Introduction-1,1908.06083-Build it Break it Fix it Method-0,1908.06083-Single-Turn Task ::: Data Collection ::: Task Formulation Details-0,1908.06083-Conclusion-0,1908.06083-5-Table5-1.png,"In this work, we study the detection of offensive language in dialogue with models that are robust to adversarial attack. We develop an automatic approach to the “Build it Break it Fix it” strategy originally adopted for writing secure programs BIBREF10, and the “Build it Break it” approach consequently adapting it for NLP BIBREF11. In the latter work, two teams of researchers, “builders” and “breakers” were used to first create sentiment and semantic role-labeling systems and then construct examples that find their faults. In this work we instead fully automate such an approach using crowdworkers as the humans-in-the-loop, and also apply a fixing stage where models are retrained to improve them. Finally, we repeat the whole build, break, and fix sequence over a number of iterations.","In order to train models that are robust to adversarial behavior, we posit that it is crucial collect and train on data that was collected in an adversarial manner. We propose the following automated build it, break it, fix it algorithm:","Since all of the collected examples are labeled as offensive, to make this task a binary classification problem, we will also add safe examples to it.","We have presented an approach to build more robust offensive language detection systems in the context of a dialogue. We proposed a build it, break it, fix it, and then repeat strategy, whereby humans attempt to break the models we built, and we use the broken examples to fix the models. We show this results in far more nuanced language than in existing datasets. The adversarial data includes less profanity, which existing classifiers can pick up on, and is instead offensive due to figurative language, negation, and by requiring more world knowledge, which all make current classifiers fail. Similarly, offensive language in the context of a dialogue is also more nuanced than stand-alone offensive utterances. We show that classifiers that learn from these more complex examples are indeed more robust to attack, and that using the dialogue context gives improved performance if the model architecture takes it into account.","Table 5: Dataset statistics for the single-turn rounds of the adversarial task data collection. There are three rounds in total all of identical size, hence the numbers above can be divided for individual statistics. The standard task is an additional dataset of exactly the same size as above.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What dataset do they use?,Sample Answer,1910.11471-Introduction-2,1910.11471-Problem Description ::: Programming Language Diversity-0,1910.11471-Problem Description ::: NLP of statements-0,1910.11471-Result Analysis-4,1910.11471-Conclusion & Future Works-0,An individual person expresses logical statements differently than other,"According to the sources, there are more than a thousand actively maintained programming languages, which signifies the diversity of these language . These languages were created to achieve different purpose and use different syntaxes. Low-level languages such as assembly languages are easier to express in human language because of the low or no abstraction at all whereas high-level, or Object-Oriented Programing (OOP) languages are more diversified in syntax and expression, which is challenging to bring into a unified human language structure. Nonetheless, portability and transparency between different programming languages also remains a challenge and an open research area. George D. et al. tried to overcome this problem through XML mapping BIBREF2. They tried to convert codes from C++ to Java using XML mapping as an intermediate language. However, the authors encountered challenges to support different features of both languages.","Although there is a finite set of expressions for each programming statements, it is a challenge to extract information from the statements of the code accurately. Semantic analysis of linguistic expression plays an important role in this information extraction. For instance, in case of a loop, what is the initial value? What is the step value? When will the loop terminate?","def __init__ ( self , regex ) :.","The main advantage of translating to a programming language is - it has a concrete and strict lexical and grammatical structure which human languages lack. The aim of this paper was to make the text-to-code framework work for general purpose programming language, primarily Python. In later phase, phrase-based word embedding can be incorporated for improved vocabulary mapping. To get more accurate target code for each line, Abstract Syntax Tree(AST) can be beneficial.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
By how much do they outperform existing state-of-the-art model on end-to-end Speech recognition?s ,Sample Answer,1912.03010-Related Work-1,1912.03010-Semantic Masking ::: Why Semantic Mask Works?-0,1912.03010-Model ::: ASR Training and Decoding-0,1912.03010-EXPERIMENT-0,1912.03010-Conclusion-0,"In terms of the model structure, the transformer-based E2E model has been investigated for both attention-based framework as well as RNN-T based models BIBREF7. Our model structure generally follows BIBREF8, with a minor difference that we used a deeper CNN before the self-attention blocks. We used a joint CTC/Attention loss to train our model following BIBREF6.","Spectrum augmentation BIBREF3 is similar to our method, since both propose to mask spectrum for E2E model training. However, the intuitions behind these two methods are different. SpecAugment randomly masks spectrum in order to add noise to the source input, making the E2E ASR problem harder and prevents the over-fitting problem in a large E2E model.","Following previous work BIBREF6, we employ a multi-task learning strategy to train the E2E model. Formally speaking, both the E2E model decoder and the CTC module predict the frame-wise distribution of $Y$ given corresponding source $X$, denoted as $P_{s2s}(\mathbf {Y}|\mathbf {X})$ and $P_{ctc}(\mathbf {Y}|\mathbf {X})$. We weighted averaged two negative log likelihoods to train our model","In this section, we describe our experiments on LibriSpeech BIBREF1 and TedLium2 BIBREF13. We compare our results with state-of-the-art hybrid and E2E systems. We implemented our approach based on ESPnet BIBREF6, and the specific settings on two datasets are the same with BIBREF6, except the decoding setting. We use the beam size 20, $\beta _1 = 0.5$, and $\beta _2=0.7$ in our experiment.","This paper presents a semantic mask method for E2E speech recognition, which is able to train a model to better consider the whole audio context for the disambiguation. Moreover, we elaborate a new architecture for E2E model, achieving state-of-the-art performance on the Librispeech test set in the scope of E2E models.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
By how much does their method outperform state-of-the-art OOD detection?,Sample Answer,1905.10247-METHODS-0,1905.10247-HCN-0,1905.10247-AE-HCN-0,1905.10247-AE-HCN-1,1905.10247-AE-HCN-CNN-0,"In this section, we first present the standard HCN model. Then we introduce the proposed AE-HCN(-CNN) model, consisting of an autoencoder and a reconstruction score-aware HCN model. Finally, we describe the counterfeit data augmentation method for training the proposed model.","As shown in Figure FIGREF8 , HCN considers a dialog as a sequence of turns. At each turn, HCN takes a tuple, INLINEFORM0 , as input to produce the next system action INLINEFORM1 , where INLINEFORM2 is a user utterance consisting of INLINEFORM3 tokens, i.e., INLINEFORM4 , INLINEFORM5 a one-hot vector encoding the previous system action and INLINEFORM6 a contextual feature vector generated by domain-specific code. The user utterance is encoded as a concatenation of a bag-of-words representation and an average of word embeddings of the user utterance: DISPLAYFORM0 ","On top of HCN, AE-HCN additionally takes as input an autoencoder's reconstruction score INLINEFORM0 for the user utterance for dialog state update (Figure FIGREF8 ): DISPLAYFORM0 ","The autoencoder is a standard seq2seq model which projects a user utterance into a latent vector and reconstructs the user utterance. Specifically, the encoder reads INLINEFORM0 using a GRU BIBREF7 to produce a 512-dimensional hidden vector INLINEFORM1 which in turn gets linearly projected to a 200-dimensional latent vector INLINEFORM2 : DISPLAYFORM0 DISPLAYFORM1 ",AE-HCN-CNN is a variant of AE-HCN where user utterances are encoded using a CNN layer with max-pooling (following BIBREF8 ) rather than equation EQREF5 : DISPLAYFORM0 ,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Is the baseline a non-heirarchical model like BERT?,Sample Answer,1905.06566-Pre-training-0,1905.06566-Experiments-0,1905.06566-Evaluations-0,1905.06566-Results-2,1905.06566-3-Figure1-1.png,"Most recent encoding neural models used in NLP (e.g., RNNs, CNNs or Transformers) can be pre-trained by predicting a word in a sentence (or a text span) using other words within the same sentence (or span). For example, ELMo BIBREF12 and OpenAI-GPT BIBREF13 predict a word using all words on its left (or right); while word2vec BIBREF33 predicts one word with its surrounding words in a fixed window and BERT BIBREF0 predicts (masked) missing words in a sentence given all the other words.",In this section we assess the performance of our model on the document summarization task. We first introduce the dataset we used for pre-training and the summarization task and give implementation details of our model. We also compare our model against multiple previous models.,"We evaluated the quality of summaries from different systems automatically using ROUGE BIBREF39 . We reported the full length F1 based ROUGE-1, ROUGE-2 and ROUGE-L on the CNNDM and NYT50 datasets. We compute ROUGE scores using the ROUGE-1.5.5.pl script.","As mentioned earlier, our pre-training includes two stages. The first stage is the open-domain pre-training stage on the GIGA-CM dataset and the following stage is the in-domain pre-training on the CNNDM (or NYT50) dataset. As shown in Table 3 , we pretrained $\text{\sc Hibert}_S$ using only open-domain stage (Open-Domain), only in-domain stage (In-Domain) or both stages (Open+In-Domain) and applied it to the CNNDM summarization task. Results on the validation set of CNNDM indicate the two-stage pre-training process is necessary.","Figure 1: The architecture of HIBERT during training. senti is a sentence in the document above, which has four sentences in total. sent3 is masked during encoding and the decoder predicts the original sent3.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What TIMIT datasets are used for testing?,Sample Answer,1910.07601-Related Works ::: Acoustic Embeddings-0,1910.07601-Model Architecture ::: Variational Auto-Encoders-5,1910.07601-Experimental Framework-0,1910.07601-Experimental Framework ::: Data and Use-1,1910.07601-Experimental Framework ::: Implementation-1,"Most interest in acoustic embeddings can be observed on acoustic word embeddings, i.e. projections that map word acoustics into a fixed size vector space. Objective functions are chosen to project different word realisations to close proximity in the embedding space. Different approaches were used in the literature - for both supervised and unsupervised learning. For the supervised case, BIBREF9 introduced a convolutional neural network (CNN) based acoustic word embedding system for speech recognition, where words that sound alike are nearby in Euclidean distance. In their work, a CNN is used to predict a word from the corresponding acoustic signal, the output of the bottleneck layer before the final softmax layer is taken to be the embedding for the corresponding word. Further work used different network architectures to obtain acoustic word embeddings: BIBREF10 introduces a recurrent neural network (RNN) based approach instead.",,,,,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
By how much do they improve upon supervised traning methods?,Sample Answer,1709.06136-Dialog Agent-2,1709.06136-User Simulator-1,1709.06136-User Simulator-2,1709.06136-Deep RL Policy Optimization-2,1709.06136-4-Figure2-1.png,"Action Modeling We use dialog acts as system actions, which can be seen as shallow representations of the utterance semantics. We treat system action modeling as a multi-class classification problem, where the agent select an appropriate action from a predefined list of system actions based on current dialog state INLINEFORM0 : DISPLAYFORM0 ","User Goal We define a user's goal INLINEFORM0 using a list of informable and requestable slots BIBREF38 . Informable slots are the slots that users can provide a value for to describe their goal (e.g. slots for food type, area, etc.). Requestable slots are the slots that users want to request the value for, such as requesting the address of a restaurant. We treat informable slots as discrete type of inputs that can take multiple values, and treat requestable slots as inputs that take binary values (i.e. a slot is either requested or not). In this work, once the a goal is sampled at the beginning of the conversation, we fix the user goal and do not change it during the conversation.","Action Selection Similar to the action modeling in dialog agent, we treat user action modeling as a multi-class classification problem conditioning on the dialog context encoded in the dialog-level LSTM state INLINEFORM0 on the user simulator side: DISPLAYFORM0 ","Action Actions of the dialog agent and user simulator are the system action outputs INLINEFORM0 and INLINEFORM1 . An action is sampled by the agent based on a stochastic representation of the policy, which produces a probability distribution over actions given a dialog state. Action space is finite and discrete for both the dialog agent and the user simulator.",Fig. 2. User simulator network architecture.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Do they train a different training method except from scheduled sampling?,Sample Answer,1812.07023-Introduction-1,1812.07023-Introduction-4,1812.07023-Models-0,1812.07023-Decoders-1,1812.07023-Conclusions-0,"Given an image and a question related to the image, the vqa challenge BIBREF5 tasked users with selecting an answer to the question. BIBREF6 identified several sources of bias in the vqa dataset, which led to deep neural models answering several questions superficially. They found that in several instances, deep architectures exploited the statistics of the dataset to select answers ignoring the provided image. This prompted the release of vqa 2.0 BIBREF7 which attempts to balance the original dataset. In it, each question is paired to two similar images which have different answers. Due to the complexity of vqa, understanding the failures of deep neural architectures for this task has been a challenge. It is not easy to interpret whether the system failed in understanding the question or in understanding the image or in reasoning over it. The CLEVR dataset BIBREF8 was hence proposed as a useful benchmark to evaluate such systems on the task of visual reasoning. Extending question answering over images to videos, BIBREF9 have proposed MovieQA, where the task is to select the correct answer to a provided question given the movie clip on which it is based.","In Section SECREF2 , we discuss existing literature on end-to-end dialogue systems with a special focus on multi-modal dialogue systems. Section SECREF3 describes the avsd dataset. In Section SECREF4 , we present the architecture of our modelname model. We describe our evaluation and experimental setup in Section SECREF5 and then conclude in Section SECREF6 .","Our modelname model is based on the hred framework for modelling dialogue systems. In our model, an utterance-level recurrent lstm encoder encodes utterances and a dialogue-level recurrent lstm encoder encodes the final hidden states of the utterance-level encoders, thus maintaining the dialogue state and dialogue coherence. We use the final hidden states of the utterance-level encoders in the attention mechanism that is applied to the outputs of the description, video, and audio encoders. The attended features from these encoders are fused with the dialogue-level encoder's hidden states. An utterance-level decoder decodes the response for each such dialogue state following a question. We also add an auxiliary decoding module which is similar to the response decoder except that it tries to generate the caption and/or the summary of the video. We present our model in Figure FIGREF2 and describe the individual components in detail below.",The auxiliary decoder is functionally similar to the answer decoder. The decoded sentence is the caption and/or description of the video. We use the Video Encoder state instead of the Dialogue-level Encoder state as input since with this module we want to learn a better video representation capable of decoding the description.,"We presented modelname, a state-of-the-art dialogue model for conversations about videos. We evaluated the model on the official AVSD test set, where it achieves a relative improvement of more than 16% over the baseline model on BLEU-4 and more than 33% on CIDEr. The challenging aspect of multi-modal dialogue is fusing modalities with varying information density. On AVSD, it is easiest to learn from the input text, while video features remain largely opaque to the decoder. modelname uses a generalization of FiLM to video that conditions video feature extraction on a question. However, similar to related work, absolute improvements of incorporating video features into dialogue are consistent but small. Thus, while our results indicate the suitability of our FiLM generalization, they also highlight that applications at the intersection between language and video are currently constrained by the quality of video features, and emphasizes the need for larger datasets.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What was the baseline for this task?,Sample Answer,1910.09982-Introduction-0,1910.09982-Introduction-1,1910.09982-Propaganda Techniques-0,1910.09982-Propaganda Techniques ::: 2. Name calling or labeling.-0,"1910.09982-Propaganda Techniques ::: 17. Obfuscation, intentional vagueness, confusion.-0","Propaganda aims at influencing people's mindset with the purpose of advancing a specific agenda. In the Internet era, thanks to the mechanism of sharing in social networks, propaganda campaigns have the potential of reaching very large audiences BIBREF0, BIBREF1, BIBREF2.","Propagandist news articles use specific techniques to convey their message, such as whataboutism, red Herring, and name calling, among many others (cf. Section SECREF3). Whereas proving intent is not easy, we can analyse the language of a claim/article and look for the use of specific propaganda techniques. Going at this fine-grained level can yield more reliable systems and it also makes it possible to explain to the user why an article was judged as propagandist by an automatic system.","Propaganda uses psychological and rhetorical techniques to achieve its objective. Such techniques include the use of logical fallacies and appeal to emotions. For the shared task, we use 18 techniques that can be found in news articles and can be judged intrinsically, without the need to retrieve supporting information from external resources. We refer the reader to BIBREF10 for more details on the propaganda techniques; below we report the list of techniques:","Labeling the object of the propaganda as something the target audience fears, hates, finds undesirable or otherwise loves or praises BIBREF12.","Using deliberately unclear words, to let the audience have its own interpretation BIBREF21, BIBREF11. For instance, when an unclear phrase with multiple possible meanings is used within the argument and, therefore, it does not really support the conclusion.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Are all the tables in the dataset from the same website?,Sample Answer,1706.02427-Matching with Designed Features-6,1706.02427-Matching with Neural Networks-8,1706.02427-Results on WebQueryTable-4,1706.02427-Results on WikiTableQuestions-3,1706.02427-Related Work-3,"(3) Sentence Level. We design features to match a query with a table at the sentence level. We use CDSSM BIBREF6 , which has been successfully applied in text retrieval. The basic computational component of CDSSM is sub-word, which makes it very suitable for dealing the misspelling queries in web search. The model composes sentence vector from sub-word embedding via convolutional neural network. We use the same model architecture to get query vector and table aspect vector, and calculate their relevance with cosine function. ","Since headers and cells have similar characteristics, we use a similar way to measure the relevance between a query and table cells. Specifically, we derive three memories $M_{cel}$ , $M_{row}$ and $M_{col}$ from table cells in order to match from cell level, row level and column level. Each memory cell in $M_{cel}$ represents the embedding of a table cell. Each cell in $M_{row}$ represent the vector a row, which is computed with weighted average over the embeddings of cells in the same row. We derive the column memory $M_{col}$ in an analogous way. We use the same module $NN_1()$ to calculate the relevance scores for these three memories. ","We conduct case study on our NeuralNet approach and find that the performance is sensitive to the length of queries. Therefore, we split the test set to several groups according to the length of queries. Results are given in Figure 4 . We can find that the performance of the approach decreases with the increase of query length. When the query length changes from 6 to 7, the performance of P@1 decreases rapidly from 58.12% to 50.23%. Through doing case study, we find that long queries contain more word dependencies. Therefore, having a good understanding about the intention of a query requires deep query understanding. Leveraging external knowledge to connect query and table is a potential solution to deal with long queries.","We can find that the effects of different aspect in designed features and neural networks are consistent. Using more aspects could achieve better performance. Using all aspects obtains the best performance. We also find that the most effective aspect for WikiTableQuestions is header. This is different from the phenomenon in WebQueryTable that the most effective aspect is caption. We believe that this is because the questions in WikiTableQuestions typically include content constrains from cells or headers. Two randomly sampled questions are “which country won the 1994 europeans men's handball championship's preliminary round?"" and “what party had 7,115 inactive voters as of october 25, 2005?"". On the contrary, queries from WebTableQuery usually do not use information from specific headers or cells. Examples include “polish rivers"", “world top 5 mountains"" and “list of american cruise lines"". From Table 1 , we can also find that the question in WikiTableQuestions are longer than the queries in WebQueryTable. In addition, we observe that not all the questions from WikiTableQuestions are suitable for table retrieval. An example is “what was the first player to be drafted in this table?"".","Our neural network approach relates to the recent advances of attention mechanism and reasoning over external memory in artificial intelligence BIBREF11 , BIBREF12 , BIBREF19 . Researchers typically represent a memory as a continuous vector or matrix, and develop neural network based controller, reader and writer to reason over the memory. The memory could be addressed by a “soft” attention mechanism trainable by standard back-propagation methods or a “hard” attention mechanism trainable by REINFORCE BIBREF20 . In this work, we use the soft attention mechanism, which could be easily optimized and has been successfully applied in nlp tasks BIBREF11 , BIBREF12 .",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How do they measure correlation between the prediction and explanation quality?,Sample Answer,1708.01776-Introduction-0,1708.01776-The QRAQ Dataset-0,1708.01776-The QRAQ Dataset-1,1708.01776-The QRAQ Dataset-2,1708.01776-The Dataset-2,"In recent years deep neural network models have been successfully applied in a variety of applications such as machine translation BIBREF0 , object recognition BIBREF1 , BIBREF2 , game playing BIBREF3 , dialog BIBREF4 and more. However, their lack of interpretability makes them a less attractive choice when stakeholders must be able to understand and validate the inference process. Examples include medical diagnosis, business decision-making and reasoning, legal and safety compliance, etc. This opacity also presents a challenge simply for debugging and improving model performance. For neural systems to move into realms where more transparent, symbolic models are currently employed, we must find mechanisms to ground neural computation in meaningful human concepts, inferences, and explanations. One approach to this problem is to treat the explanation problem itself as a learning problem and train a network to explain the results of a neural computation. This can be done either with a single network learning jointly to explain its own predictions or with separate networks for prediction and explanation. Regardless, the availability of sufficient labelled training data is a key impediment. In previous work BIBREF5 we developed a synthetic conversational reasoning dataset in which the User presents the Agent with a simple, ambiguous story and a challenge question about that story. Ambiguities arise because some of the entities in the story have been replaced by variables, some of which may need to be known to answer the challenge question. A successful Agent must reason about what the answers might be, given the ambiguity, and, if there is more than one possible answer, ask for the value of a relevant variable to reduce the possible answer set. In this paper we present a new dataset e-QRAQ constructed by augmenting the QRAQ simulator with the ability to provide detailed explanations about whether the Agent's response was correct and why. Using this dataset we perform some preliminary experiments, training an extended End-to-End Memory Network architecture BIBREF6 to jointly predict a response and a partial explanation of its reasoning. We consider two types of partial explanation in these experiments: the set of relevant variables, which the Agent must know to ask a relevant, reasoned question; and the set of possible answers, which the Agent must know to answer correctly. We demonstrate a strong correlation between the qualities of the prediction and explanation.","A QRAQ domain, as introduced in BIBREF5 , has two actors, the User and the Agent. The User provides a short story set in a domain similar to the HomeWorld domain of BIBREF24 , BIBREF27 given as an initial context followed by a sequence of events, in temporal order, and a challenge question. The stories are semantically coherent but may contain hidden, sometimes ambiguous, entity references, which the Agent must potentially resolve to answer the question.","To do so, the Agent can query the User for the value of variables which hide the identity of entities in the story. At each point in the interaction, the Agent must determine whether it knows the answer, and if so, provide it; otherwise it must determine a variable to query which will reduce the potential answer set (a “relevant” variable).","In example SECREF1 the actors $v, $w, $x and $y are treated as variables whose value is unknown to the Agent. In the first event, for example, $v refers to either Hannah or Emma, but the Agent can't tell which. In a realistic text this entity obfuscation might occur due to spelling or transcription errors, unknown descriptive references such as “Emma's sibling”, or indefinite pronouns such as “somebody”. Several datasets with 100k problems each and of varying difficulty have been released to the research community and are available for download BIBREF28 .","In Example UID13 , illustrating a successful interaction, the Agent asks for the value of $V0 and the User responds with the answer (Silvia) as well as an explanation indicating that it was correct (helpful) and why. Specifically, in this instance it was helpful because it enabled an inference which reduced the possible answer set (and reduced the set of relevant variables). On the other hand, in Example UID30 , we see an example of a bad query and corresponding critical explanation.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How is morphology knowledge implemented in the method?,Sample Answer,2001.01589-Introduction-1,2001.01589-Introduction-3,2001.01589-Introduction-4,2001.01589-Approach ::: Morpheme Segmentation ::: Stem with Combined Suffix-0,2001.01589-2-Table1-1.png,"Both the Turkish and Uyghur are agglutinative and highly-inflected languages in which the word is formed by suffixes attaching to a stem BIBREF4. The word consists of smaller morpheme units without any splitter between them and its structure can be denoted as “stem + suffix1 + suffix2 + ... + suffixN”. A stem is attached in the rear by zero to many suffixes that have many inflected and morphological variants depending on case, number, gender, and so on. The complex morpheme structure and relatively free constituent order can produce very large vocabulary because of the derivational morphology, so when translating from the agglutinative languages, many words are unseen at training time. Moreover, due to the semantic context, the same word generally has different segmentation forms in the training corpus.",Stem with combined suffix,Stem with singular suffix,"In this segmentation strategy, each word is segmented into a stem unit and a combined suffix unit. We add “##” behind the stem unit and add “$$” behind the combined suffix unit. We denote this method as SCS. The segmented word can be denoted as two parts of “stem##” and “suffix1suffix2...suffixN$$”. If the original word has no suffix unit, the word is treated as its stem unit. All the following segmentation strategies will follow this rule.",Table 1: The sentence examples with different segmentation strategies for Turkish-English.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the sign language recognition task investigated?,Sample Answer,1909.11232-Our Approach ::: Recurrent Neural Networks (RNN)-0,1909.11232-Our Approach ::: 3D Convolutional Neural Network-0,1909.11232-Experiments ::: Effect of Same Subject Data in Training-1,1909.11232-5-Figure6-1.png,1909.11232-7-Figure10-1.png,"RNN has shown success in modeling sequential pattern in dataBIBREF6. It can capture temporal dynamics in data by maintaining an internal state. However, the basic RNN has problems dealing with long term dependencies in data due to the vanishing gradient problem BIBREF21. Some solutions to the vanishing gradient problem involve careful initialization of network parameters or early stopping BIBREF22. But the most effective solution is to modify the RNN architecture in such a way that there exists a memory state (cell state) at every time step that can identify what to remember and what to forget. This architecture is referred to as long short term memory (LSTM) network BIBREF23. While the basic RNN is a direct transformation of the previous state and the current input, the LSTM maintains an internal memory and has mechanisms to update and use that memory. This is achieved by deploying four separate neural networks also called gates. Figure FIGREF12 depicts a cell of an LSTM network which shows input at the current time step ${x_t}$ and the previous state ${h_{t-1}}$ enter into the cell; and get concatenated. The forget gate processes it to remove unnecessary information, and outputs ${f_t}$ which gets multiplied with the previously stored memory ${C_{t-1}}$ and produces a refined memory for the current time.","Traditional convolutional neural network (CNN) is two dimensional in which each layer has a stack of 2D feature maps generated from previous layer or from inputs in case of first layer. A layer also has a certain numbers of filters which are rectangular patches of parameters. Each filter convolves over the stack of 2D feature maps at previous layer and produces feature maps (equal to the number of filters in the current layer) at current layer. The operation is given by Equation DISPLAY_FORM17 where $F_{i,j}^{l}$ denotes the value of feature map at $l^{th}$ layer at location $(i,j)$. $\odot $ represents dot product of filter $W$ and associated feature map patch in previous layer.","Figure FIGREF32 shows the effect of added training data from test subjects in the retraining on six subjects from our dataset in case of Spatial AI-LSTM model. We see that, adding data from a test subject increase recognition accuracy for all of the subjects shown. It is interesting to observe that adding even $10\%$ of data from a test subject gives significant improvement in recognition accuracy (close to $95\%$) for almost all of the subjects shown.",Fig. 6. Proposed architectures. Fig (a): Axis independent LSTM network where data from each axis enters into different LSTM networks and at the end we take the concatenation of individual states. Fig (b): Combined architecture. Here 3D CNN symbolizes the architecture we presented in Figure 5. Here both CNN and LSTM network model data separately. At the end we take the maximum of probability scores produced by both network.,Fig. 10. Effect of adding data to the training from test subject in Spatial AI-LSTM model. X axis is the fraction of test subject’s data used in training. Y axis is the test accuracy.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the performance of the best model in the sign language recognition task?,Sample Answer,1909.11232-Our Approach ::: 3D Convolutional Neural Network-0,1909.11232-Experiments ::: Effect of Same Subject Data in Training-0,1909.11232-Experiments ::: Effect of Same Subject Data in Training-1,1909.11232-5-Figure6-1.png,1909.11232-7-Figure10-1.png,"Traditional convolutional neural network (CNN) is two dimensional in which each layer has a stack of 2D feature maps generated from previous layer or from inputs in case of first layer. A layer also has a certain numbers of filters which are rectangular patches of parameters. Each filter convolves over the stack of 2D feature maps at previous layer and produces feature maps (equal to the number of filters in the current layer) at current layer. The operation is given by Equation DISPLAY_FORM17 where $F_{i,j}^{l}$ denotes the value of feature map at $l^{th}$ layer at location $(i,j)$. $\odot $ represents dot product of filter $W$ and associated feature map patch in previous layer.","In addition to having the cross subject accuracy described in section SECREF29, we also want to know the impact of adding a test subject's data to the training process. It is obvious that adding test subject's data to the training must increase the accuracy of the network for the subject. However, we want to know how much or what fraction of data is necessary for significant improvement in performance. This is important for assessing the practial usability of a recognition system. In other words, we want to know how quickly or with what amount of data, the current system can be adapted for a subject completely unknown to the system. To do that, we first pick a test subject and train a model for the test subject with data from all other subjects in our dataset. Then we retrain the model with some fraction of data from the test subject. We keep increasing the fraction of data being used from the test subject in the retraining process up to $50\%$. The other half of the test subject's data is used for testing the model.","Figure FIGREF32 shows the effect of added training data from test subjects in the retraining on six subjects from our dataset in case of Spatial AI-LSTM model. We see that, adding data from a test subject increase recognition accuracy for all of the subjects shown. It is interesting to observe that adding even $10\%$ of data from a test subject gives significant improvement in recognition accuracy (close to $95\%$) for almost all of the subjects shown.",Fig. 6. Proposed architectures. Fig (a): Axis independent LSTM network where data from each axis enters into different LSTM networks and at the end we take the concatenation of individual states. Fig (b): Combined architecture. Here 3D CNN symbolizes the architecture we presented in Figure 5. Here both CNN and LSTM network model data separately. At the end we take the maximum of probability scores produced by both network.,Fig. 10. Effect of adding data to the training from test subject in Spatial AI-LSTM model. X axis is the fraction of test subject’s data used in training. Y axis is the test accuracy.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the deep learning architectures used?,Sample Answer,1909.11232-Dataset-0,1909.11232-Dataset ::: Collection Protocol-1,1909.11232-Experiments ::: Effect of Same Subject Data in Training-0,1909.11232-3-Figure3-1.png,1909.11232-7-Figure10-1.png,"ASL recognition with skeletal data has received little attention, resulting in a scarcity of public datasets. There exists one dataset for ASL recognition with skeletal data BIBREF19. This dataset has 9800 samples from 6 subjects and more than 3300 sign classes. The number of samples per class was small for use in deep learning based models. Adding to this, the samples were collected in controlled settings with uncluttered background. In contrast, GMU-ASL51 has 13107 samples for 51 word level classes from 12 distinct subjects of different height, build and signing (using sign language) experience.","To gather individual samples from the continuous data, segmentation marks were interleaved through a user interface. This was later used to segment individual samples. These samples were further segmented using motion calculation of the wrist joint co-ordinates from skeletal data. Figure FIGREF7 (a) illustrates the distribution of number of samples per gesture class in GMU-ASL51 dataset. Figure FIGREF7 (b) shows the distribution of duration of videos in our dataset.","In addition to having the cross subject accuracy described in section SECREF29, we also want to know the impact of adding a test subject's data to the training process. It is obvious that adding test subject's data to the training must increase the accuracy of the network for the subject. However, we want to know how much or what fraction of data is necessary for significant improvement in performance. This is important for assessing the practial usability of a recognition system. In other words, we want to know how quickly or with what amount of data, the current system can be adapted for a subject completely unknown to the system. To do that, we first pick a test subject and train a model for the test subject with data from all other subjects in our dataset. Then we retrain the model with some fraction of data from the test subject. We keep increasing the fraction of data being used from the test subject in the retraining process up to $50\%$. The other half of the test subject's data is used for testing the model.","Fig. 3. Visualization of hand shapes and skeletal joints of two sign classes. Top panel shows the sign Alarm and middle panel shows the sign Doorbell. For each sign, first two rows are the left and right hand image patches and third row is the skeletal configuration. We can see for Alarm and Doorbell, the skeletal motion is almost similar but has different hand shapes. Bottom panel shows another sign Weather which has quite distinguishable skeletal motion from top two.",Fig. 10. Effect of adding data to the training from test subject in Spatial AI-LSTM model. X axis is the fraction of test subject’s data used in training. Y axis is the test accuracy.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How big is their model?,Sample Answer,1902.09314-Introduction-0,1902.09314-Introduction-4,1902.09314-Embedding Layer-1,1902.09314-Main Results-2,1902.09314-Conclusion-0,"Targeted sentiment classification is a fine-grained sentiment analysis task, which aims at determining the sentiment polarities (e.g., negative, neutral, or positive) of a sentence over “opinion targets” that explicitly appear in the sentence. For example, given a sentence “I hated their service, but their food was great”, the sentiment polarities for the target “service” and “food” are negative and positive respectively. A target is usually an entity or an entity aspect.","This paper propose an attention based model to solve the problems above. Specifically, our model eschews recurrence and employs attention as a competitive alternative to draw the introspective and interactive semantics between target and context words. To deal with the label unreliability issue, we employ a label smoothing regularization to encourage the model to be less confident with fuzzy labels. We also apply pre-trained BERT BIBREF8 to this task and show our model enhances the performance of basic BERT model. Experimental results on three benchmark datasets show that the proposed model achieves competitive performance and is a lightweight alternative of the best RNN based models.","BERT embedding uses the pre-trained BERT to generate word vectors of sequence. In order to facilitate the training and fine-tuning of BERT model, we transform the given context and target to “[CLS] + context + [SEP]” and “[CLS] + target + [SEP]” respectively.","Feature-based SVM is still a competitive baseline, but relying on manually-designed features. Rec-NN gets the worst performances among all neural network baselines as dependency parsing is not guaranteed to work well on ungrammatical short texts such as tweets and comments. Like AEN, MemNet also eschews recurrence, but its overall performance is not good since it does not model the hidden semantic of embeddings, and the result of the last attention is essentially a linear combination of word embeddings.","In this work, we propose an attentional encoder network for the targeted sentiment classification task. which employs attention based encoders for the modeling between context and target. We raise the the label unreliability issue add a label smoothing regularization to encourage the model to be less confident with fuzzy labels. We also apply pre-trained BERT to this task and obtain new state-of-the-art results. Experiments and analysis demonstrate the effectiveness and lightweight of the proposed model.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the three steps to feature elimination?,Sample Answer,1701.08229-Introduction-0,1701.08229-Features-8,1701.08229-Feature Elimination-3,1701.08229-Feature Elimination-7,1701.08229-Future Work-0,"In recent years, there has been a movement to leverage social medial data to detect, estimate, and track the change in prevalence of disease. For example, eating disorders in Spanish language Twitter tweets BIBREF0 and influenza surveillance BIBREF1 . More recently, social media has been leveraged to monitor social risks such as prescription drug and smoking behaviors BIBREF2 , BIBREF3 , BIBREF4 as well as a variety of mental health disorders including suicidal ideation BIBREF5 , attention deficient hyperactivity disorder BIBREF6 and major depressive disorder BIBREF7 . In the case of major depressive disorder, recent efforts range from characterizing linguistic phenomena associated with depression BIBREF8 and its subtypes e.g., postpartum depression BIBREF5 , to identifying specific depressive symptoms BIBREF9 , BIBREF10 e.g., depressed mood. However, more research is needed to better understand the predictive power of supervised machine learning classifiers and the influence of feature groups and feature sets for efficiently classifying depression-related tweets to support mental health monitoring at the population-level BIBREF11 .","A more detailed description of leveraged features and their values, including LIWC categories, can be found in BIBREF10 .",Rank We cumulatively plotted the average F1-score performances of each incrementally added percentile of top ranked features. We report the percentile and count of features resulting in the first occurrence of the highest average F1-score for each class.,"We observed peak F1-score performances at low percentiles for fatigue or loss of energy (5th percentile), disturbed sleep (10th percentile) as well as depressive symptoms and no evidence of depression (both 15th percentile) suggesting fewer features are needed to reach optimal performance. In contrast, peak F1-score performances occurred at moderate percentiles for evidence of depression (30th percentile) and depressed mood (55th percentile) suggesting that more features are needed to reach optimal performance. However, one notable difference between these two classes is the dramatic F1-score improvements for depressed mood i.e., 20 point increase from the 1st percentile to the 20th percentile compared to the more gradual F1-score improvements for evidence of depression i.e., 11 point increase from the 1st percentile to the 20th percentile. This finding suggests that for identifying depressed mood a variety of features are needed before incremental gains are observed.","Our next step is to address the classification of rarer depressive symptoms suggestive of major depressive disorder from our dataset and hierarchy including inappropriate guilt, difficulty concentrating, psychomotor agitation or retardation, weight loss or gain, and anhedonia BIBREF15 , BIBREF16 . We are developing a population-level monitoring framework designed to estimate the prevalence of depression (and depression-related symptoms and psycho-social stressors) over millions of United States-geocoded tweets. Identifying the most discriminating feature sets and natural language processing classifiers for each depression symptom is vital for this goal.",1.0,1.0,1.0,1.0,1.0,0.22222222222222224,0.25,0.2
How is the dataset annotated?,Sample Answer,1701.08229-Feature Contribution-2,1701.08229-Feature Elimination-3,1701.08229-Feature Elimination-7,1701.08229-Future Work-0,1701.08229-Conclusions-0,"Unsurprisingly, lexical features (unigrams) were the largest contributor to feature counts in the dataset. We observed that lexical features are also critical for identifying depressive symptoms, specifically for depressed mood and for disturbed sleep. For the classes higher in the hierarchy - no evidence of depression, evidence of depression, and depressive symptoms - the classifier produced consistent F1-scores, even slightly above the baseline for depressive symptoms and minor fluctuations of change in recall and precision when removing other feature groups suggesting that the contribution of non-lexical features to classification performance was limited. However, notable changes in F1-score were observed for the classes lower in the hierarchy including disturbed sleep and fatigue or loss of energy. For instance, changes in F1-scores driven by both recall and precision were observed for disturbed sleep by ablating demographics, emotion, and sentiment features, suggesting that age or gender (“mid-semester exams have me restless”), polarity and subjective terms (“lack of sleep is killing me”), and emoticons (“wide awake :(”) could be important for both identifying and correctly classifying a subset of these tweets.",Rank We cumulatively plotted the average F1-score performances of each incrementally added percentile of top ranked features. We report the percentile and count of features resulting in the first occurrence of the highest average F1-score for each class.,"We observed peak F1-score performances at low percentiles for fatigue or loss of energy (5th percentile), disturbed sleep (10th percentile) as well as depressive symptoms and no evidence of depression (both 15th percentile) suggesting fewer features are needed to reach optimal performance. In contrast, peak F1-score performances occurred at moderate percentiles for evidence of depression (30th percentile) and depressed mood (55th percentile) suggesting that more features are needed to reach optimal performance. However, one notable difference between these two classes is the dramatic F1-score improvements for depressed mood i.e., 20 point increase from the 1st percentile to the 20th percentile compared to the more gradual F1-score improvements for evidence of depression i.e., 11 point increase from the 1st percentile to the 20th percentile. This finding suggests that for identifying depressed mood a variety of features are needed before incremental gains are observed.","Our next step is to address the classification of rarer depressive symptoms suggestive of major depressive disorder from our dataset and hierarchy including inappropriate guilt, difficulty concentrating, psychomotor agitation or retardation, weight loss or gain, and anhedonia BIBREF15 , BIBREF16 . We are developing a population-level monitoring framework designed to estimate the prevalence of depression (and depression-related symptoms and psycho-social stressors) over millions of United States-geocoded tweets. Identifying the most discriminating feature sets and natural language processing classifiers for each depression symptom is vital for this goal.","In summary, we conducted two feature study experiments to assess the contribution of feature groups and to determine the optimal percentile of top ranked features for classifying Twitter tweets in the depression schema hierarchy. From these experiments, we conclude that simple lexical features and reduced feature sets can produce comparable results to the much larger feature dataset.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How do they obtain human judgements?,Sample Answer,1904.08386-Introduction-1,1904.08386-Clustering city representations-0,1904.08386-Quantitative comparison-1,1904.08386-Related work-0,1904.08386-1-Figure1-1.png,"Framed as a dialogue between the traveler Marco Polo and the emperor Kublai Khan, Invisible Cities consists of 55 prose poems, each of which describes an imaginary city. Calvino categorizes these cities into eleven thematic groups that deal with human emotions (e.g., desires, memories), general objects (eyes, sky, signs), and unusual properties (continuous, hidden, thin). Many critics argue that Calvino's labels are not meaningful, while others believe that there is a distinct thematic separation between the groups, including the author himself BIBREF4 . The unique structure of this novel — each city's description is short and self-contained (Figure FIGREF1 ) — allows us to computationally examine this debate.","Given 55 city representations, how do we group them into eleven clusters of five cities each? Initially, we experimented with a graph-based community detection algorithm that maximizes cluster modularity BIBREF20 , but we found no simple way to constrain this method to produce a specific number of equally-sized clusters. The brute force approach of enumerating all possible cluster assignments is intractable given the large search space ( INLINEFORM0 possible assignments). We devise a simple clustering algorithm to approximate this process. First, we initialize with random cluster assignments and define “cluster strength” to be the relative difference between “intra-group” Euclidean distance and “inter-group” Euclidean distance. Then, we iteratively propose random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence. To evaluate the quality of the computationally-derived clusters against those of Calvino, we measure cluster purity BIBREF21 : given a set of predicted clusters INLINEFORM1 and ground-truth clusters INLINEFORM2 that both partition a set of INLINEFORM3 data points, INLINEFORM4 ","City representations computed using language model-based representation (ELMo and BERT) achieve significantly higher purity than a clustering induced from random representations, indicating that there is at least some meaningful coherence to Calvino's thematic groups (first row of Table TABREF11 ). ELMo representations yield the highest purity among the three methods, which is surprising as BERT is a bigger model trained on data from books (among other domains). Both ELMo and BERT outperform GloVe, which intuitively makes sense because the latter do not model the order or structure of the words in each description.","Most previous work within the NLP community applies distant reading BIBREF1 to large collections of books, focusing on modeling different aspects of narratives such as plots and event sequences BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 , characters BIBREF2 , BIBREF26 , BIBREF27 , BIBREF28 , and narrative similarity BIBREF3 . In the same vein, researchers in computational literary analysis have combined statistical techniques and linguistics theories to perform quantitative analysis on large narrative texts BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , but these attempts largely rely on techniques such as word counting, topic modeling, and naive Bayes classifiers and are therefore not able to capture the meaning of sentences or paragraphs BIBREF34 . While these works discover general patterns from multiple literary works, we are the first to use cutting-edge NLP techniques to engage with specific literary criticism about a single narrative.","Figure 1: Calvino labels the thematically-similar cities in the top row as cities & the dead. However, although the bottom two cities share a theme of desire, he assigns them to different groups.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
On how many language pairs do they show that preordering assisting language sentences helps translation quality?,Sample Answer,1811.00383-Addressing Lexical Divergence-0,1811.00383-Languages-0,1811.00383-Network-1,1811.00383-Network-2,1811.00383-4-Table4-1.png," BIBREF3 explored transfer learning for NMT on low-resource languages. They studied the influence of language divergence between languages chosen for training the parent and child model, and showed that choosing similar languages for training the parent and child model leads to better improvements from transfer learning. A limitation of BIBREF3 approach is that they ignore the lexical similarity between languages and also the source language embeddings are randomly initialized. BIBREF10 , BIBREF11 , BIBREF12 take advantage of lexical similarity between languages in their work. BIBREF10 proposed to use Byte-Pair Encoding (BPE) to represent the sentences in both the parent and the child language to overcome the above limitation. They show using BPE benefits transfer learning especially when the involved languages are closely-related agglutinative languages. Similarly, BIBREF11 utilize lexical similarity between the source and assisting languages by training a character-level NMT system. BIBREF12 address lexical divergence by using bilingual embeddings and mixture of universal token embeddings. One of the languages' vocabulary, usually English vocabulary is considered as universal tokens and every word in the other languages is represented as a mixture of universal tokens. They show results on extremely low-resource languages.","We experimented with English INLINEFORM0 Hindi translation as the parent task. English is the assisting source language. Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks. Hindi, Bengali, Gujarati and Marathi are Indo-Aryan languages, while Malayalam and Tamil are Dravidian languages. All these languages have a canonical SOV word order.","English vocabulary consists of INLINEFORM0 tokens appearing at least 2 times in the English training corpus. For constructing the Hindi vocabulary we considered only those tokens appearing at least 5 times in the training split resulting in a vocabulary size of INLINEFORM1 tokens. For representing English and other source languages into a common space, we translate each word in the source language into English using a bilingual dictionary (Google Translate word translation in our case). In an end-to-end solution, it would have been ideal to use bilingual embeddings or obtain word-by-word translations via bilingual embeddings BIBREF14 . But, the quality of publicly available bilingual embeddings for English-Indian languages is very low for obtaining good-quality, bilingual representations BIBREF26 , BIBREF27 . We also found that these embeddings were not useful for transfer learning.","We use the CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages BIBREF17 , and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering BIBREF28 . These Hindi-tuned rules have been found to improve reordering for many English to Indian language pairs BIBREF29 .",Table 4: Sample Hindi translation generated by the Gujarati-Hindi NMT model. Text in red indicates phrase dropped by the no pre-ordered model.,1.0,1.0,1.0,1.0,1.0,0.33333333333333337,1.0,0.2
What was their performance on emotion detection?,Sample Answer,1611.02988-Facebook reactions as labels-1,1611.02988-Facebook reactions as labels-2,1611.02988-Facebook reactions as labels-3,1611.02988-Features-4,1611.02988-Acknowledgements-0,"In February 2016, after a short trial, Facebook made a more explicit reaction feature available world-wide. Rather than allowing for the underspecified “like” as the only wordless response to a post, a set of six more specific reactions was introduced, as shown in Figure FIGREF1 : Like, Love, Haha, Wow, Sad and Angry. We use such reactions as proxies for emotion labels associated to posts.","We collected Facebook posts and their corresponding reactions from public pages using the Facebook API, which we accessed via the Facebook-sdk python library. We chose different pages (and therefore domains and stances), aiming at a balanced and varied dataset, but we did so mainly based on intuition (see Section SECREF4 ) and with an eye to the nature of the datasets available for evaluation (see Section SECREF5 ). The choice of which pages to select posts from is far from trivial, and we believe this is actually an interesting aspect of our approach, as by using different Facebook pages one can intrinsically tackle the domain-adaptation problem (See Section SECREF6 for further discussion on this). The final collection of Facebook pages for the experiments described in this paper is as follows: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.","Note that thankful was only available during specific time spans related to certain events, as Mother's Day in May 2016.","As additional feature, we also included Word Embeddings, namely distributed representations of words in a vector space, which have been exceptionally successful in boosting performance in a plethora of NLP tasks. We use three different embeddings:","In addition to the anonymous reviewers, we want to thank Lucia Passaro and Barbara Plank for insightful discussions, and for providing comments on draft versions of this paper.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the morphological constraint added?,Sample Answer,1909.02855-Morphological Dictionaries ::: Comparison with MUSE ::: Word Frequency-0,1909.02855-Morphological Dictionaries ::: Comparison with MUSE ::: Train–test Paradigm Leakage-0,1909.02855-Morphological Generalization ::: Controlling for Word Frequency-0,1909.02855-Morphological Generalization ::: Controlling for Morphology-0,1909.02855-Morphological Generalization ::: Experiments on an Unrelated Language Pair-1,"The first and most prominent difference lies in the skew towards frequent word forms in MUSE evaluation. While our test dictionaries contain a representative sample of forms in lower frequency bins, the majority of forms present in MUSE are ranked in the top 10k in their respective language vocabularies. This is clearly presented in Figure FIGREF2 for the French–Spanish resource and also holds for the remaining 11 dictionaries.","Finally, we carefully analyze the magnitude of the train–test paradigm leakage. We found that, on average 20% (299 out of 1500) of source words in MUSE test dictionaries share their lemma with a word in the corresponding train dictionary. E.g. the French–Spanish test set includes the form perdent—a third-person plural present indicative of perdre (to lose) which is present in the train set. Note that the splits we provide for our dictionaries do not suffer from any leakage as we ensure that each dictionary contains the full paradigm of every lemma.","For highly inflectional languages, many of the infrequent types are rare forms of otherwise common lexemes and, given the morphological regularity of less frequent forms, a model that generalizes well should be able to translate those capably. Thus, to gain insight into the models' generalization ability we first examine the relation between their performance and the frequency of words in the test set.","From the results of the previous section, it is not clear whether the models perform badly on inflections of generally infrequent lemmata or whether they fail on infrequent morphosyntactic categories, independently of the lexeme frequency. Indeed, the frequency of different morphosyntactic categories is far from uniform. To shed more light on the underlying cause of the performance drop in sec:freqcontrol, we first analyze the differences in the models' performance as they translate forms belonging to different categories and, next, look at the distribution of these categories across the frequency bins.","The results of our experiments are presented in the last two rows of Table TABREF35 and, for Polish–Spanish, also in Figure FIGREF39. As expected, the BLI results on unrelated languages are generally, but not uniformly, worse than those on related language pairs. The accuracy for Spanish–Polish is particularly low, at 28% (for in vocabulary pairs). We see large variation in performance across morphosyntactic categories and more and less frequent lexemes, similar to that observed for related language pairs. In particular, we observe that —the category difficult for Polish–Czech BLI is also among the most challenging for Polish–Spanish. However, one of the highest performing categories for Polish–Czech, , yields much worse accuracy for Polish–Spanish.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What were their results on the classification and regression tasks,Sample Answer,1907.03187-Introduction-0,1907.03187-Introduction-4,1907.03187-Introduction-6,1907.03187-Cleaning-10,1907.03187-Cleaning-12,"- !`Socorro, me ha picado una víbora!","- Help, I was bitten by a snake!",- Not free.,#AlertaESI!!!!,se parte.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Which models do they use for phrase-based SMT?,Sample Answer,1806.09652-Introduction-1,1806.09652-Model-2,1806.09652-Dataset-2,1806.09652-1-Table1-1.png,1806.09652-5-Table3-1.png,"Comparable corpora such as Wikipedia, are collections of topic-aligned but non-sentence-aligned multilingual documents which are rich resources for extracting parallel sentences from. For example, Figure FIGREF1 shows that there are equivalent sentences on the page about Donald Trump in Tamil and English, and the phrase alignment for an example sentence is shown in Table TABREF4 .","The forward RNN reads the variable-length sentence and updates its recurrent state from the first token until the last one to create a fixed-size continuous vector representation of the sentence. The backward RNN processes the sentence in reverse. In our experiments, we use the concatenation of the last recurrent state in both directions as a final representation INLINEFORM0 DISPLAYFORM0 ","An English-Hindi parallel corpus BIBREF12 containing a total of INLINEFORM0 sentence pairs, from which a set of INLINEFORM1 sentence pairs were picked randomly.",Table 1. No. of articles on Wikipedia against the available parallel sentences in X-En Corpus.,Table 3. BLEU score results for En-Ta,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How is their NER model trained?,Sample Answer,1908.10001-Introduction-1,1908.10001-Chatbot architecture-2,1908.10001-Models ::: Intent model-0,1908.10001-Models ::: External validation-1,1908.10001-Conclusion-1,The travel industry is an excellent target for e-commerce chatbots for several reasons:,"The hotel search is backed by a database of approximately 100,000 cities and 300,000 hotels, populated using data from our partners. Each database entry contains the name of the city or hotel, geographic information (e.g., address, state, country), and various metadata (e.g., review score, number of bookings).","The intent model processes each incoming user message and classifies it as one of several intents. The most common intents are thanks, cancel, stop, search, and unknown (described in Table TABREF12); these intents were chosen for automation based on volume, ease of classification, and business impact. The result of the intent model is used to determine the bot's response, what further processing is necessary (in the case of search intent), and whether to direct the conversation to a human agent (in the case of unknown intent).","External metrics serve as a proxy for our NLP system's performance, since users are more likely to request an agent and less likely to complete their booking when the bot fails. Thus, an improvement in these metrics after a model deployment validates that the model functions as intended in the real world. However, both metrics are noisy and are affected by factors unrelated to NLP, such as seasonality and changes in the hotel supply chain.","Our success demonstrates that our chatbot is a viable alternative to traditional mobile and web applications for commerce. Indeed, we believe that innovations in task-oriented chatbot technology will have tremendous potential to improve consumer experience and drive business growth in new and unexplored channels.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How well does the system perform?,Sample Answer,1908.10001-Related work-0,1908.10001-Related work-1,1908.10001-Chatbot architecture-1,1908.10001-Models ::: Intent model-0,1908.10001-Models ::: Named entity recognition-0,"Numerous task-oriented chatbots have been developed for commercial and recreational purposes. Most commercial chatbots today use a frame-based dialogue system, which was first proposed in 1977 for a flight booking task BIBREF1. Such a system uses a finite-state automaton to direct the conversation, which fills a set of slots with user-given values before an action can be taken. Modern frame-based systems often use machine learning for the slot-filling subtask BIBREF2.","Natural language processing has been applied to other problems in the travel industry, for example, text mining hotel information from user reviews for a recommendation system BIBREF3, or determining the economic importance of various hotel characteristics BIBREF4. Sentiment analysis techniques have been applied to hotel reviews for classifying polarity BIBREF5 and identifying common complaints to report to hotel management BIBREF6.","At any point in the conversation, the user may request to talk to a customer support agent by clicking an “agent” or “help” button. The bot also sends the conversation to an agent if the user says something that the bot does not understand. Thus, the bot handles the most common use cases, while humans handle a long tail of specialized and less common requests.","The intent model processes each incoming user message and classifies it as one of several intents. The most common intents are thanks, cancel, stop, search, and unknown (described in Table TABREF12); these intents were chosen for automation based on volume, ease of classification, and business impact. The result of the intent model is used to determine the bot's response, what further processing is necessary (in the case of search intent), and whether to direct the conversation to a human agent (in the case of unknown intent).","For queries identified as search intent, we perform named entity recognition (NER) to extract spans from the query representing names of hotels and cities. Recently, neural architectures have shown to be successful for NER BIBREF9, BIBREF10. Typically, they are trained on the CoNLL-2003 Shared Task BIBREF11 which features four entity types (persons, organizations, locations, and miscellaneous).",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Where does their information come from?,Sample Answer,1908.10001-Related work-0,1908.10001-Related work-1,1908.10001-Chatbot architecture-1,1908.10001-Models ::: Named entity recognition-2,1908.10001-Conclusion-1,"Numerous task-oriented chatbots have been developed for commercial and recreational purposes. Most commercial chatbots today use a frame-based dialogue system, which was first proposed in 1977 for a flight booking task BIBREF1. Such a system uses a finite-state automaton to direct the conversation, which fills a set of slots with user-given values before an action can be taken. Modern frame-based systems often use machine learning for the slot-filling subtask BIBREF2.","Natural language processing has been applied to other problems in the travel industry, for example, text mining hotel information from user reviews for a recommendation system BIBREF3, or determining the economic importance of various hotel characteristics BIBREF4. Sentiment analysis techniques have been applied to hotel reviews for classifying polarity BIBREF5 and identifying common complaints to report to hotel management BIBREF6.","At any point in the conversation, the user may request to talk to a customer support agent by clicking an “agent” or “help” button. The bot also sends the conversation to an agent if the user says something that the bot does not understand. Thus, the bot handles the most common use cases, while humans handle a long tail of specialized and less common requests.","“double room in the cosmopolitan, las vegas for Aug 11-16”,","Our success demonstrates that our chatbot is a viable alternative to traditional mobile and web applications for commerce. Indeed, we believe that innovations in task-oriented chatbot technology will have tremendous potential to improve consumer experience and drive business growth in new and unexplored channels.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the best performing model?,Sample Answer,1802.09233-Introduction-0,1802.09233-Preprocessing and Normalization-1,1802.09233-Preprocessing and Normalization-4,1802.09233-Features ُExtraction-5,1802.09233-Features ُExtraction-8,"Sentiment analysis in Twitter is the problem of identifying people’s opinions expressed in tweets. It normally involves the classification of tweets into categories such as “positive”, “negative” and in some cases, “neutral”. The main challenges in designing a sentiment analysis system for Twitter are the following:",Normalization: Each tweet in English is converted to the lowercase. URLs and usernames are omitted. Non-Arabic letters are removed from each tweet in the Arabic-language sets. Words with repeated letters (i.e. elongated) are corrected.,"It is necessary to mention that in Ar-SiTAKA we did not use all the Arabic negation words due to the ambiguity of some of them. For example, the first word ما>, is a question mark in the following ""ما رأيك في ما حدث؟>-What do you think about what happened?"" and it means ""which/that"" in the following example ""إن ما حدث اليوم سيء جدا> - The matter that happened today was very bad"".","Part of Speech (POS): Subjective and objective texts have different POS tags BIBREF17 . According to BIBREF18 , non-neutral terms are more likely to exhibit the following POS tags in Twitter: nouns, adjectives, adverbs, abbreviations and interjections. The number of occurrences of each part of speech tag is used to represent each tweet.","The polarity of a tweet T given a lexicon L is calculated using the equation (1). First, the tweet is tokenized. Then, the number of positive (P) and negative (N) tokens found in the lexicon are counted. Finally, the polarity measure is calculated as follows: DISPLAYFORM0 ",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
"What does it mean for sentences to be ""lexically overlapping""?",Sample Answer,1802.03052-Related Work-2,1802.03052-Related Work-4,1802.03052-Related Work-6,1802.03052-Procedure and Explanation Review-4,1802.03052-Explanation Tablestore Growth-0,"At the collection end of the spectrum, Pasupat and Liang pasupat:2015 extract 2,108 HTML tables from Wikipedia, and propose a method of answering these questions by reasoning over the tables using formal logic. They also introduce the WikiTableQuestions dataset, a set of 22,033 question-answer pairs (such as “Greece held its last Summer Olympics during which year?”) that can be answered using these tables. Demonstrating the ability for collection at scale, Sun et al. sun:2016table extract a total of 104 million tables from Wikipedia and the web, and develop a model that constructs relational chains between table rows using a deep-learning framework. Using their system and table store, Sun et al. demonstrate state-of-the-art performance on several benchmark datasets, including WebQuestions BIBREF17 , a set of popular questions asked from the web designed to be answerable using the large structured knowledge graph Freebase (e.g. “What movies does Morgan Freeman star in?”).","Elementary science exams contain a variety of complex and challenging inference problems BIBREF1 , BIBREF2 , with nearly 70% of questions requiring some form of causal, process, or model-based reasoning to solve and produce an explanation for. In spite of these exams being taken by millions of students each year, elementary students tend not to be fast or voluminous readers by adult standards, making this a surprisingly low-resource domain for grade-appropriate study guides and other materials. The questions also tend to require world knowledge expressed in grade-appropriate language (like that bears have fur and that fur keeps animals warm) to solve. Because of these requirements and limitations, table stores for elementary science QA tend to be manually or semi-automatically constructed, and comparatively small.","To help improve the quality of automatically generated tables, Dalvi et al. Dalvi2016IKE introduce an interactive tool for semi-automatic table generation that allows annotators to query patterns over large corpora. They demonstrate that this tool can improve the speed of knowledge generation by up to a factor of 4 over manual methods, while increasing the precision and utility of the tables up to seven fold compared to completely automatic methods.","Each annotator required approximately 60 hours of initial training for this explanation authoring task. We found that most explanations could be constructed within 5-10 minutes, with the review process taking approximately 5 more minutes per question.","Finally, we examine the growth of the tablestore as it relates to the number of questions in the corpus. Figure 6 shows a monte-carlo simulation of the number of unique tablestore rows required to author explanations for specific corpus sizes. This relationship is strongly correlated (R=0.99) with an exponential proportional decrease. For this elementary science corpus, this asymptotes at approximately 6,000 unique table rows, and 10,000 questions, providing an estimate of the upper-bound of knowledge required in this domain, and the number of unique questions that can be generated within the scope of the elementary science curriculum.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What evaluation metric do they use?,Sample Answer,1604.05372-Introduction-1,1604.05372-Academic texts as Comparable Corpora-0,1604.05372-Academic texts as Comparable Corpora-2,1604.05372-Learning to Translate: Ukrainian-to-Russian transformations-5,1604.05372-5-Table2-1.png,"When a text collection contains documents in several languages, it becomes impractical to simply represent the documents as vectors of words occurring in them (""bag-of-words""), as the words surface forms are different, even in closely-related languages. Thus, one has to invent means to cross the inter-lingual gap and bring all documents to some sort of shared representation, without losing information about their topics or categories.","The Russian and Ukrainian languages are mainly spoken in Russian Federation and the Ukraine and belong to the East-Slavic group of the Indo-European language family. They share many common morphosyntactic features: both are SVO languages with free word order and rich morphology, both use the Cyrillic alphabet and share many common cognates.","The Ukrainian subcorpus contains about 60 thousand extended summaries (Russian and Ukrainian russian`автореферат', `avtoreferat') of theses submitted between 1998 and 2011. The Russian subcorpus is smaller in the number of documents (about 16 thousand, approximately the same time period), but the documents are full texts of theses, thus the total volume of the Russian subcorpus is notably larger: 830 million tokens versus 250 million tokens in the Ukrainian one. Generally, the texts belong to one genre that can be defined as post-Soviet expository academic prose, submitted for academic degree award process.","Thus, we use normal equation to find the optimal transformation matrix. The algebraic solution to each of 300 normal equations (one for each vector component $i$ ) is shown in the Equation 3 : ",Table 2: Clustering correspondence to document topics,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the linguistic differences between each class?,Sample Answer,1709.05404-Generic Dataset (Gen)-2,1709.05404-Generic Dataset (Gen)-7,1709.05404-Rhetorical Questions and Hyperbole-11,1709.05404-Learning Experiments-8,1709.05404-Linguistic Analysis-5,"We put out the remaining 11,040 posts on Mechanical Turk. As in LukinWalker13, we present the posts in ""quote-response"" pairs, where the response post to be annotated is presented in the context of its “dialogic parent”, another post earlier in the thread, or a quote from another post earlier in the thread BIBREF15 . In the task instructions, annotators are presented with a definition of sarcasm, followed by one example of a quote-response pair that clearly contains sarcasm, and one pair that clearly does not. Each task consists of 20 quote-response pairs that follow the instructions. Figure FIGREF13 shows the instructions and layout of a single quote-response pair presented to annotators. As in LukinWalker13 and Walkeretal12d, annotators are asked a binary question: Is any part of the response to this quote sarcastic?.","Rows 1 and 2 of Table TABREF9 show examples of posts that are labeled sarcastic in our final generic sarcasm set. Using our filtering method, we are able to reduce the number of posts annotated from our original 30K to around 11K, achieving a percentage of 20% sarcastic posts, even though we choose to use a conservative threshold of at least 6 out of 9 sarcasm labels. Since the number of posts being annotated is only a third of the original set size, this method reduces annotation effort, time, and cost, and helps us shift the distribution of sarcasm to more efficiently expand our dataset than would otherwise be possible.","ColstonObrien00b provide a theoretical framework that explains why hyperbole is so strongly associated with sarcasm. Hyperbole exaggerates the literal situation, introducing a discrepancy between the ""truth"" and what is said, as a matter of degree. A key observation is that this is a type of contrast BIBREF24 , BIBREF1 . In their framework:","Weakly-Supervised Learning. AutoSlog-TS is a weakly supervised pattern learner that only requires training documents labeled broadly as sarcastic or not-sarcastic. AutoSlog-TS uses a set of syntactic templates to define different types of linguistic expressions. The left-hand side of Table TABREF28 lists each pattern template and the right-hand side illustrates a specific lexico-syntactic pattern (in bold) that represents an instantiation of each general pattern template for learning sarcastic patterns in our data. In addition to these 17 templates, we added patterns to AutoSlog for adjective-noun, adverb-adjective and adjective-adjective, because these patterns are frequent in hyperbolic sarcastic utterances.","Hyperbole. One common pattern for hyperbole involves adverbs and adjectives, as noted above. We did not use this pattern to retrieve hyperbole, but because each hyperbolic sarcastic utterance contains multiple cues, we learn an expanded class of patterns for hyperbole. Table TABREF33 illustrates some of the new adverb adjective patterns that are frequent, high-precision indicators of sarcasm.",1.0,1.0,1.0,1.0,1.0,0.20000000000000004,0.2,0.2
what was their system's f1 score?,Sample Answer,1708.05482-Introduction-4,1708.05482-Related Work-1,1708.05482-More Insights into the ConvMS-Memnet-3,1708.05482-More Insights into the ConvMS-Memnet-10,1708.05482-Conclusions-0,"Existing approaches to emotion cause extraction mostly rely on methods typically used in information extraction, such as rule based template matching, sequence labeling and classification based methods. Most of them use linguistic rules or lexicon features, but do not consider the semantic information and ignore the relation between the emotion word and emotion cause. In this paper, we present a new method for emotion cause extraction. We consider emotion cause extraction as a question answering (QA) task. Given a text containing the description of an event which [id=lq]may or may not cause a certain emotion, we take [id=lq]an emotion word [id=lq]in context, such as “sad”, as a query. The question to the QA system is: “Does the described event cause the emotion of sadness?”. The [id=lq]expected answer [id=lq]is either “yes” or “no”. (see Figure FIGREF1 ). We build our QA system based on a deep memory network. The memory network has two inputs: a piece of text, [id=lq]referred to as a story in QA systems, and a query. The [id=lq]story is represented using a sequence of word embeddings.","In emotion analysis, we first need to determine the taxonomy of emotions. Researchers have proposed a list of primary emotions BIBREF7 , BIBREF8 , BIBREF9 . In this study, we adopt Ekman's emotion classification scheme BIBREF8 , which identifies six primary emotions, namely happiness, sadness, fear, anger, disgust and surprise, as known as the “Big6” scheme in the W3C Emotion Markup Language. This emotion classification scheme is agreed upon by most previous works in Chinese emotion analysis.","Essentially, memory network aims to measure the weight of each word in the clause with respect to the emotion word. The question is, will the model really focus on the words which describe the emotion cause? We choose one example to show the attention results in Table 5:","Ex.3 45 days, it is long time for the parents who lost their baby. If the baby comes back home, they would become so happy in this Spring Festival.","In this [id=lq]work, we [id=lq]treat emotion cause extraction as a QA task and propose a new model based on deep memory networks for identifying [id=lq]the emotion causes for an emotion expressed in text. [id=lq]The key property of this approach is the use of context information in the learning process which is ignored in the original memory network. Our new [id=lq]memory network architecture is able [id=lq]to store context in different memory slots to capture context information [id=lq]in proper sequence by convolutional operation. Our model achieves the state-of-the-art performance on a dataset for emotion cause detection when compared to a number of competitive baselines. In the future, we will explore effective ways [id=lq]to model discourse relations among clauses and develop a QA system which can directly output the cause of emotions as answers.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what lexical features are extracted?,Sample Answer,1708.05482-Introduction-1,1708.05482-Introduction-2,1708.05482-More Insights into the ConvMS-Memnet-2,1708.05482-3-Figure2-1.png,1708.05482-8-Table5-1.png,Ex.1 我的手机昨天丢了，我现在很难过。,"Ex.1 Because I lost my phone yesterday, I feel sad now.","It is widely acknowledged that computational models using deep architecture with multiple layers have better ability to learn data representations with multiple levels of abstractions. In this section, we evaluate the power of multiple hops in this task. We set the number of hops from 1 to 9 with 1 standing for the simplest single layer network shown in Figure 4. The more hops are stacked, the more complicated the model is. Results are shown in Table 4. The single layer network has achieved a competitive performance. With the increasing number of hops, the performance improves. However, when the number of hops is larger than 3, the performance decreases due to overfitting. Since the dataset for this task is small, more parameters will lead to overfitting. As such, we choose 3 hops in our final model since it gives the best performance in our experiments.",Figure 2: A single layer memory network.,Table 5: The distribution of attention in different hops.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what word level sequences features are extracted?,Sample Answer,1708.05482-Experiments and Evaluation-0,1708.05482-Evaluation and Comparison-1,1708.05482-Evaluation and Comparison-3,1708.05482-More Insights into the ConvMS-Memnet-11,1708.05482-3-Figure2-1.png,We first presents the experimental settings and then report the results in this section.,RB (Rule based method): The rule based method proposed in BIBREF33 .,RB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base): This methods was previously proposed for emotion cause classification in BIBREF36 . It takes rules and facts in a knowledge base as features for classifier training. We train a SVM using features extracted from the rules defined in BIBREF33 and the Chinese Emotion Cognition Lexicon BIBREF35 .,"In this example, the cause of emotion “happy” is described in the third clause.",Figure 2: A single layer memory network.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Why challenges does word segmentation in Vietnamese pose?,Sample Answer,1906.07662-TEXT MODELLING AND FEATURES-1,1906.07662-Maximum Matching-0,1906.07662-Maximum Entropy (ME)-1,1906.07662-Conditional Random Fields-0,1906.07662-Support Vector Machines-0,"There are two common modeling methods for basic NLP tasks, including n-gram model and bag-of-words model. The n-gram model is widely used in natural language processing while the bag-of-words model is a simplified representation used in natural language processing and information retrieval BIBREF17 , BIBREF18 . According to the bag-of-words model, the representative vector of sentences in the document does not preserve the order of the words in the original sentences. It represents the word using term frequency collected from the document rather than the order of words or the structure of sentences in the document. The bag-of-words model is commonly used in methods of document classification, where the frequency of occurrence of each word is used as an attribute feature for training a classifier. In contrast, an n-gram is a contiguous sequence of n items from a given sequence of text. An n-gram model is a type of probabilistic language model for predicting the next item in a given sequence in form of a Markov model. To address word segmentation issue, the n-gram model is usually used for approaches because it considers the order of tokens in the original sentences. The sequence is also kept the original order as input and output sentences.","Maximum matching (MM) is one of the most popular fundamental and structural segmentation algorithms for word segmentation BIBREF19 . This method is also considered as the Longest Matching (LM) in several research BIBREF9 , BIBREF3 . It is used for identifying word boundary in languages like Chinese, Vietnamese and Thai. This method is a greedy algorithm, which simply chooses longest words based on the dictionary. Segmentation may start from either end of the line without any difference in segmentation results. If the dictionary is sufficient BIBREF19 , the expected segmentation accuracy is over 90%, so it is a major advantage of maximum matching . However, it does not solve the problem of ambiguous words and unknown words that do not exist in the dictionary.",P(s) is also a product of probabilities of words created from sentence INLINEFORM0 (1). Each conditional probability of word is based on context h of the last n word in the sentence s.,"To tokenize a Vietnamese word, in HMM or ME, authors only rely on features around a word segment position. Some other features are also affected by adding more special attributes, such as, in case ’?’ question mark at end of sentence, Part of Speech (POS), and so on. Conditional Random Fields is one of methods that uses additional features to improve the selection strategy BIBREF7 .","Support Vector Machines (SVM) is a supervised machine learning method which considers dataset as a set of vectors and tries to classify them into specific classes. Basically, SVM is a binary classifier. however, most classification tasks are multi-class classifiers. When applying SVMs, the method has been extended to classify three or more classes. Particular NLP tasks, like word segmentation and Part-of-speech task, each token/word in documents will be used as a feature vector. For the word segmentation task, each token and its features are considered as a vector for the whole document, and the SVM model will classify this vector into one of the three tags (B-IO).",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How successful are the approaches used to solve word segmentation in Vietnamese?,Sample Answer,1906.07662-TEXT MODELLING AND FEATURES-1,1906.07662-Maximum Entropy (ME)-1,1906.07662-Conditional Random Fields-0,1906.07662-Conditional Random Fields-1,1906.07662-Support Vector Machines-0,"There are two common modeling methods for basic NLP tasks, including n-gram model and bag-of-words model. The n-gram model is widely used in natural language processing while the bag-of-words model is a simplified representation used in natural language processing and information retrieval BIBREF17 , BIBREF18 . According to the bag-of-words model, the representative vector of sentences in the document does not preserve the order of the words in the original sentences. It represents the word using term frequency collected from the document rather than the order of words or the structure of sentences in the document. The bag-of-words model is commonly used in methods of document classification, where the frequency of occurrence of each word is used as an attribute feature for training a classifier. In contrast, an n-gram is a contiguous sequence of n items from a given sequence of text. An n-gram model is a type of probabilistic language model for predicting the next item in a given sequence in form of a Markov model. To address word segmentation issue, the n-gram model is usually used for approaches because it considers the order of tokens in the original sentences. The sequence is also kept the original order as input and output sentences.",P(s) is also a product of probabilities of words created from sentence INLINEFORM0 (1). Each conditional probability of word is based on context h of the last n word in the sentence s.,"To tokenize a Vietnamese word, in HMM or ME, authors only rely on features around a word segment position. Some other features are also affected by adding more special attributes, such as, in case ’?’ question mark at end of sentence, Part of Speech (POS), and so on. Conditional Random Fields is one of methods that uses additional features to improve the selection strategy BIBREF7 .","There are several CRF libraries, such as CRF++, CRFsuite. These machine learning toolkits can be used to solve the task by providing an annotated corpus with extracted features. The toolkit will be used to train a model based on the corpus and extract a tagging model. The tagging model will then be used to tag on input text without annotated corpus. In the training and tagging stages, extracting features from the corpus and the input text is necessary for both stages.","Support Vector Machines (SVM) is a supervised machine learning method which considers dataset as a set of vectors and tries to classify them into specific classes. Basically, SVM is a binary classifier. however, most classification tasks are multi-class classifiers. When applying SVMs, the method has been extended to classify three or more classes. Particular NLP tasks, like word segmentation and Part-of-speech task, each token/word in documents will be used as a feature vector. For the word segmentation task, each token and its features are considered as a vector for the whole document, and the SVM model will classify this vector into one of the three tags (B-IO).",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What approaches without reinforcement learning have been tried?,Sample Answer,1909.00542-Related Work-0,1909.00542-Classification vs. Regression Experiments-8,1909.00542-Classification vs. Regression Experiments-10,1909.00542-Reinforcement Learning-3,1909.00542-Evaluation Correlation Analysis-1,"The BioASQ challenge has organised annual challenges on biomedical semantic indexing and question answering since 2013 BIBREF0. Every year there has been a task about semantic indexing (task a) and another about question answering (task b), and occasionally there have been additional tasks. The tasks defined for 2019 are:",: Label as “summary” all sentences from the input text that have a ROUGE score above a threshold $t$.,"As in BIBREF11, The ROUGE score of an input sentence was the ROUGE-SU4 F1 score of the sentence against the set of reference summaries.",Candidate sentence;,"The Pearson correlation between two variables is computed as the covariance of the two variables divided by the product of their standard deviations. This correlation is a good indication of a linear relation between the two variables, but may not be very effective when there is non-linear correlation.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are new best results on standard benchmark?,Sample Answer,1909.01013-Introduction-1,1909.01013-Approach-0,1909.01013-Approach ::: Regularizers for Dual Models-0,1909.01013-Conclusion-0,1909.01013-2-Figure2-1.png,"Recent research has attempted to induce unsupervised bilingual lexicons by aligning monolingual word vector spaces BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15. Given a pair of languages, their word alignment is inherently a bi-directional problem (e.g. English-Italian vs Italian-English). However, most existing research considers mapping from one language to another without making use of symmetry. Our experiments show that separately learned UBLI models are not always consistent in opposite directions. As shown in Figure 1a, when the model of BIBREF11 Conneau18a is applied to English and Italian, the primal model maps the word “three” to the Italian word “tre”, but the dual model maps “tre” to “two” instead of “three”.","We take BIBREF11 as our baseline, introducing a novel regularizer to enforce cycle consistency. Let $X=\lbrace x_1,...,x_n\rbrace $ and $Y=\lbrace y_1,...,y_m\rbrace $ be two sets of $n$ and $m$ word embeddings for a source and a target language, respectively. The primal UBLI task aims to learn a linear mapping $\mathcal {F}:X\rightarrow Y$ such that for each $x_i$, $\mathcal {F}(x_i)$ corresponds to its translation in $Y$. Similarly, a linear mapping $\mathcal {G}:Y\rightarrow X$ is defined for the dual task. In addition, we introduce two language discriminators $D_x$ and $D_y$, which are trained to discriminate between the mapped word embeddings and the original word embeddings.","We train $\mathcal {F}$ and $\mathcal {G}$ jointly and introduce two regularizers. Formally, we hope that $\mathcal {G}(\mathcal {F}(X))$ is similar to $X$ and $\mathcal {F}(\mathcal {G}(Y))$ is similar to $Y$. We implement this constraint as a cycle consistency loss. As a result, the proposed model has two learning objectives: i) an adversarial loss ($\ell _{adv}$) for each model as in the baseline. ii) a cycle consistency loss ($\ell _{cycle}$) on each side to avoid $\mathcal {F}$ and $\mathcal {G}$ from contradicting each other. The overall architecture of our model is illustrated in Figure FIGREF4.","We investigated a regularization method to enhance unsupervised bilingual lexicon induction, by encouraging symmetry in lexical mapping between a pair of word embedding spaces. Results show that strengthening bi-directional mapping consistency significantly improves the effectiveness over the state-of-the-art method, leading to the best results on a standard benchmark.",Figure 2: The proposed framework. (a)X → F(X)→ G(F(X))→ X; (b) Y → G(Y )→ F(G(Y ))→ Y .,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How better is performance compared to competitive baselines?,Sample Answer,1909.01013-Introduction-1,1909.01013-Related Work-1,1909.01013-Related Work-2,1909.01013-Approach-0,1909.01013-Approach ::: Regularizers for Dual Models-3,"Recent research has attempted to induce unsupervised bilingual lexicons by aligning monolingual word vector spaces BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15. Given a pair of languages, their word alignment is inherently a bi-directional problem (e.g. English-Italian vs Italian-English). However, most existing research considers mapping from one language to another without making use of symmetry. Our experiments show that separately learned UBLI models are not always consistent in opposite directions. As shown in Figure 1a, when the model of BIBREF11 Conneau18a is applied to English and Italian, the primal model maps the word “three” to the Italian word “tre”, but the dual model maps “tre” to “two” instead of “three”.","Cycle Consistency. Forward-backward consistency has been used to discover the correspondence between unpaired images BIBREF21, BIBREF22. In machine translation, similar ideas were exploited, BIBREF23, BIBREF24 and BIBREF25 use dual learning to train two “opposite” language translators by minimizing the reconstruction loss. BIBREF26 consider back-translation, where a backward model is used to build synthetic parallel corpus and a forward model learns to generate genuine text based on the synthetic output.","Closer to our method, BIBREF27 jointly train two autoencoders to learn supervised bilingual word embeddings. BIBREF28 use sinkhorn distance BIBREF29 and back-translation to align word embeddings. However, they cannot perform fully unsupervised training, relying on WGAN BIBREF30 for providing initial mappings. Concurrent with our work, BIBREF31 build a adversarial autoencoder with cycle consistency loss and post-cycle reconstruction loss. In contrast to these works, our method is fully unsupervised, simpler, and empirically more effective.","We take BIBREF11 as our baseline, introducing a novel regularizer to enforce cycle consistency. Let $X=\lbrace x_1,...,x_n\rbrace $ and $Y=\lbrace y_1,...,y_m\rbrace $ be two sets of $n$ and $m$ word embeddings for a source and a target language, respectively. The primal UBLI task aims to learn a linear mapping $\mathcal {F}:X\rightarrow Y$ such that for each $x_i$, $\mathcal {F}(x_i)$ corresponds to its translation in $Y$. Similarly, a linear mapping $\mathcal {G}:Y\rightarrow X$ is defined for the dual task. In addition, we introduce two language discriminators $D_x$ and $D_y$, which are trained to discriminate between the mapped word embeddings and the original word embeddings.",Full objective. The final objective is:,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What 6 language pairs is experimented on?,Sample Answer,1909.01013-Approach ::: Regularizers for Dual Models-0,1909.01013-Approach ::: Regularizers for Dual Models-2,1909.01013-Experiments ::: The Effectiveness of Dual Learning-1,1909.01013-3-Table1-1.png,1909.01013-4-Table4-1.png,"We train $\mathcal {F}$ and $\mathcal {G}$ jointly and introduce two regularizers. Formally, we hope that $\mathcal {G}(\mathcal {F}(X))$ is similar to $X$ and $\mathcal {F}(\mathcal {G}(Y))$ is similar to $Y$. We implement this constraint as a cycle consistency loss. As a result, the proposed model has two learning objectives: i) an adversarial loss ($\ell _{adv}$) for each model as in the baseline. ii) a cycle consistency loss ($\ell _{cycle}$) on each side to avoid $\mathcal {F}$ and $\mathcal {G}$ from contradicting each other. The overall architecture of our model is illustrated in Figure FIGREF4.","where $\Delta $ denotes the discrepancy criterion, which is set as the average cosine similarity in our model.","Table TABREF13 shows the inconsistency rates of back translation between Adv-C and our method on MUSE. Compared with Adv-C, our model significantly reduces the inconsistency rates on all language pairs, which explains the overall improvement in Table TABREF12. Table TABREF14 gives several word translation examples. In the first three cases, our regularizer successfully fixes back translation errors. In the fourth case, ensuring cycle consistency does not lead to the correct translation, which explains some errors by our system. In the fifth case, our model finds a related word but not the same word in the back translation, due to the use of cosine similarity for regularization.",Table 1: Accuracy on MUSE and Vecmap.,"Table 4: Accuracy (P@1) on Vecmap. The best results are bolded. †Results as reported in the original paper. For unsupervised methods, we report the average accuracy across 10 runs.",1.0,1.0,1.0,1.0,1.0,0.5714285714285715,1.0,0.4
What textual features are used?,Sample Answer,1910.01340-Introduction-0,1910.01340-Related Work on IRA Trolls-1,1910.01340-Textual Representation ::: Thematic Information-9,1910.01340-Experiments and Analysis ::: Analysis-1,1910.01340-Experiments and Analysis ::: Analysis-2,"Recent years have seen a large increase in the amount of disinformation and fake news spread on social media. False information was used to spread fear and anger among people, which in turn, provoked crimes in some countries. The US in the recent years experienced many similar cases during the presidential elections, such as the one commonly known as “Pizzagate"" . Later on, Twitter declared that they had detected a suspicious campaign originated in Russia by an organization named Internet Research Agency (IRA), and targeted the US to affect the results of the 2016 presidential elections. The desired goals behind these accounts are to spread fake and hateful news to further polarize the public opinion. Such attempts are not limited to Twitter, since Facebook announced in mid-2019 that they detected a similar attempt originating from UAE, Egypt and Saudi Arabia and targeting other countries such as Qatar, Palestine, Lebanon and Jordan. This attempt used Facebook pages, groups, and user accounts with fake identities to spread fake news supporting their ideological agendas. The automatic detection of such attempts is very challenging, since the true identity of these suspicious accounts is hidden by imitating the profiles of real persons from the targeted audience; in addition, sometimes they publish their suspicious idea in a vague way through their tweets' messages.","The research works studied IRA trolls from several perspectives. The work in BIBREF4 studied the links' domains that were mentioned by IRA trolls and how much they overlap with other links used in tweets related to ""Brexit"". In addition, they compare ""Left"" and ""Right"" ideological trolls in terms of the number of re-tweets they received, number of followers, etc, and the online propaganda strategies they used. The authors in BIBREF2 analyzed IRA campaign in both Twitter and Facebook, and they focus on the evolution of IRA paid advertisements on Facebook before and after the US presidential elections from a thematic perspective.","LIWC: We use a set of linguistic categories from the LIWC linguistic dictionary BIBREF20. The used categories are: pronoun, anx, cogmech, insight, cause, discrep, tentat, certain, inhib, incl.","Why do the thematic information help? The Flip-Flop behavior. As an example, let's considering the fear and joy emotions in Figure FIGREF34. We can notice that all the themes that used to nudge the division issues have a decreasing dashed line, where others such as Supporting Trump theme has an extremely increasing dashed line. Therefore, we manually analyzed the tweets of some IRA accounts and we found this observation clear, as an example from user $x$:","Islam and War: (A) @RickMad: Questions are a joke, a Muslim asks how SHE will be protected from Islamaphobia! Gmaffb! How will WE be protected from terrori…",1.0,1.0,1.0,1.0,1.0,0.13333333333333333,0.1,0.2
What is the performance of the baseline?,Sample Answer,2003.08385-Related Work ::: Representation Learning for Stance Detection-1,2003.08385-The x-stance Dataset ::: Data Collection ::: Filtering-1,2003.08385-The x-stance Dataset ::: Data Split-1,2003.08385-The x-stance Dataset ::: Analysis ::: Class Distribution-0,2003.08385-Baseline Experiments ::: How Important are Spelled-Out Targets?-0,"However, they only experimented with a single-segment encoding of the input, preventing cross-target transfer of the model. BIBREF11 propose a conditional encoding approach to encode both the target and the tweet as sequences. They use a bidirectional LSTM to condition the encoding of the tweets on the encoding of the target, and then apply a nonlinear projection on the conditionally encoded tweet. This allows them to train a model that can generalize to previously unseen targets.","In the context of communal and cantonal elections, candidates have answered both local questions and a subset of the national questions. Of those elections, we only considered answers to the questions that also had been asked in a national election. Furthermore, they were only used to augment the training set while the validation and test sets were restricted to answers from national elections.","Furthermore, in order to test cross-question generalization performance within previously seen topics, we manually selected 16 held-out questions that are distributed over the remaining 10 topics. We selected the held-out questions manually because we wanted to make sure that they are truly unseen and that no paraphrases of the questions are found in the training set.","Figure FIGREF23 visualizes the proportion of `favor' and `against` stances for each target in the dataset. The ratio differs between questions but is relatively equally distributed across the topics. In particular, the questions belonging to the held-out topics (with a `favor' ratio of 49.4%) have a similar class distribution as the questions within other topics (with a `favor' ratio of 50.0%).","Finally we test whether the target really needs to be represented by natural language (e.g. “Do you support X?”). Namely, an alternative is to represent the target with a trainable embedding instead of a question.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What was the performance of multilingual BERT?,Sample Answer,2003.08385-The x-stance Dataset ::: Data Split-1,2003.08385-The x-stance Dataset ::: Analysis ::: Class Distribution-0,2003.08385-The x-stance Dataset ::: Analysis ::: Linguistic Properties-0,2003.08385-Baseline Experiments ::: How Important are the Segments?-3,2003.08385-Baseline Experiments ::: How Important are the Segments?-8,"Furthermore, in order to test cross-question generalization performance within previously seen topics, we manually selected 16 held-out questions that are distributed over the remaining 10 topics. We selected the held-out questions manually because we wanted to make sure that they are truly unseen and that no paraphrases of the questions are found in the training set.","Figure FIGREF23 visualizes the proportion of `favor' and `against` stances for each target in the dataset. The ratio differs between questions but is relatively equally distributed across the topics. In particular, the questions belonging to the held-out topics (with a `favor' ratio of 49.4%) have a similar class distribution as the questions within other topics (with a `favor' ratio of 50.0%).","Not every question is unique; some questions are paraphrases describing the same political issue. For example, in the 2015 election, the candidates were asked: “Should the consumption of cannabis as well as its possession for personal use be legalised?” Four years later they were asked: “Should cannabis use be legalized?” However, we do not see any need to consolidate those duplicates because they contribute to the diversity of the training data.","In both cases the performance decreases across all evaluation settings (Table TABREF39). The loss in performance is much higher when comments are missing, indicating that the comments contain the most important information about stance. As can be expected, the score achieved without comments is only slightly different from the target-wise majority class baseline.","The results in Table TABREF39 show that the performance of the model decreases in both cases, confirming that it learns to take into account both segments.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Are the annotations automatic or manually created?,Sample Answer,1804.11346-Related Work-0,1804.11346-Related Work-1,1804.11346-Related Work-2,1804.11346-Collection methodology-3,1804.11346-A Baseline for Portuguese NLI-0,"NLI has attracted a lot of attention in recent years. Due to the availability of suitable data, as discussed earlier, this attention has been particularly focused on English. The most notable examples are the two editions of the NLI shared task organized in 2013 BIBREF6 and 2017 BIBREF7 .","Even though most NLI research has been carried out on English data, an important research trend in recent years has been the application of NLI methods to other languages, as discussed in multilingual-nli. Recent NLI studies on languages other than English include Arabic BIBREF8 and Chinese BIBREF9 , BIBREF10 . To the best of our knowledge, no study has been published on Portuguese and the NLI-PT dataset opens new possibilities of research for Portuguese. In Section ""A Baseline for Portuguese NLI"" we present the first simple baseline results for this task.","Finally, as NLI-PT can be used in other applications besides NLI, it is important to point out that a number of studies have been published on educational NLP applications for Portuguese and on the compilation of learner language resources for Portuguese. Examples of such studies include grammatical error correction BIBREF11 , automated essay scoring BIBREF12 , academic word lists BIBREF13 , and the learner corpora presented in the previous section.","Concerning the corpus design, there is some variability among the sources we used. Leiria corpus and PEAPL2 followed a similar approach for data collection and show a close design. They consider a close list of topics, called “stimulus”, which belong to three general areas: (i) the individual; (ii) the society; (iii) the environment. Those topics are presented to the students in order to produce a written text. As a whole, texts from PEAPL2 and Leiria represent 36 different stimuli or topics in the dataset. In COPLE2 corpus the written texts correspond to written exercises done during Portuguese lessons, or to official Portuguese proficiency tests. For this reason, the topics considered in COPLE2 corpus are different from the topics in Leiria and PEAPL2. The number of topics is also larger in COPLE2 corpus: 149 different topics. There is some overlap between the different topics considered in COPLE2, that is, some topics deal with the same subject. This overlap allowed us to reorganize COPLE2 topics in our dataset, reducing them to 112.","To demonstrate the usefulness of the dataset we present the first lexical baseline for Portuguese NLI using a sub-set of NLI-PT. To the best of our knowledge, no study has been published on Portuguese NLI and our work fills this gap.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How long are the essays on average?,Sample Answer,1804.11346-Introduction-2,1804.11346-Related Work-1,1804.11346-Related Work-2,1804.11346-Applications-0,1804.11346-A Baseline for Portuguese NLI-0,"The aforementioned Portuguese learner corpora contain very useful data for research, particularly for Native Language Identification (NLI), a task that has received much attention in recent years. NLI is the task of determining the native language (L1) of an author based on their second language (L2) linguistic productions BIBREF4 . NLI works by identifying language use patterns that are common to groups of speakers of the same native language. This process is underpinned by the presupposition that an author’s L1 disposes them towards certain language production patterns in their L2, as influenced by their mother tongue. A major motivation for NLI is studying second language acquisition. NLI models can enable analysis of inter-L1 linguistic differences, allowing us to study the language learning process and develop L1-specific pedagogical methods and materials.","Even though most NLI research has been carried out on English data, an important research trend in recent years has been the application of NLI methods to other languages, as discussed in multilingual-nli. Recent NLI studies on languages other than English include Arabic BIBREF8 and Chinese BIBREF9 , BIBREF10 . To the best of our knowledge, no study has been published on Portuguese and the NLI-PT dataset opens new possibilities of research for Portuguese. In Section ""A Baseline for Portuguese NLI"" we present the first simple baseline results for this task.","Finally, as NLI-PT can be used in other applications besides NLI, it is important to point out that a number of studies have been published on educational NLP applications for Portuguese and on the compilation of learner language resources for Portuguese. Examples of such studies include grammatical error correction BIBREF11 , automated essay scoring BIBREF12 , academic word lists BIBREF13 , and the learner corpora presented in the previous section.","NLI-PT was developed primarily for NLI, but it can be used for other research purposes ranging from second language acquisition to educational NLP applications. Here are a few examples of applications in which the dataset can be used:","To demonstrate the usefulness of the dataset we present the first lexical baseline for Portuguese NLI using a sub-set of NLI-PT. To the best of our knowledge, no study has been published on Portuguese NLI and our work fills this gap.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What type of model are the ELMo representations used in?,Sample Answer,1809.09795-Experimental Setup-1,1809.09795-Experimental Setup-3,1809.09795-Experimental Setup-4,1809.09795-Results-3,1809.09795-4-Table2-1.png,"Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 . The dataset was manually annotated using binary labels. We also use the dataset by BIBREF4 , which is manually annotated for sarcasm. Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.","Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 .","In Table TABREF1 , we see a notable difference in terms of size among the Twitter datasets. Given this circumstance, and in light of the findings by BIBREF18 , we are interested in studying how the addition of external soft-annotated data impacts on the performance. Thus, in addition to the datasets introduced before, we use two corpora for augmentation purposes. The first dataset was collected using the Twitter API, targeting tweets with the hashtags #sarcasm or #irony, resulting on a total of 180,000 and 45,000 tweets respectively. On the other hand, to obtain non-sarcastic and non-ironic tweets, we relied on the SemEval 2018 Task 1 dataset BIBREF25 . To augment each dataset with our external data, we first filter out tweets that are not in English using language guessing systems. We later extract all the hashtags in each target dataset and proceed to augment only using those external tweets that contain any of these hashtags. This allows us to, for each class, add a total of 36,835 tweets for the Ptáček corpus, 8,095 for the Riloff corpus and 26,168 for the SemEval-2018 corpus.","Finally, our experiments showed that enlarging existing Twitter datasets by adding external soft-labeled data from the same media source does not yield improvements in the overall performance. This complies with the observations made by BIBREF18 . Since we have designed our augmentation tactics to maximize the overlap in terms of topic, we believe the soft-annotated nature of the additional data we have used is the reason that keeps the model from improving further.",Table 2: Summary of our obtained results.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How was lexical diversity measured?,Sample Answer,1707.06939-Introduction-1,1707.06939-Related Work-0,1707.06939-Related Work-2,1707.06939-4-Figure1-1.png,1707.06939-7-Figure2-1.png,"Efficiency gains can be achieved either collectively at the level of the entire crowd or by helping individual workers. At the crowd level, efficiency can be gained by assigning tasks to workers in the best order BIBREF2 , by filtering out poor tasks or workers, or by best incentivizing workers BIBREF3 . At the individual worker level, efficiency gains can come from helping workers craft more accurate responses and complete tasks in less time.","An important goal of crowdsourcing research is achieving efficient scalability of the crowd to very large sets of tasks. Efficiency in crowdsourcing manifests both in receiving more effective information per worker and in making individual workers faster and/or more accurate. The former problem is a significant area of interest BIBREF5 , BIBREF6 , BIBREF7 while less work has been put towards the latter.","The usability feature we study here is an autocompletion user interface (AUI). AUIs are broadly familiar to online workers at this point, thanks in particular to their prominence on Google's main search bar (evolving out of the original Google Instant implementation). However, literature on the benefits of AUIs (and related word prediction and completion interfaces) in terms of improving efficiency is decidedly mixed.",Figure 1. Screenshots of our conceptualization task interface. The presence of the AUI is the only difference between the task interfaces.,"Figure 2. Distributions of time delays. Workers in the AUI treatment were significantly slower than in the control, and this was primarily due to the submission delay between when they finished entering text and when they submitted their response.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what previous RNN models do they compare with?,Sample Answer,1808.09029-Introduction-1,1808.09029-Pyramidal transformation for input-3,1808.09029-Pyramidal transformation for input-5,1808.09029-Grouped linear transformation for context-2,1808.09029-Pyramidal Recurrent Unit-1,"A natural way to improve the expressiveness of linear transformations is to increase the number of dimensions of the input and context vectors, but this comes with a significant increase in the number of parameters which may limit generalizability. An example is shown in Figure FIGREF1 , where LSTMs performance decreases with the increase in dimensions of the input and context vectors. Moreover, the semantics of the input and context vectors are different, suggesting that each may benefit from specialized treatment.",where INLINEFORM0 indicates concatenation. We note that pyramidal transformation with INLINEFORM1 is the same as the linear transformation.,"We sub-sample the input vector INLINEFORM0 into INLINEFORM1 pyramidal levels using the kernel-based approach BIBREF8 , BIBREF9 . Let us assume that we have a kernel INLINEFORM2 with INLINEFORM3 elements. Then, the input vector INLINEFORM4 can be sub-sampled as: DISPLAYFORM0 ","The linear transformation INLINEFORM0 maps INLINEFORM1 linearly to INLINEFORM2 . Grouped linear transformations break the linear interactions by factoring the linear transformation into two steps. First, a GLT splits the input vector INLINEFORM3 into INLINEFORM4 smaller groups such that INLINEFORM5 . Second, a linear transformation INLINEFORM6 is applied to map INLINEFORM7 linearly to INLINEFORM8 , for each INLINEFORM9 . The INLINEFORM10 resultant output vectors INLINEFORM11 are concatenated to produce the final output vector INLINEFORM12 . DISPLAYFORM0 ","At time INLINEFORM0 , the PRU combines the input vector INLINEFORM1 and the previous context vector (or previous hidden state vector) INLINEFORM2 using the following transformation function as: DISPLAYFORM0 ",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the dataset used as input to the Word2Vec algorithm?,Sample Answer,2001.09332-Introduction-3,2001.09332-Implementation details-2,2001.09332-Results ::: Analysis of the various models-1,2001.09332-Results ::: Comparison with other models-1,2001.09332-Results ::: Comparison with other models-2,"In this work, by setting the size of the embedding to a commonly used average value, various parameters are analysed as the number of learning epochs changes, depending on the window sizes and the negatively backpropagated samples.","Note that among the special characters are also included punctuation marks, which therefore do not appear within the vocabulary. However, some of them (`.', `?' and `!') are later removed, as they are used to separate the sentences.","Figure FIGREF11 shows the trends of the total accuracy at different epochs for the various models using 3COSMUL (the trend obtained with 3COSADD is very similar). Here we can see how the use of high negative sampling can worsen performance, even causing the network to oscillate (W5N20) in order to better adapt to all the data. The choice of the negative sampling to be used should therefore be strongly linked to the choice of the window size as well as to the number of training epochs.","As it can be seen, regardless of the metric used, our model has significantly better results than the other two models, both overall and within the two macro-areas. Furthermore, the other two models seem to be more subject to the metric used, perhaps due to a stabilization not yet reached for the few training epochs.","For a complete comparison, both models were also tested considering only the subset of the analogies in common with our model (i.e. eliminating from the test all those analogies that were not executable by one or the other model). Tables TABREF16 and TABREF17 again highlight the marked increase in performance of our model compared to both.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the results of the experiment?,Sample Answer,1911.13087-Related work-0,1911.13087-The BD-4SK-ASR Dataset ::: Phoeset-0,1911.13087-The BD-4SK-ASR Dataset ::: Filler phones-0,1911.13087-The BD-4SK-ASR Dataset ::: The File IDs-0,1911.13087-The BD-4SK-ASR Dataset ::: The Transcription-0,"The work on Automatic Speech Recognition (ASR) has a long history, but we could not retrieve any literature on Kurdish ASR at the time of compiling this article. However, the literature on ASR for different languages is resourceful. Also, researchers have widely used CMUSphinx for ASR though other technologies have been emerging in recent years BIBREF1.",The phoneset includes 34 phones for Sorani Kurdish. A sample of the file content is given below.,"The filler phone file usually contains fillers in spoken sentences. In our basic sentences, we have only considered silence. Therefore it only includes three lines to indicate the possible pauses at the beginning and end of the sentences and also after each word.","This file includes the list of files in which the narrated sentences have been recorded. The recorded files are in wav formats. However, in the file IDs, the extension is omitted. A sample of the file content is given below. The test directory is the directory in which the files are located.",This file contains the transcription of each sentence based on the phoneset along with the file ID in which the equivalent narration has been saved. The following is a sample of the content of the file.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How was the dataset collected?,Sample Answer,1911.13087-The BD-4SK-ASR Dataset ::: Phoeset-4,1911.13087-The BD-4SK-ASR Dataset ::: Phoeset-5,1911.13087-The BD-4SK-ASR Dataset ::: Phoeset-8,1911.13087-The BD-4SK-ASR Dataset ::: Phoeset-10,1911.13087-The BD-4SK-ASR Dataset ::: Filler phones-0,SIL,SH,W,"Figure FIGREF3 shows the Sorani letters in Persian-Arabic script, the suggested phoneme (capital English letters), and an example of the transformation of words in the developed corpus.","The filler phone file usually contains fillers in spoken sentences. In our basic sentences, we have only considered silence. Therefore it only includes three lines to indicate the possible pauses at the beginning and end of the sentences and also after each word.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How many annotators participated?,Sample Answer,1911.13087-The BD-4SK-ASR Dataset ::: Phoeset-4,1911.13087-The BD-4SK-ASR Dataset ::: Phoeset-5,1911.13087-The BD-4SK-ASR Dataset ::: Phoeset-8,1911.13087-The BD-4SK-ASR Dataset ::: Phoeset-10,1911.13087-The BD-4SK-ASR Dataset ::: Filler phones-0,SIL,SH,W,"Figure FIGREF3 shows the Sorani letters in Persian-Arabic script, the suggested phoneme (capital English letters), and an example of the transformation of words in the developed corpus.","The filler phone file usually contains fillers in spoken sentences. In our basic sentences, we have only considered silence. Therefore it only includes three lines to indicate the possible pauses at the beginning and end of the sentences and also after each word.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How many rules had to be defined?,Sample Answer,1909.05438-Introduction-0,1909.05438-Problem Statement-0,1909.05438-Meta-Learning-3,1909.05438-Table-Based Semantic Parsing-0,1909.05438-Conversational Table-Based Semantic Parsing-1,"Semantic parsing aims to map natural language questions to the logical forms of their underlying meanings, which can be regarded as programs and executed to yield answers, aka denotations BIBREF0 . In the past few years, neural network based semantic parsers have achieved promising performances BIBREF1 , however, their success is limited to the setting with rich supervision, which is costly to obtain. There have been recent attempts at low-resource semantic parsing, including data augmentation methods which are learned from a small number of annotated examples BIBREF2 , and methods for adapting to unseen domains while only being trained on annotated examples in other domains.","We focus on the task of executive semantic parsing. The goal is to map a natural language question/utterance INLINEFORM0 to a logical form/program INLINEFORM1 , which can be executed over a world INLINEFORM2 to obtain the correct answer INLINEFORM3 .","If we only have examples covered by rules, such as those used in the initialization phase, meta-learning learns to learn a good initial parameter that is evaluated by its usefulness on the examples from the same distribution. In the training phase, datapoints from both tasks are generated, and meta-learning learns to learn an initialization parameter which can be quickly and efficiently adapted to examples from both tasks.","Given a natural language INLINEFORM0 and a table INLINEFORM1 with INLINEFORM2 columns and INLINEFORM3 rows as the input, the task is to output a SQL query INLINEFORM4 , which could be executed on table INLINEFORM5 to yield the correct answer of INLINEFORM6 . We conduct experiments on WikiSQL BIBREF8 , which provides 87,726 annotated question-SQL pairs over 26,375 web tables. In this work, we do not use either SQL queries or answers in the training process. We use execution accuracy as the evaluation metric, which measures the percentage of generated SQL queries that result in the correct answer.","Given a natural language question at the current turn, a web table, and previous turn questions in a conversation as the input, the task aims to generate a program (i.e. logical form), which can be executed on the table to obtain the correct answer of the current turn question.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What dataset is used?,Sample Answer,1704.06960-Related work-3,1704.06960-Belief and behavior-8,1704.06960-Tasks-5,1704.06960-6-Figure5-1.png,1704.06960-7-Figure6-1.png,"Our evaluation in this paper investigates communication strategies that arise in a number of different games, including reference games and an extended-horizon driving game. Communication strategies for reference games were previously explored by Vogel13Grice, Andreas16Pragmatics and Kazemzadeh14ReferIt, and reference games specifically featuring end-to-end communication protocols by Yu16Reinforcer. On the control side, a long line of work considers nonverbal communication strategies in multiagent policies BIBREF9 .","sec:philosophy examined the consequences of a mismatch between the set of primitives available in two languages. In general we would like some measure of our approach's robustness to the lack of an exact correspondence between two languages. In the case of humans in particular we expect that a variety of different strategies will be employed, many of which will not correspond to the behavior of the learned agent. It is natural to want some assurance that we can identify the DCP's strategy as long as some human strategy mirrors it. Our second observation is that it is possible to exactly recover a translation of a DCP strategy from a mixture of humans playing different strategies:","Your goal is to drive the red car onto the red square. Be careful! You're driving in a thick fog, and there is another car on the road that you cannot see. However, you can talk to the other driver to make sure you both reach your destinations safely.","Figure 5: Simplified game representation used for analysis in Section 6. A speaker agent sends a message to a listener agent, which takes a single action and receives a reward.","Figure 6: Tasks used to evaluate the translation model. (a–b) Reference games: both players observe a pair of reference candidates (colors or images); Player a is assigned a target (marked with a star), which player b must guess based on a message from a. (c) Driving game: each car attempts to navigate to its goal (marked with a star). The cars cannot see each other, and must communicate to avoid a collision.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How much is performance improved on NLI?,Sample Answer,1909.03405-Method ::: Gathering More Document-level Information-1,1909.03405-Method ::: Gathering More Document-level Information-2,1909.03405-Experiment Settings-7,1909.03405-Results of MRC Tasks ::: SQuAD v1.1 and v2.0-0,1909.03405-Results of Chinese NLP Tasks-2,"Specifically, we also include the in-adjacent sentences in the sentence-pair classification task. The in-adjacent sentences next to the IsPrev and IsNext sentences are sampled, labeled as IsPrevInadj and IsNextInadj (cf. the bottom of Fig. FIGREF6). Note that these in-adjacent sentences will introduce much more training noise to the model. Therefore, the label smoothing technique is adopted to reduce the noise of these additional samples. It achieves this by relaxing our confidence on the labels, e.g., transforming the target probability from (1.0, 0.0) to (0.8, 0.2) in a binary classification problem.","In summary, when A is given, the pre-training example for each label is constructed as follows:","In the following, we first show that BERT is order-sensitive and the use of PSP remedies this problem. Then we provide experimental results on the NLI and MRC tasks to verify the effectiveness of the proposed method. At last, the proposed method is evaluated on several Chinese datasets.","We also evaluate our method on the MRC tasks. The Stanford Question Answering Dataset (SQuAD v1.1) is a question answering (QA) dataset, which consists of 100K samples BIBREF15. Each data sample has a question and a corresponding Wikipedia passage that contains the answer. The goal is to extract the answer from the passage for the given question.",LCQMC BIBREF20 is a dataset for sequence matching. A binary label is annotated for a sentence pair in the dataset to indicate whether these two sentences have the same intention.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Which unlabeled data do they pretrain with?,Sample Answer,1904.05862-Model-2,1904.05862-Data-1,1904.05862-Decoding-2,1904.05862-Ablations-1,1904.05862-7-Table5-1.png,"Next, we apply the context network INLINEFORM0 to the output of the encoder network to mix multiple latent representations INLINEFORM1 into a single contextualized tensor INLINEFORM2 for a receptive field size INLINEFORM3 . The context network has seven layers and each layer has kernel size three and stride one. The total receptive field of the context network is about 180ms.",To train the baseline acoustic model we compute 80 log-mel filterbank coefficients for a 25ms sliding window with stride 10ms. Final models are evaluated in terms of both word error rate (WER) and letter error rate (LER).,"For decoding WSJ, we tune the hyperparameters INLINEFORM0 , INLINEFORM1 and INLINEFORM2 using a random search. Finally, we decode the emissions from the acoustic model with the best parameter setting for INLINEFORM3 , INLINEFORM4 and INLINEFORM5 , and a beam size of 4000 and beam score threshold of 250.","Next, we analyze the effect of data augmentation through cropping audio sequences (§ SECREF11 ). When creating batches we crop sequences to a pre-defined maximum length. Table shows that a crop size of 150K frames results in the best performance. Not restricting the maximum length (None) gives an average sequence length of about 207K frames and results in the worst accuracy. This is most likely because the setting provides the least amount of data augmentation.",Table 5: Effect of different number of tasks K (cf. Table 3).,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How many convolutional layers does their model have?,Sample Answer,1904.05862-Results-0,1904.05862-Ablations-1,1904.05862-Ablations-2,1904.05862-6-Table3-1.png,1904.05862-7-Table5-1.png,"Different to BIBREF15 , we evaluate the pre-trained representations directly on downstream speech recognition tasks. We measure speech recognition performance on the WSJ benchmark and simulate various low resource setups (§ SECREF12 ). We also evaluate on the TIMIT phoneme recognition task (§ SECREF13 ) and ablate various modeling choices (§ SECREF14 ).","Next, we analyze the effect of data augmentation through cropping audio sequences (§ SECREF11 ). When creating batches we crop sequences to a pre-defined maximum length. Table shows that a crop size of 150K frames results in the best performance. Not restricting the maximum length (None) gives an average sequence length of about 207K frames and results in the worst accuracy. This is most likely because the setting provides the least amount of data augmentation.",Table shows that predicting more than 12 steps ahead in the future does not result in better performance and increasing the number of steps increases training time.,Table 3: Effect of different number of negative samples during pre-training for TIMIT on the development set.,Table 5: Effect of different number of tasks K (cf. Table 3).,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the size of their published dataset?,Sample Answer,1803.08614-Guidelines-0,1803.08614-Dataset Characteristics-0,1803.08614-Agreement Scores-7,1803.08614-Difficult Examples-0,1803.08614-Difficult Examples-1,"In the OpeNER annotation scheme (see Table TABREF8 for a short summary), an annotator reads a review and must first decide if there is any positive or negative attitudes in the sentence. If there are, they then decide if the sentence is on topic. Since these reviews are about hotels, we constrain the opinion targets and opinion expressions to those that deal with aspects of the hotel. Annotators should annotate the span of text which refers to:","The reviews are typical hotel reviews, which often mention various aspects of the hotel or experience and the polarity towards these aspects. An example is shown in Example","The agreement score for opinion holders is somewhat lower and stems from the fact that there were relatively few instances of explicit opinion holders. Additionally, Catalan and Basque both have agreement features for verbs, which could be considered an implicit mention of the opinion holder. This is not always clear, however. Finally, the mean squared error of the polarity scores shows that annotators generally agree on where and which polarity score should be given. Again, the mean squared error in this annotation scheme requires both annotators to choose the same span and the same polarity to achieve perfect agreement.","During annotation, there were certain sentences which presented a great deal of problems for the annotators. Many of these are difficult because of 1) nested opinions, 2) implicit opinions reported only through the presence or absence of certain aspects, or 3) the difficulty to identify the span of an expression. Here, we give examples of each difficulty and detail how these were resolved during the annotation process.","In the Basque sentence in Example UID18 , we can see that there are two distinct levels of aspects. First, the aspect `hotel', which has a positive polarity and then the sub-aspect `workers'. We avoid the problem of deciding which is the opinion target by treating these as two separate opinions, whose targets are `hotel' and `workers'.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what was the margin their system outperformed previous ones?,Sample Answer,1602.04341-Introduction-4,1602.04341-HABCNN-4,1602.04341-HABCNN-13,1602.04341-HABCNN Variants-2,1602.04341-Results-1,"In both roadmaps, convolutional neural network (CNN) is explored to model all types of text. As human beings usually do for such a QA task, our model is expected to be able to detect the key snippets, key sentences, and key words or phrases in the document. In order to detect those informative parts required by questions, we explore an attention mechanism to model the document so that its representation contains required information intensively. In practice, instead of imitating human beings in QA task top-down, our system models the document bottom-up, through accumulating the most relevant information from word level to snippet level.","Note that the sentence-CNNs for query and all document sentences share the same weights, so that the resulting sentence representations are comparable.","where INLINEFORM0 . With the same highway network, we can generate the overall query representation, INLINEFORM1 in Figure FIGREF3 , by combining the two representations of the query at sentence and snippet levels.","Variant-II: How to model attention at the granularity of words was shown in BIBREF2 ; see their paper for details. We develop their attention idea and model attention at the granularity of sentence and snippet. Our attention gives different weights to sentences/snippets (not words), then computes the document representation as a weighted average of all sentence/snippet representations.","As said before, both AR and NR systems aim to generate answers in entity form. Their designs might not suit this machine comprehension task, in which the answers are openly-formed based on summarizing or abstracting the clues. To be more specific, AR models D always at word level, attentions are also paid to corresponding word representations, which is applicable for entity-style answers, but is less suitable for comprehension at sentence level or even snippet level. NR contrarily models D in sentence level always, neglecting the discovering of key phrases which however compose most of answers. In addition, the attention of AR system and the question-fact interaction in NR system both bring large numbers of parameters, this potentially constrains their power in a dataset of limited size.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what rnn classifiers were used?,Sample Answer,1801.04433-Introduction-0,1801.04433-Related Work-0,1801.04433-Features-1,1801.04433-Dataset-0,1801.04433-Results-5,"Social media is a very popular way for people to express their opinions publicly and to interact with others online. In aggregation, social media can provide a reflection of public sentiment on various events. Unfortunately, many users engaging online, either on social media, forums or blogs, will often have the risk of being targeted or harassed via abusive language, which may severely impact their online experience and the community in general. The existence of social networking services creates the need for detecting user-generated hateful messages prior to publication. All published text that is used to express hatred towards some particular group with the intention to humiliate its members is considered a hateful message.","Simple word-based approaches, if used for blocking the posting of text or blacklisting users, not only fail to identify subtle offensive content, but they also affect the freedom of speech and expression. The word ambiguity problem – that is, a word can have different meanings in different contexts – is mainly responsible for the high false positive rate in such approaches. Ordinary NLP approaches on the other hand, are ineffective to detect unusual spelling, experienced in user-generated comment text. This is best known as the spelling variation problem, and it is caused either by unintentional or intentional replacement of single characters in a token, aiming to obfuscate the detectors.","Furthermore, we choose to model the input tweets in the form of vectors using word-based frequency vectorization. That is, the words in the corpus are indexed based on their frequency of appearance in the corpus, and the index value of each word in a tweet is used as one of the vector elements to describe that tweet. We note that this modelling choice provides us with a big advantage, because the model is independent of the language used for posting the message.","We experimented with a dataset of approximately 16k short messages from Twitter, that was made available by BIBREF12 . The dataset contains 1943 tweets labeled as Racism, 3166 tweets labeled as Sexism and 10889 tweets labeled as Neutral (i.e., tweets that neither contain sexism nor racism). There is also a number of dual labeled tweets in the dataset. More particularly, we found 42 tweets labeled both as both `Neutral' and `Sexism', while six tweets were labelled as both `Racism' and `Neutral'. According to the dataset providers, the labeling was performed manually.","We also present the performance of each of the tested models per class label in Table TABREF25 . Results by other researchers have not been included, as these figures are not reported in the existing literature. As can be seen, sexism is quite easy to classify in hate-speech, while racism seems to be harder; similar results were reported by BIBREF7 . This result is consistent across all ensembles.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what results did their system obtain?,Sample Answer,1801.04433-Introduction-0,1801.04433-Introduction-1,1801.04433-Introduction-3,1801.04433-Related Work-5,1801.04433-Deep learning model-1,"Social media is a very popular way for people to express their opinions publicly and to interact with others online. In aggregation, social media can provide a reflection of public sentiment on various events. Unfortunately, many users engaging online, either on social media, forums or blogs, will often have the risk of being targeted or harassed via abusive language, which may severely impact their online experience and the community in general. The existence of social networking services creates the need for detecting user-generated hateful messages prior to publication. All published text that is used to express hatred towards some particular group with the intention to humiliate its members is considered a hateful message.","Although hate speech is protected under the free speech provisions in the United States, there are other countries, such as Canada, France, United Kingdom, and Germany, where there are laws prohibiting it as being promoting violence or social disorder. Social media services such as Facebook and Twitter have been criticized for not having done enough to prohibit the use of their services for attacking people belonging to some specific race, minority etc. BIBREF0 . They have announced though that they would seek to battle against racism and xenophobia BIBREF1 . Nevertheless, the current solutions deployed by them have attempted to address the problem with manual effort, relying on users to report offensive comments BIBREF2 . This not only requires a huge effort by human annotators, but it also has the risk of applying discrimination under subjective judgment. Moreover, a non-automated task by human annotators would have strong impact on system response times, since a computer-based solution can accomplish this task much faster than humans. The massive rise in the user-generated content in the above social media services, with manual filtering not being scalable, highlights the need for automating the process of on-line hate-speech detection.","There is a plethora of unsupervised learning models in the existing literature to deal with hate-speech BIBREF3 , as well as in detecting the sentiment polarity in tweets BIBREF4 . At the same time, the supervised learning approaches have not been explored adequately so far. While the task of sentence classification seems similar to that of sentiment analysis; nevertheless, in hate-speech even negative sentiment could still provide useful insight. Our intuition is that the task of hate-speech detection can be further benefited by the incorporation of other sources of information to be used as features into a supervised learning model. A simple statistical analysis on an existing annotated dataset of tweets by BIBREF5 , can easily reveal the existence of significant correlation between the user tendency in expressing opinions that belong to some offensive class (Racism or Sexism), and the annotation labels associated with that class. More precisely, the correlation coefficient value that describes such user tendency was found to be 0.71 for racism in the above dataset, while that value reached as high as 0.76 for sexism. In our opinion, utilizing such user-oriented behavioural data for reinforcing an existing solution is feasible, because such information is retrieva2ble in real-world use-case scenarios like Twitter. This highlights the need to explore the user features more systematically to further improve the classification accuracy of a supervised learning system.","In spite of the high popularity of NLP approaches in hate-speech classification BIBREF3 , we believe there is still a high potential for deep learning models to further contribute to the issue. At this point it is also relevant to note the inherent difficulty of the challenge itself, which can be clearly noted by the fact that no solution thus far has been able to obtain an F-score above 0.93.",The Input (a.k.a Embedding) Layer. The input layer's size is defined by the number of inputs for that classifier. This number equals the size to the word vector plus the number of additional features. The word vector dimension was set to 30 so that to be able to encode every word in the vocabulary used.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Which baselines did they compare against?,Sample Answer,1809.02286-Related Work-5,1809.02286-Tree-LSTM-0,1809.02286-Structure-aware Tag Representation-1,1809.02286-Leaf-LSTM-2,1809.02286-SATA Tree-LSTM-7,"Finally, some recent works BIBREF10 , BIBREF11 have proposed latent tree-structured models that learn how to formulate tree structures from only sequences of tokens, without the aid of syntactic trees or linguistic information. The latent tree models have the advantage of being able to find the optimized task-specific order of composition rather than a sequential or syntactic one. In experiments, we compare our model with not only syntactic tree-based models but also latent tree models, demonstrating that modeling with explicit linguistic knowledge can be an attractive option.","The LSTM BIBREF1 architecture was first introduced as an extension of the RNN architecture to mitigate the vanishing and exploding gradient problems. In addition, several works have discovered that applying the LSTM cell into tree structures can be an effective means of modeling sentence representations.","One way of deriving structure-aware tag representations from the original tag embeddings is to introduce a separate tag-level tree-LSTM which accepts the typical tag embeddings at each node of a tree and outputs the computed structure-aware tag representations for the nodes. Note that the module concentrates on extracting useful syntactic features by considering only the tags and structures of the trees, excluding word information.","Formally, we denote a sequence of words in an input sentence as $w_{1:n}$ ( $n$ : the length of the sentence), and the corresponding word embeddings as $\mathbf {x}_{1:n}$ . Then, the operation of the leaf-LSTM at time $t$ can be formulated as, ","Then, the $\tilde{\mathbf {c}}_{t}$ and $\tilde{\mathbf {h}}_{t}$ can be utilized as input tokens to the word tree-LSTM, with the left (right) child of the target node corresponding to the $t$ th word in the input sentence. ",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What was the performance on the self-collected corpus?,Sample Answer,1909.06937-Introduction-10,1909.06937-CM-Net ::: Overview-0,1909.06937-3-Figure2-1.png,1909.06937-7-Figure4-1.png,1909.06937-7-Table4-1.png,Our CM-Net achieves the state-of-the-art results on two major SLU benchmarks (ATIS and SNIPS) in most of criteria.,"In this section, we start with a brief overview of our CM-Net and then proceed to introduce each module. As shown in Figure FIGREF16, the input utterance is firstly encoded with the Embedding Layer, and then is transformed by multiple CM-blocks with the assistance of slot and intent memories, and finally make predictions in the Inference Layer.","Figure 2: Overview of our proposed CM-Net. The input utterance is firstly encoded with the Embedding Layer (bottom), and then is transformed by multiple CM-blocks with the assistance of both slot and intent memories (on both sides). Finally we make predictions of slots and the intent in the Inference Layer (top).","Figure 4: Investigations of the collaborative retrieval approach on slot filling (on the left) and intent detection (on the right), where “no slot2int” indicates removing slow-aware attention for the intent representation, and similarly for “no int2slot” and “neither”.","Table 4: Ablation experiments on the SNIPS to investigate the impacts of various components, where “- slot memory” indicates removing the slot memory and its interactions with other components correspondingly. Similarly for the other options.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the size of their dataset?,Sample Answer,1909.06937-Introduction-1,1909.06937-Introduction-2,1909.06937-CM-Net ::: Overview-0,1909.06937-Analysis ::: Ablation Experiments-1,1909.06937-3-Figure2-1.png,"Intuitively, slots and intents from similar fields tend to occur simultaneously, which can be observed from Figure FIGREF2 and Table TABREF3. Therefore, it is beneficial to generate the representations of slots and intents with the guidance from each other. Some works explore enhancing the slot filling task unidirectionally with the guidance from intent representations via gating mechanisms BIBREF7, BIBREF8, while the predictions of intents lack the guidance from slots. Moreover, the capsule network with dynamic routing algorithms BIBREF9 is proposed to perform interactions in both directions. However, there are still two limitations in this model. The one is that the information flows from words to slots, slots to intents and intents to words in a pipeline manner, which is to some extent limited in capturing complicated correlations among words, slots and intents. The other is that the local context information which has been shown highly useful for the slot filling BIBREF10, is not explicitly modeled.","In this paper, we try to address these issues, and thus propose a novel $\mathbf {C}$ollaborative $\mathbf {M}$emory $\mathbf {N}$etwork, named CM-Net. The main idea is to directly capture semantic relationships among words, slots and intents, which is conducted simultaneously at each word position in a collaborative manner. Specifically, we alternately perform information exchange among the task-specific features referred from memories, local context representations and global sequential information via the well-designed block, named CM-block, which consists of three computational components:","In this section, we start with a brief overview of our CM-Net and then proceed to introduce each module. As shown in Figure FIGREF16, the input utterance is firstly encoded with the Embedding Layer, and then is transformed by multiple CM-blocks with the assistance of slot and intent memories, and finally make predictions in the Inference Layer.","Once the slot memory and its corresponding interactions with other components are removed, scores on both tasks decrease to some extent, and a more obvious decline occurs for the slot filling (row 1 vs. row 0), which is consistent with the conclusion of Section SECREF45. Similar observations can be found for the intent memory (row 2). The local calculation layer is designed to capture better local context representations, which has an evident impact on the slot filling and slighter effect on the intent detection (row 3 vs. row 0). Opposite observations occur in term of global recurrence, which is supposed to model global sequential information and thus has larger effect on the intent detection (row 4 vs. row 0).","Figure 2: Overview of our proposed CM-Net. The input utterance is firstly encoded with the Embedding Layer (bottom), and then is transformed by multiple CM-blocks with the assistance of both slot and intent memories (on both sides). Finally we make predictions of slots and the intent in the Inference Layer (top).",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What metrics are used?,Sample Answer,1911.07620-Introduction-6,1911.07620-Background and Related Work ::: Neural Networks for Text Classification-2,1911.07620-Background and Related Work ::: Neural Networks for Text Classification-3,1911.07620-Model ::: Identifying Security Vulnerabilities-3,1911.07620-Conclusions and Future Work-1,"RQ2: Does extracting class-level features before and after the change instead of using only the commit diff improve the identification of security-relevant commits? For this research question, we test the hypothesis that the source code of the entire Java class contains more information than just the commit diff and could potentially improve the performance of our model.","Deep neural networks are prone to overfitting due to the possibility of the network learning complicated relationships that exist in the training set but not in unseen test data. Dropout prevents complex co-adaptations of hidden units on training data by randomly removing (i.e. dropping out) hidden units along with their connections during training BIBREF10. Embedding dropout, used by BIBREF11 for neural language modeling, performs dropout on entire word embeddings. This effectively removes a proportion of the input tokens randomly at each training iteration, in order to condition the model to be robust against missing input.","While dropout works well for regularizing fully-connected layers, it is less effective for convolutional layers due to the spatial correlation of activation units in convolutional layers. There have been a number of attempts to extend dropout to convolutional neural networks BIBREF12. DropBlock is a form of structured dropout for convolutional layers where units in a contiguous region of a feature map are dropped together BIBREF13.","For RQ3, we adapt the code2vec model used by BIBREF16 for predicting method names into a model for predicting whether a commit is security-relevant by modifying the final layer. We then repeat our experiments on both the ground-truth and augmented dataset.","Network architectures that are effective on a certain task, such as predicting method names, are not necessarily effective on related tasks. Thus, choices between neural models should be made considering the nature of the task and the amount of training data available. Based on the model's ability to predict method names in files across different projects, BIBREF16 claim that code2vec can be used for a wide range of programming language processing tasks. However, for predicting the security relevance of commits, H-CNN and HR-CNN appear to be much better than code2vec.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How long is the dataset?,Sample Answer,1911.07620-Introduction-0,1911.07620-Introduction-5,1911.07620-Background and Related Work ::: Neural Networks for Text Classification-3,1911.07620-Model ::: Identifying Security Vulnerabilities-3,1911.07620-Results and Discussion-5,"The use of open-source software has been steadily increasing for some time now, with the number of Java packages in Maven Central doubling in 2018. However, BIBREF0 states that there has been an 88% growth in the number of vulnerabilities reported over the last two years. In order to develop secure software, it is essential to analyze and understand security vulnerabilities that occur in software systems and address them in a timely manner. While there exist several approaches in the literature for identifying and managing security vulnerabilities, BIBREF1 show that an effective vulnerability management approach must be code-centric. Rather than relying on metadata, efforts must be based on analyzing vulnerabilities and their fixes at the code level.","RQ1: Can we effectively identify security-relevant commits using only the commit diff? For this research question, we do not use any of the commit metadata such as the commit message or information about the author. We treat source code changes like unstructured text without using path-based representations from the abstract syntax tree.","While dropout works well for regularizing fully-connected layers, it is less effective for convolutional layers due to the spatial correlation of activation units in convolutional layers. There have been a number of attempts to extend dropout to convolutional neural networks BIBREF12. DropBlock is a form of structured dropout for convolutional layers where units in a contiguous region of a feature map are dropped together BIBREF13.","For RQ3, we adapt the code2vec model used by BIBREF16 for predicting method names into a model for predicting whether a commit is security-relevant by modifying the final layer. We then repeat our experiments on both the ground-truth and augmented dataset.",RQ3: Does exploiting path-based representations of the Java classes before and after the change improve the identification of security-relevant commits?,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What dataset do they use?,Sample Answer,1911.07620-Introduction-0,1911.07620-Introduction-1,1911.07620-Introduction-5,1911.07620-Introduction-13,1911.07620-Results and Discussion ::: Threats to Validity-0,"The use of open-source software has been steadily increasing for some time now, with the number of Java packages in Maven Central doubling in 2018. However, BIBREF0 states that there has been an 88% growth in the number of vulnerabilities reported over the last two years. In order to develop secure software, it is essential to analyze and understand security vulnerabilities that occur in software systems and address them in a timely manner. While there exist several approaches in the literature for identifying and managing security vulnerabilities, BIBREF1 show that an effective vulnerability management approach must be code-centric. Rather than relying on metadata, efforts must be based on analyzing vulnerabilities and their fixes at the code level.","Common Vulnerabilities and Exposures (CVE) is a list of publicly known cybersecurity vulnerabilities, each with an identification number. These entries are used in the National Vulnerability Database (NVD), the U.S. government repository of standards based vulnerability management data. The NVD suffers from poor coverage, as it contains only 10% of the open-source vulnerabilities that have received a CVE identifier BIBREF2. This could be due to the fact that a number of security vulnerabilities are discovered and fixed through informal communication between maintainers and their users in an issue tracker. To make things worse, these public databases are too slow to add vulnerabilities as they lag behind a private database such as Snyk's DB by an average of 92 days BIBREF0 All of the above pitfalls of public vulnerability management databases (such as NVD) call for a mechanism to automatically infer the presence of security threats in open-source projects, and their corresponding fixes, in a timely manner.","RQ1: Can we effectively identify security-relevant commits using only the commit diff? For this research question, we do not use any of the commit metadata such as the commit message or information about the author. We treat source code changes like unstructured text without using path-based representations from the abstract syntax tree.","We envision that this work would ultimately allow for monitoring open-source repositories in real-time, in order to automatically detect security-relevant changes such as vulnerability fixes.","The lexer and tokenizer we use from the javalang library target Java 8. We are not able to verify that all the projects and their forks in this study are using the same version of Java. However, we do not expect considerable differences in syntax between Java 7 and Java 8 except for the introduction of lambda expressions.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What was their perplexity score?,Sample Answer,1810.10254-Introduction-2,1810.10254-Related Work-0,1810.10254-Related Work-1,1810.10254-Conclusion-0,1810.10254-4-Figure2-1.png,"In this paper, we propose a novel language-agnostic method to learn how to generate code-switching sentences by using a pointer-generator network BIBREF7 . The model is trained from concatenated sequences of parallel sentences to generate code-switching sentences, constrained by code-switching texts. The pointer network copies words from both languages and pastes them into the output, generating code switching sentences in matrix language to embedded language and vice versa. The attention mechanism helps the decoder to generate meaningful and grammatical sentences without needing any sequence alignment. This idea is also in line with code-mixing by borrowing words from the embedded language BIBREF8 and intuitively, the copying mechanism can be seen as an end-to-end approach to translate, align, and reorder the given words into a grammatical code-switching sentence. This approach is the unification of all components in the work of BIBREF5 into a single computational model. A code-switching language model learned in this way is able to capture the patterns and constraints of the switches and mitigate the out-of-vocabulary (OOV) issue during sequence generation. By adding the generated sentences and incorporating syntactic information to the training data, we achieve better performance by INLINEFORM0 compared to an LSTM baseline BIBREF9 and INLINEFORM1 to the equivalent constraint.","The synthetic code-switching generation approach was introduced by adapting equivalence constraint on monolingual sentence pairs during the decoding step on an automatic speech recognition (ASR) model BIBREF5 . BIBREF10 explored Functional Head Constraint, which was found to be more restrictive than the Equivalence Constraint, but complex to be implemented, by using a lattice parser with a weighted finite-state transducer. BIBREF11 extended the RNN by adding POS information to the input layer and factorized output layer with a language identifier. Then, Factorized RNN networks were combined with an n-gram backoff model using linear interpolation BIBREF12 . BIBREF13 added syntactic and semantic features to the Factorized RNN networks. BIBREF14 adapted an effective curriculum learning by training a network with monolingual corpora of both languages, and subsequently train on code-switched data. A further investigation of Equivalence Constraint and Curriculum Learning showed an improvement in language modeling BIBREF6 . A multi-task learning approach was introduced to train the syntax representation of languages by constraining the language generator BIBREF9 .","A copy mechanism was proposed to copy words directly from the input to the output using an attention mechanism BIBREF15 . This mechanism has proven to be effective in several NLP tasks including text summarization BIBREF7 , and dialog systems BIBREF16 . The common characteristic of these tasks is parts of the output are exactly the same as the input source. For example, in dialog systems the responses most of the time have appeared in the previous dialog steps.","We introduce a new learning method for code-switching sentence generation using a parallel monolingual corpus that is applicable to any language pair. Our experimental result shows that adding generated sentences to the training data, effectively improves our model performance. Combining the generated samples with code-switching dataset reduces perplexity. We get further performance gain after using syntactic information of the input. In future work, we plan to explore reinforcement learning for sequence generation and employ more parallel corpora.","Fig. 2. Univariate data distribution for unigram (topleft), bigram (top-right), trigram (bottom-left), and fourgram (bottom-right). The showed n-grams are sampled from 3-best data pointer-generator model.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What parallel corpus did they use?,Sample Answer,1810.10254-Introduction-0,1810.10254-Results-2,1810.10254-Results-3,1810.10254-2-Figure1-1.png,1810.10254-4-Figure2-1.png,"Language mixing has been a common phenomenon in multilingual communities. It is motivated in response to social factors as a way of communication in a multicultural society. From a sociolinguistic perspective, individuals do code-switching in order to construct an optimal interaction by accomplishing the conceptual, relational-interpersonal, and discourse-presentational meaning of conversation BIBREF0 . In its practice, the variation of code-switching will vary due to the traditions, beliefs, and normative values in the respective communities. A number of studies BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 found that code-switching is not produced indiscriminately, but follows syntactic constraints. Many linguists formulated various constraints to define a general rule for code-switching BIBREF1 , BIBREF3 , BIBREF4 . However, the constraints are not enough to make a good generalization of real code-switching constraints, and they have not been tested in large-scale corpora for many language pairs.","Importance of Linguistic Constraint: The result in Table TABREF9 emphasizes that linguistic constraints have some significance in replicating the real code-switching patterns, specifically the equivalence constraint. There is a slight reduction in perplexity around 6 points on the test set. In addition, when we ignore the constraint, we lose performance because it still allows switches in the inversion grammar cases.","Does the pointer-generator learn how to switch? We found that our pointer-generator model generates sentences that have not been seen before. The example in Figure FIGREF1 shows that our model is able to construct a new well-formed sentence such as “我们要去(We want to) check"". It is also shown that the pointer-generator model has the capability to learn the characteristics of the linguistic constraints from data without any word alignment between the matrix and embedded languages. On the other hand, training using 3-best data obtains better performance compared to 1-best data. We found a positive correlation from Table TABREF6 , where 3-best data is more similar to the test set in terms of segment length and number of switches compared to 1-best data. Adding more samples INLINEFORM0 may improve the performance, but it will be saturated at a certain point. One way to solve this is by using more parallel samples.",Fig. 1. Pointer Generator Networks [8]. The figure shows the example of input and 3-best generated sentences.,"Fig. 2. Univariate data distribution for unigram (topleft), bigram (top-right), trigram (bottom-left), and fourgram (bottom-right). The showed n-grams are sampled from 3-best data pointer-generator model.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Do single-language BERT outperforms multilingual BERT?,Sample Answer,1908.09892-Introduction-2,1908.09892-Introduction-3,1908.09892-Structure-dependent agreement relations-2,1908.09892-Structure-dependent agreement relations-10,1908.09892-Data-0,"BIBREF17 adapted the experimental setup of BIBREF13, BIBREF11 and BIBREF18 to use the cloze test to assess BERT's sensitivity to number agreement in English subject-verb agreement relations. The results showed that the single-language BERT model performed surprisingly well at this task (above 80% accuracy in all experiments), even when there were multiple “distractors” in the sentence (other nouns that differed from the subject in number). This suggests that BERT is actually learning to approximate structure-dependent computation, and not simply relying on flawed heuristics.","However, English subject-verb agreement is a rather restricted phenomenon, with the majority of verbs having only two inflected forms and only one morphosyntactic feature (number) involved. To what extent does Goldberg's BIBREF17 result hold for subject-verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations? Building on Goldberg's BIBREF17 work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging examples.",The keys to the door are on the table.,"Previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject-verb agreement in number BIBREF17, BIBREF11, BIBREF13. In our work, we study all four types of agreement relations and all four features discussed above. Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model BIBREF23, BIBREF17, BIBREF24, BIBREF25, BIBREF26, BIBREF27. We expand this line of work to 26 languages. Not all languages in our sample exhibit all four types of agreement nor use all four features examined, but they all exhibit at least one of the agreement types involving at least one of the features.","Our study requires two types of data. First, we need sentences containing agreement relations. We mask out one of the words in the agreement relation and ask BERT to predict the masked word. We are interested in BERT's ability to predict words that respect the agreement relation, that is, words which share the morphosyntactic features of the word with which it agrees. To measure this, we need to know the feature values for each word in BERT's vocabulary. This is our second type of data. Throughout this paper, we refer to the first type of data as the cloze data, and the second as the feature data.",1.0,1.0,1.0,1.0,1.0,0.20000000000000004,0.2,0.2
What types of agreement relations do they explore?,Sample Answer,1908.09892-Introduction-3,1908.09892-Data ::: Feature data-1,1908.09892-Results-1,1908.09892-Conclusions & future work-0,1908.09892-6-Figure4-1.png,"However, English subject-verb agreement is a rather restricted phenomenon, with the majority of verbs having only two inflected forms and only one morphosyntactic feature (number) involved. To what extent does Goldberg's BIBREF17 result hold for subject-verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations? Building on Goldberg's BIBREF17 work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging examples.","In this work, a word can take on a particular bundle of feature values (e.g. singular, feminine and third person) if it appears with those features in either UD or UniMorph. The UniMorph data directly specifies what bundles of feature values a word can take on. For the Universal Dependencies data, we say a word can take on a particular bundle if we ever see it with that bundle of feature values in a Universal Dependencies corpus for that language. Both sources individually allow for a word to have multiple feature bundles (e.g. sheep in English can be singular or plural). In these cases, we keep all possible feature bundles. Finally, we filter out words that do not appear in BERT's vocabulary.","In figure FIGREF14, we see BERT's performance for each language. BERT performs well for the majority of languages, although some fare much worse than others. It is important to note that it is an unfair comparison because even though the datasets were curated using the same methodology, each language's dataset is different. It is possible, for example, that the examples we have for Basque are simply harder than they are for Portuguese.","Core linguistic phenomena depend on syntactic structure. Yet current state-of-the-art models in language representations, such as BERT, do not have explicit syntactic structural representations. Previous work by BIBREF17 showed that BERT captures English subject-verb number agreement well despite this lack of explicit structural representation. We replicated this result using a different evaluation methodology that addresses shortcomings in the original methodology and expanded the study to 26 languages. Our study further broadened existing work by considering the most cross-linguistically common agreement types as well as the most common morphosyntactic features. The main result of this expansion into more languages, types and features is that BERT, without explicit syntactic structure, is still able to capture syntax-sensitive agreement patterns well. However, our analysis highlights an important qualification of this result. We showed that BERT's ability to model syntax-sensitive agreement relations decreases slightly as the dependency becomes longer range, and as the number of distractors increases. We release our new curated cross-linguistic datasets and code in the hope that it is useful to future research that may probe why this pattern appears.","Figure 4: Accuracy as a function of number of distractors (other nouns in the sentence with different feature values), aggregated across all languages and agreement types. As with distance, BERT is quite robust to distractors although there is a more noticeable decrease in accuracy as more distractors are present. Error bars are bootstrapped 95% confidence intervals.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How much F1 was improved after adding skip connections?,Sample Answer,1912.10435-Methods ::: BERTQA - Directed Coattention-1,1912.10435-Methods ::: BERTQA - Directed Coattention-2,1912.10435-Methods ::: BERTQA - Directed Coattention-4,1912.10435-Results and Analysis-6,1912.10435-7-Figure4-1.png,"The BERT embeddings are masked to produce seperate query and context embedding vectors (Equations DISPLAY_FORM3 , DISPLAY_FORM4).","Where E is the contextualized embeddings derived from BERT, m is the mask, and c and q are the context and query respectively.","Where Q, K, and V are the query, key and value vectors.",Figure FIGREF26 shows an example cropped context and question that our ensemble model answers correctly while the BERT large model answers incorrectly. It seems that the BERT large model combined the words spirit and Christian to answer this question even thought the word spirit belongs to martial and the word Christian belongs to piety. Our model was able to keep the paired words together and realize that the question has no answer. We believe that our model was able to get the correct answer because of the coattention which is able to keep the words paired together correctly.,Figure 4: Visualization of attention produced by our model,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are state-of-the-art baselines?,Sample Answer,1909.00786-Introduction-2,1909.00786-Methodology ::: Utterance-Table Encoder-0,1909.00786-Methodology ::: Query Editing Mechanism-2,1909.00786-Related Work-3,1909.00786-Experimental Results ::: Effect of Query Editing-0,"To exploit the correlation between sequentially generated queries and generalize the system to different domains, in this paper, we study an editing-based approach for cross-domain context-dependent text-to-SQL generation task. We propose query generation by editing the query in the previous turn. To this end, we first encode the previous query as a sequence of tokens, and the decoder computes a switch to change it at the token level. This sequence editing mechanism models token-level changes and is thus robust to error propagation. Furthermore, to capture the user utterance and the complex database schemas in different domains, we use an utterance-table encoder based on BERT to jointly encode the user utterance and column headers with co-attention, and adopt a table-aware decoder to perform SQL generation with attentions over both the user utterance and column headers.","An effective encoder captures the meaning of user utterances, the structure of table schema, and the relationship between the two. To this end, we build an utterance-table encoder with co-attention between the two as illustrated in Figure FIGREF7.","Based on this observation, we extend our table-ware decoder with a query editing mechanism. We first encode the previous query using another bi-LSTM, and its hidden states are the query token embeddings $\mathbf {h}^{Q}_{i,j^{\prime }}$ (i.e., the $j^{\prime }$-th token of the $i$-th query). We then extend the context vector with the attention to the previous query:","Concurrent with our work, yu2019cosql introduced CoSQL, a large-scale cross-domain conversational text-to-SQL corpus collected under the Wizard-of-Oz setting. Each dialogue in CoSQL simulates a DB querying scenario with a crowd worker as a user and a college computer science student who is familiar with SQL as an expert. Question-SQL pairs in CoSQL reflect greater diversity in user backgrounds compared to other corpora and involve frequent changes in user intent between pairs or ambiguous questions that require user clarification. These features pose new challenges for text-to-SQL systems.","We further investigate the effect of our query editing mechanism. To this end, we apply editing from both the gold query and the predicted query on our model with or without the utterance-table BERT embedding. We also perform an ablation study to validate the contribution of query attention and sequence editing separately.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Which methods are considered to find examples of biases and unwarranted inferences??,Sample Answer,1605.06083-Introduction-0,1605.06083-Linguistic bias-2,1605.06083-Unwarranted inferences-3,1605.06083-Unwarranted inferences-4,1605.06083-3-Table1-1.png,"The Flickr30K dataset BIBREF0 is a collection of over 30,000 images with 5 crowdsourced descriptions each. It is commonly used to train and evaluate neural network models that generate image descriptions (e.g. BIBREF2 ). An untested assumption behind the dataset is that the descriptions are based on the images, and nothing else. Here are the authors (about the Flickr8K dataset, a subset of Flickr30K):","One well-studied example BIBREF4 , BIBREF5 is sexist language, where the sex of a person tends to be mentioned more frequently if their role or occupation is inconsistent with `traditional' gender roles (e.g. female surgeon, male nurse). Beukeboom also notes that adjectives are used to create “more narrow labels [or subtypes] for individuals who do not fit with general social category expectations” (p. 3). E.g. tough woman makes an exception to the `rule' that women aren't considered to be tough.",Many dark-skinned individuals are called African-American regardless of whether the picture has been taken in the USA or not (4280272). And people who look Asian are called Chinese (1434151732) or Japanese (4834664666).,"In image 4183120 (Figure FIGREF16 ), people sitting at a gym are said to be watching a game, even though there could be any sort of event going on. But since the location is so strongly associated with sports, crowdworkers readily make the assumption.","Table 1: Number of times ethnicity/race was mentioned per category, per image. The average is expressed as a percentage of the number of descriptions. Counts in the last column correspond to the number of descriptions containing an ethnic/racial marker. Images were found by looking for descriptions matching",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What biases are found in the dataset?,Sample Answer,1605.06083-Introduction-2,1605.06083-Unwarranted inferences-0,1605.06083-Unwarranted inferences-2,1605.06083-Unwarranted inferences-3,1605.06083-3-Table1-1.png,What this assumption overlooks is the amount of interpretation or recontextualization carried out by the annotators. Let us take a concrete example. Figure FIGREF1 shows an image from the Flickr30K dataset.,"Unwarranted inferences are statements about the subject(s) of an image that go beyond what the visual data alone can tell us. They are based on additional assumptions about the world. After inspecting a subset of the Flickr30K data, I have grouped these inferences into six categories (image examples between parentheses):","We've seen an example of this in the introduction, where the `manager' was said to be talking about job performance and scolding [a worker] in a stern lecture (8063007).",Many dark-skinned individuals are called African-American regardless of whether the picture has been taken in the USA or not (4280272). And people who look Asian are called Chinese (1434151732) or Japanese (4834664666).,"Table 1: Number of times ethnicity/race was mentioned per category, per image. The average is expressed as a percentage of the number of descriptions. Counts in the last column correspond to the number of descriptions containing an ethnic/racial marker. Images were found by looking for descriptions matching",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are two baseline methods?,Sample Answer,1911.01770-Introduction-0,1911.01770-Materials and Methods ::: Loss function-4,1911.01770-Materials and Methods ::: Loss function-5,1911.01770-Experimental Setup and Results-1,1911.01770-Experimental Setup and Results-5,"Social media and designated online cooking platforms have made it possible for large populations to share food culture (diet, recipes) by providing a vast amount of food-related data. Despite the interest in food culture, global eating behavior still contributes heavily to diet-related diseases and deaths, according to the Lancet BIBREF0. Nutrition assessment is a demanding, time-consuming and expensive task. Moreover, the conventional approaches for nutrition assessment are cumbersome and prone to errors. A tool that enables users to easily and accurately estimate the nutrition content of a meal, while at the same time minimize the need for tedious work is of great importance for a number of different population groups. Such a tool can be utilized for promoting a healthy lifestyle, as well as to support patients suffering food-related diseases such as diabetes. To this end, a number of computer vision approaches have been developed, in order to extract nutrient information from meal images by using machine learning. Typically, such systems detect the different food items in a picture BIBREF1, BIBREF2, BIBREF3, estimate their volumes BIBREF4, BIBREF5, BIBREF6 and calculate the nutrient content using a food composition database BIBREF7. In some cases however, inferring the nutrient content of a meal from an image can be really challenging - due to unseen ingredients (e.g. sugar, oil) or the structure of the meal (mixed food, soups, etc.).","with $c_r$ and $c_v$ as semantic recipe-class and semantic image-class, respectively, while $c_r=c_v$ if the food image and recipe text are a positive pair.","For the triplet loss, we define $\phi ^q$ as query embedding, $\phi ^{d+}$ as matching image counterpart and $\phi ^{d-}$ as another random sample taken from $S$. Further $\phi ^{d_{sem}+} \in S \wedge \phi ^{d_{sem}+} \ne \phi ^{d(q)}$ is a sample from $S$ sharing the same semantic class as $\phi ^q$ and $\phi ^{d_{sem}-}$ is a sample from any other class. The triplet loss is formulated as follows:","Similarly to BIBREF19 and BIBREF17, we evaluated our model on 10 subsets of 1000 samples each. One sample of these subsets is composed of text embedding and image embedding in the shared latent space. Since our interest lies in the recipe retrieval task, we optimized and evaluated our model by using each image embedding in the subsets as query against all text embeddings. By ranking the query and the candidate embeddings according to their cosine distance, we estimate the median rank. The model's performance is best, if the matching text embedding is found at the first rank. Further, we estimate the recall percentage at the top K percent over all queries. The recall percentage describes the quantity of queries ranked amid the top K closest results. In Table TABREF11 the results are presented, in comparison to baseline methods.","We use heat maps on instruction words as tool to visualize words relevant to ingredient-lists in plain instruction text. In Figure FIGREF15, we demonstrate how easily we can achieve insight into the models decision making.",1.0,1.0,1.0,1.0,1.0,0.22222222222222224,0.25,0.2
How does model compare to the baselines?,Sample Answer,1911.01770-Introduction-0,1911.01770-Introduction-1,1911.01770-Introduction-3,1911.01770-Materials and Methods ::: Attention Mechanisms-0,1911.01770-Conclusions-0,"Social media and designated online cooking platforms have made it possible for large populations to share food culture (diet, recipes) by providing a vast amount of food-related data. Despite the interest in food culture, global eating behavior still contributes heavily to diet-related diseases and deaths, according to the Lancet BIBREF0. Nutrition assessment is a demanding, time-consuming and expensive task. Moreover, the conventional approaches for nutrition assessment are cumbersome and prone to errors. A tool that enables users to easily and accurately estimate the nutrition content of a meal, while at the same time minimize the need for tedious work is of great importance for a number of different population groups. Such a tool can be utilized for promoting a healthy lifestyle, as well as to support patients suffering food-related diseases such as diabetes. To this end, a number of computer vision approaches have been developed, in order to extract nutrient information from meal images by using machine learning. Typically, such systems detect the different food items in a picture BIBREF1, BIBREF2, BIBREF3, estimate their volumes BIBREF4, BIBREF5, BIBREF6 and calculate the nutrient content using a food composition database BIBREF7. In some cases however, inferring the nutrient content of a meal from an image can be really challenging - due to unseen ingredients (e.g. sugar, oil) or the structure of the meal (mixed food, soups, etc.).","Humans often use information from diverse sensory modalities (visual, auditory, haptic) to infer logical conclusions. This kind of multi-sensory integration helps us process complex tasks BIBREF8. In this study, we investigate the use of recipe information, in order to better estimate nutrient content of complex meal compositions. With the aim to develop a pipeline for holistic dietary assessment, we present and evaluate a method based on machine learning to retrieve recipe information from images, as a first step towards more accurate nutrient estimation. Such recipe information can then be utilized together with the volume of the food item to enhance an automatic system to estimate the nutrient content of complex meals, such as lasagna, crock pot or stew.","The emergence of multi-modal databases has led to novel approaches for meal image analysis. The fusion of visual features learned from images by deep Convolution Neural Networks (CNN) and textual features lead to outstanding results in food recognition applications. An early approach for recipe retrieval was based on jointly learning to predict food category and its ingredients using deep CNN BIBREF16. In a following step, the predicted ingredients are matched against a large corpus of recipes. More recent approach is proposed by BIBREF15 and is based on jointly learning recipe-text and image representations in a shared latent space. Recurrent Neural Networks (RNN) and CNN are mainly used to map text and image into the shared space. To align the text and image embedding vectors between matching recipe-image pairs, cosine similarity loss with margin was applied. Carvalho et al. BIBREF17 proposed a similar multi-modal embedding method for aligning text and image representations in a shared latent space. In contrast to Salvador et al. BIBREF15, they formulated a joint objective function which incorporates the loss for the cross-modal retrieval task and a classification loss, instead of using the latent space for a multitask learning setup. To address the challenge of encoding long sequences (like recipe instructions), BIBREF15 chose to represent single instructions as sentence embedding using the skip-thought technique BIBREF18. These encoded instruction sentences are referred to as skip-instructions and their embedding is not fine tuned when learning the image-text joint embedding.","The instruction encoder follows a transformer based encoder, as suggested by BIBREF20. Since we do not focus on syntactic rules, but mostly on weak sentence semantics or single words, we built a more shallow encoder containing only 2 stacked layers, where each of this layers contains two sub-layers. The first is the multi-head attention layer, and the second is a position-wise densely connected feed-forward network (FFN). Due to recipes composed of over 600 words as instructions, we decided to trim words per instruction sentence to restrict the overall words per recipe to 300. In order to avoid removing complete instructions at the end of the instruction table, we removed a fraction of words from each instruction, based on this instruction's length and the overall recipe-instruction length. This strategy reinforces the neglect of syntactic structures in the instruction encoding process. With such a model, we can directly perform the instruction encoding during the learning process for the joint embedding, thus saving training time and reducing disk space consumption. The transformer-like encoder does not make use of any recurrent units, thus providing the opportunity for a more lightweight architecture. By using self-attention BIBREF20, the model learns to focus on instructions relevant to recipe-retrieval-relevant, parts of instructions or single instruction-words. Furthermore we gain insight into which instructions are important to distinguish recipes with similar ingredients but different preparation styles.","In this paper, we have introduced self-attention for instruction encoding in the context of the recipe retrieval task and ingredient attention for disclosing ingredient dependent meal preparation steps. Our main contribution is the aforementioned ingredient attention, empowering our model to solve the recipe retrieval without any upstream skip instruction embedding, as well as the light-weight architecture provided by the transformer-like instruction encoder. On the recipe retrieval task, our method performs similarly to our baseline implementation of BIBREF17. Regarding training time on the other hand, we increased the efficiency significantly for cross-modal based retrieval methods. There is no need for a maximum number of instructions for a recipe to be considered as valid for training or testing; only for total words, making more samples of the large Recipe1M corpus usable for training. Through ingredient attention, we are able to unveil internal focus in the text processing path by observing attention weights. Incorporation of new samples in the train set can be done by retraining just one model. Overall, an accurate and flexible method for recipe retrieval from meal images could provide downstream models (e.g. automatic nutrient content estimation) with decisive information and significantly improve their results.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
"How many texts/datapoints are in the SemEval, TASS and SENTIPOLC datasets?",Sample Answer,1612.05270-Cross-language Features-4,1612.05270-Cross-language Features-5,1612.05270-Parameter Optimization-2,1612.05270-Performance on sentiment analysis contests-1,1612.05270-7-Figure1-1.png,"N-words (word sequences) are widely used in many NLP tasks, and they have also been used in Sentiment Analysis BIBREF9 and BIBREF10 . To compute the N-words, the text is tokenized and N-words are calculated from tokens. For example, let INLINEFORM0 be the text, so its 1-words (unigrams) are each word alone, and its 2-words (bigrams) set are the sequences of two words, the set ( INLINEFORM1 ), and so on. INLINEFORM2 = {the lights, lights and, and shadows, shadows of, of your, your future}, so, given text of size INLINEFORM3 words, we obtain a set containing at most INLINEFORM4 elements. Generally, N-words are used up to 2 or 3-words because it is uncommon to find, between texts, good matches of word sequences greater than three or four words BIBREF11 .","In addition to the traditional N-words representation, we represent the resulting text as q-grams. A q-grams is an agnostic language transformation that consists in representing a document by all its substring of length INLINEFORM0 . For example, let INLINEFORM1 be the text, its 3-grams set are INLINEFORM2 ","To guarantee a better or equal performance than random search, the H+M process starts with the best configuration found in the random search. By using H+M, sample size can be set to 32 or 64, as rule of thumb, and even reach improvements in most cases (see § SECREF4 ). Nonetheless, this simplification and performance boosting comes along with possible higher optimization times. Finally, the performance of each configuration is obtained using a cross-validation technique on the training data, and the metrics are usually used in classification such as: accuracy, score INLINEFORM0 , and recall, among others.","Figures UID18 - UID21 illustrates the results on each challenge, all competitors are ordered in score's descending order (higher is better). The achieved performance of our approach is marked with a horizontal line on each figure. Figure UID22 briefly describes each challenge and summarizes our performance on each contest; also, we added three standard measures to simplify the insight's creation of the reader.",Figure 1: The performance listing in four difference challenges. The horizontal lines appearing in a) to d) correspond to B4MSA’s performance. All scores were computed using the official gold-standard and the proper score for each challenge.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
In which languages did the approach outperform the reported results?,Sample Answer,1612.05270-Introduction-3,1612.05270-Cross-language Features-2,1612.05270-Language Dependent Features-1,1612.05270-Text Representation-0,1612.05270-Performance on sentiment analysis contests-1,"The rest of the manuscript is organized as follows. Section SECREF2 describes our proposed Sentiment Analysis method. Section SECREF3 describes the datasets and contests used to test our approach; whereas, the experimental results, and, the discussion are presented on Section SECREF4 . Finally, Section SECREF5 concludes.","We classified around 500 most popular emoticons, included text emoticons, and the whole set of unicode emoticons (around INLINEFORM0 ) defined by BIBREF8 into three classes: positive, negative and neutral, which are grouped under its corresponding polarity word defined by the class name.","In many languages, there is a set of extremely common words such as determiners or conjunctions ( INLINEFORM0 or INLINEFORM1 ) which help to build sentences but do not carry any meaning for themselves. These words are known as Stopwords, and they are removed from text before any attempt to classify them. Generally, a stopword list is built using the most frequent terms from a huge document collection. We used the Spanish, English and Italian stopword lists included in the NLTK Python package BIBREF6 in order to identify them.","After text-transformations, it is needed to represent the text in suitable form in order to use a traditional classifier such as SVM. It was decided to select the well known vector representation of a text given its simplicity and powerful representation. Particularly, it is used the Term Frequency-Inverse Document Frequency which is a well-known weighting scheme in NLP. TF-IDF computes a weight that represents the importance of words or terms inside a document in a collection of documents, i.e., how frequently they appear across multiple documents. Therefore, common words such as the and in, which appear in many documents, will have a low score, and words that appear frequently in a single document will have high score. This weighting scheme selects the terms that represent a document.","Figures UID18 - UID21 illustrates the results on each challenge, all competitors are ordered in score's descending order (higher is better). The achieved performance of our approach is marked with a horizontal line on each figure. Figure UID22 briefly describes each challenge and summarizes our performance on each contest; also, we added three standard measures to simplify the insight's creation of the reader.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what dataset was used?,Sample Answer,1710.09589-Introduction-0,1710.09589-Introduction-1,1710.09589-Task Description-0,1710.09589-Task Description-1,1710.09589-4-Table3-1.png,"Customer feedback analysis is the task of classifying short text messages into a set of predefined labels (e.g., bug, request). It is an important step towards effective customer support.","However, a real bottleneck for successful classification of customer feedback in a multilingual environment is the limited transferability of such models, i.e., typically each time a new language is encountered a new model is built from scratch. This is clearly impractical, as maintaining separate models is cumbersome, besides the fact that existing annotations are simply not leveraged.","The customer feedback analysis task BIBREF5 is a short text classification task. Given a customer feedback message, the goal is to detect the type of customer feedback. For each message, the organizers provided one or more labels. To give a more concrete idea of the data, the following are examples of the English dataset:",“Still calls keep dropping with the new update” (bug),"Table 3: Results on the test data, weighted F1. MONOLING: monolingual models. MULTILING: the multilingual ALL-IN-1 model. TRANS: translated targets to English and classified with EN model.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the relationship between author and emotional valence?,Sample Answer,1605.05195-Introduction-0,1605.05195-Spatial-0,1605.05195-Spatial-2,1605.05195-Temporal-0,1605.05195-Results-2,Twitter is a micro-blogging platform and a social network where users can publish and exchange short messages of up to 140 characters long (also known as tweets). Twitter has seen a great rise in popularity in recent years because of its availability and ease-of-use. This rise in popularity and the public nature of Twitter (less than 10% of Twitter accounts are private BIBREF0 ) have made it an important tool for studying the behaviour and attitude of people.,"All of the 18 million tweets in our dataset originate from the USA and are geo-tagged. Naturally, the tweets are not evenly distributed across the 50 states given the large variation between the population of each state. Figure FIGREF25 shows the percentage of tweets per state, sorted from smallest to largest. Not surprisingly, California has the highest number of tweets ( INLINEFORM0 ), and Wyoming has the lowest number of tweets ( INLINEFORM1 ).","It is interesting to note that even with the noisy dataset, our ranking of US states based on their Twitter sentiment correlates with the ranking of US states based on the well-being index calculated by Oswald and Wu BIBREF25 in their work on measuring well-being and life satisfaction across America. Their data is from the behavioral risk factor survey score (BRFSS), which is a survey of life satisfaction across the United States from INLINEFORM0 million citizens. Figure FIGREF27 shows this correlation ( INLINEFORM1 , INLINEFORM2 ).","We looked at three temporal variables: time of day, day of the week and month. All tweets are tagged with timestamp data, which we used to extract these three variables. Since all timestamps in the Twitter historical archives (and public API) are in the UTC time zone, we first converted the timestamp to the local time of the location where the tweet was sent from. We then calculated the sentiment for each day of week (figure FIGREF29 ), hour (figure FIGREF30 ) and month (figure FIGREF31 ), averaged across all 18 million tweets over three years. The 18 million tweets were divided evenly between each month, with INLINEFORM0 million tweets per month. The tweets were also more or less evenly divided between each day of week, with each day having somewhere between INLINEFORM1 and INLINEFORM2 of the tweets. Similarly, the tweets were almost evenly divided between each hour, with each having somewhere between INLINEFORM3 and INLINEFORM4 of the tweets.","Table TABREF36 shows the precision, recall and F1 score of the positive and negative class for the full contextual classifier (Contextual-All).",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the relationship between time and emotional valence?,Sample Answer,1605.05195-Introduction-0,1605.05195-Spatial-0,1605.05195-Spatial-1,1605.05195-Spatial-2,1605.05195-8-Figure7-1.png,Twitter is a micro-blogging platform and a social network where users can publish and exchange short messages of up to 140 characters long (also known as tweets). Twitter has seen a great rise in popularity in recent years because of its availability and ease-of-use. This rise in popularity and the public nature of Twitter (less than 10% of Twitter accounts are private BIBREF0 ) have made it an important tool for studying the behaviour and attitude of people.,"All of the 18 million tweets in our dataset originate from the USA and are geo-tagged. Naturally, the tweets are not evenly distributed across the 50 states given the large variation between the population of each state. Figure FIGREF25 shows the percentage of tweets per state, sorted from smallest to largest. Not surprisingly, California has the highest number of tweets ( INLINEFORM0 ), and Wyoming has the lowest number of tweets ( INLINEFORM1 ).","Even the state with the lowest percentage of tweets has more than ten thousand tweets, which is enough to calculate a statistically significant average sentiment for that state. The sentiment for all states averaged across the tweets from the three years is shown in Figure FIGREF26 . Note that an average sentiment of INLINEFORM0 means that all tweets were labelled as positive, INLINEFORM1 means that all tweets were labelled as negative and INLINEFORM2 means that there was an even distribution of positive and negative tweets. The average sentiment of all the states leans more towards the positive side. This is expected given that INLINEFORM3 of the tweets in our dataset were labelled as positive.","It is interesting to note that even with the noisy dataset, our ranking of US states based on their Twitter sentiment correlates with the ranking of US states based on the well-being index calculated by Oswald and Wu BIBREF25 in their work on measuring well-being and life satisfaction across America. Their data is from the behavioral risk factor survey score (BRFSS), which is a survey of life satisfaction across the United States from INLINEFORM0 million citizens. Figure FIGREF27 shows this correlation ( INLINEFORM1 , INLINEFORM2 ).",Figure 7: Number of users (logarithmic) in bins of 50 tweets. The first bin corresponds to number of users that have less than 50 tweets throughout the three years and so on.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the five different binary classification tasks?,Sample Answer,1904.04358-Joint variability of electrodes-0,1904.04358-Deep autoencoder for spatio-temporal information-0,1904.04358-Performance analysis and discussion-2,1904.04358-2-Figure2-1.png,1904.04358-4-Table2-1.png,"Multichannel EEG data is high dimensional multivariate time series data whose dimensionality depends on the number of electrodes. It is a major hurdle to optimally encode information from these EEG data into lower dimensional space. In fact, our investigation based on a development set (as we explain later) showed that well-known deep neural networks (e.g., fully connected networks such as convolutional neural networks, recurrent neural networks and autoencoders) fail to individually learn such complex feature representations from single-trial EEG data. Besides, we found that instead of using the raw multi-channel high-dimensional EEG requiring large training times and resource requirements, it is advantageous to first reduce its dimensionality by capturing the information transfer among the electrodes. Instead of the conventional approach of selecting a handful of channels as BIBREF17 , BIBREF18 , we address this by computing the channel cross-covariance, resulting in positive, semi-definite matrices encoding the connectivity of the electrodes. We define channel cross-covariance (CCV) between any two electrodes INLINEFORM0 and INLINEFORM1 as: INLINEFORM2 . Next, we reject the channels which have significantly lower cross-covariance than auto-covariance values (where auto-covariance implies CCV on same electrode). We found this measure to be essential as the higher cognitive processes underlying speech planning and synthesis involve frequent information exchange between different parts of the brain. Hence, such matrices often contain more discriminative features and hidden information than mere raw signals. This is essentially different than our previous work BIBREF16 where we extract per-channel 1-D covariance information and feed it to the networks. We present our sample 2-D EEG cross-covariance matrices (of two individuals) in Fig. FIGREF2 .","As we found the individually-trained parallel networks (CNN and LSTM) to be useful (see Table TABREF12 ), we suspected the combination of these two networks could provide a more powerful discriminative spatial and temporal representation of the data than each independent network. As such, we concatenate the last fully-connected layer from the CNN with its counterpart in the LSTM to compose a single feature vector based on these two penultimate layers. Ultimately, this forms a joint spatio-temporal encoding of the cross-covariance matrix.","To further investigate the feature representation achieved by our model, we plot T-distributed Stochastic Neighbor Embedding (tSNE) corresponding to INLINEFORM0 and V/C classification tasks in Fig. FIGREF8 . We particularly select these two tasks as our model exhibits respectively minimum and maximum performance for these two. The tSNE visualization reveals that the second set of features are more easily separable than the first one, thereby giving a rationale for our performance.","Fig. 2. Cross covariance Matrices : Rows correspond to two different subjects; Columns (from left to right) correspond to sample examples for bilabial, nasal, vowel, /uw/, and /iy/.",Table 2. Results in accuracy on 10% test data in the first study,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How was the spatial aspect of the EEG signal computed?,Sample Answer,1904.04358-Deep autoencoder for spatio-temporal information-0,1904.04358-Training and hyperparameter selection-1,1904.04358-Performance analysis and discussion-0,1904.04358-Performance analysis and discussion-3,1904.04358-Acknowledgments-0,"As we found the individually-trained parallel networks (CNN and LSTM) to be useful (see Table TABREF12 ), we suspected the combination of these two networks could provide a more powerful discriminative spatial and temporal representation of the data than each independent network. As such, we concatenate the last fully-connected layer from the CNN with its counterpart in the LSTM to compose a single feature vector based on these two penultimate layers. Ultimately, this forms a joint spatio-temporal encoding of the cross-covariance matrix.","The architectural parameters and hyperparameters listed in Table TABREF6 were selected through an exhaustive grid-search based on the validation set of PHASE-ONE. We conducted a series of empirical studies starting from single hidden-layered networks for each of the blocks and, based on the validation accuracy, we increased the depth of each given network and selected the optimal parametric set from all possible combinations of parameters. For the gradient boosting classification, we fixed the maximum depth at 10, number of estimators at 5000, learning rate at 0.1, regularization coefficient at 0.3, subsample ratio at 0.8, and column-sample/iteration at 0.4. We did not find any notable change of accuracy while varying other hyperparameters while training gradient boost classifier.","To demonstrate the significance of the hierarchical CNN-LSTM-DAE method, we conducted separate experiments with the individual networks in PHASE-ONE of experiments and summarized the results in Table TABREF12 From the average accuracy scores, we observe that the mixed network performs much better than individual blocks which is in agreement with the findings in BIBREF21 . A detailed analysis on repeated runs further shows that in most of the cases, LSTM alone does not perform better than chance. CNN, on the other hand, is heavily biased towards the class label which sees more training data corresponding to it. Though the situation improves with combined CNN-LSTM, our analysis clearly shows the necessity of a better encoding scheme to utilize the combined features rather than mere concatenation of the penultimate features of both networks.","Next, we provide performance comparison of the proposed approach with the baseline methods for PHASE-TWO of our study (cross-validation experiment) in Table TABREF15 . Since the model encounters the unseen data of a new subject for testing, and given the high inter-subject variability of the EEG data, a reduction in the accuracy was expected. However, our network still managed to achieve an improvement of 18.91, 9.95, 67.15, 2.83 and 13.70 % over BIBREF17 . Besides, our best model shows more reliability compared to previous works: The standard deviation of our model's classification accuracy across all the tasks is reduced from 22.59% BIBREF17 and 17.52% BIBREF18 to a mere 5.41%.",This work was funded by the Natural Sciences and Engineering Research Council (NSERC) of Canada and Canadian Institutes for Health Research (CIHR).,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How big is the difference in performance between proposed model and baselines?,Sample Answer,1910.08293-Methodology ::: Overall Task-3,1910.08293-Methodology ::: Overall Task-4,1910.08293-Results and Analysis ::: Performance: ALOHA vs. Uniform Model-1,1910.08293-Conclusion and Future Work-0,1910.08293-3-Figure4-1.png,"We define the language style of a character as its diction, tone, and speech patterns. It is a character specific language model refined from a general language model. We must learn to recover the language style of $c_t$ without its dialogue as our objective is to imitate human-like qualities, and hence the model must understand the language styles of characters based on their traits. If we feed $c_t$'s dialogue during training, the model will likely not effectively learn to imitate language styles based on HLAs, but based on the correlation between text in the training and testing dialogues BIBREF26.","We define character space as the character representations within the HLA latent space (see Figure FIGREF4), and the set $C = \lbrace c_1,c_2,...,c_n\rbrace $ as the set of all characters. We define Observation (OBS) as the input that is fed into any dialogue model. This can be a single or multiple lines of dialogue along with other information. The goal of the dialogue model is to find the best response to this OBS.","ALOHA demonstrates an accuracy boost for all five evaluation characters, showing that the system is robust and stable and has the ability to recover the dialogue styles of fictional characters regardless of the character's profile and identity, genre of the show, and context of the dialogue.","We proposed Human Level Attributes (HLAs) as a novel approach to model human-like attributes of characters, and collected a large volume of dialogue data for various characters with complete and robust profiles. We also proposed and evaluated a system, ALOHA, that uses HLAs to recommend tailored responses traceable to specific characters, and demonstrated its outperformance of the baselines and ability to effectively recover language styles of various characters, showing promise for learning character or personality styles. ALOHA was also shown to be stable regardless of the character's identity, genre of show, and context of dialogue.","Figure 4: Illustration of our Collaborative Filtering procedure. Green check-marks indicate a character having an HLA, and ‘X’ indicates otherwise. We randomly mask 30% of this data for validation, as marked by the ‘?’.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what elements of each profile did they use?,Sample Answer,1605.05166-Data Collection and Datasets-4,1605.05166-Linguistic Models-3,1605.05166-Linguistic Models-15,1605.05166-Linguistic Models-16,1605.05166-Temporal Models-4,"Figure 1 shows the distribution of the number of posts per user for Twitter and Facebook for our collected dataset. In the figure, the data for the number of posts has been divided into 500 bins. For the Twitter data, each bin corresponds to 80 tweets, while for the Facebook data, it corresponds to 10 posts. Table 1 shows some statistics about the data collected, including the average number of posts per user for each of the sites.","However, parameter estimation in this full model is intractable, as the number of possible word combinations grows exponentially with sequence length. N-gram models address this with the approximation $\tilde{\Pr }(w_i|w_{i-n+1}^{i-1}) \approx \Pr (w_i|w_1^{i-1})$ using only the preceding $n-1$ words for context. A bigram model ( $n=2$ ) uses the preceding word for context, while a unigram model ( $n=1$ ) does not use any context.","TF-IDF is a method of converting text into numbers so that it can be represented meaningfully by a vector BIBREF23 . TF-IDF is the product of two statistics, TF or Term Frequency and IDF or Inverse Document Frequency. Term Frequency measures the number of times a term (word) occurs in a document. Since each document will be of different size, we need to normalize the document based on its size. We do this by dividing the Term Frequency by the total number of terms.","TF considers all terms as equally important, however, certain terms that occur too frequently should have little effect (for example, the term “the""). And conversely, terms that occur less in a document can be more relevant. Therefore, in order to weigh down the effects of the terms that occur too frequently and weigh up the effects of less frequently occurring terms, an Inverse Document Frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely. Generally speaking, the Inverse Document Frequency is a measure of how much information a word provides, that is, whether the term is common or rare across all documents.","Table 3 shows the performance of each of these models. Although the performance of the temporal models were not as strong as the linguistic ones, they all vastly outperformed the baseline. Also, note that here as with the linguistic models, the confusion model greatly outperformed the other models.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How do they combine the socioeconomic maps with Twitter data? ,Sample Answer,1804.01155-Introduction-3,1804.01155-Related Work-0,1804.01155-Standard usage of negation-0,1804.01155-Standard usage of negation-1,1804.01155-Standard usage of plural ending of written words-0,"Sociolinguistics has traditionally carried out research on the quantitative analysis of the so-called linguistic variables, i.e. points of the linguistic system which enable speakers to say the same thing in different ways, with these variants being ""identical in reference or truth value, but opposed in their social [...] significance"" BIBREF4 . Such variables have been described in many languages: variable pronunciation of -ing as [in] instead of [iŋ] in English (playing pronounced playin'); optional realization of the first part of the French negation (je (ne) fume pas, ""I do not smoke""); optional realization of the plural ending of verb in Brazilian Portuguese (eles disse(ram), ""they said""). For decades, sociolinguistic studies have showed that hearing certain variants triggers social stereotypes BIBREF5 . The so-called standard variants (e.g. [iŋ], realization of negative ne and plural -ram) are associated with social prestige, high education, professional ambition and effectiveness. They are more often produced in more formal situation. Non-standard variants are linked to social skills, solidarity and loyalty towards the local group, and they are produced more frequently in less formal situation.","For decades, sociolinguistic studies have repeatedly shown that speakers vary the way they talk depending on several factors. These studies have usually been limited to the analysis of small scale datasets, often obtained by surveying a set of individuals, or by direct observation after placing them in a controlled experimental setting. In spite of the volume of data collected generally, these studies have consistently shown the link between linguistic variation and social factors BIBREF10 , BIBREF11 .","The basic form of negation in French includes two negative particles: ne (no) before the verb and another particle after the verb that conveys more accurate meaning: pas (not), jamais (never), personne (no one), rien (nothing), etc. Due to this double construction, the first part of the negation (ne) is optional in spoken French, but it is obligatory in standard writing. Sociolinguistic studies have previously observed the realization of ne in corpora of recorded everyday spoken interactions. Although all the studies do not converge, a general trend is that ne realization is more frequent in speakers with higher socioeconomic status than in speakers with lower status BIBREF30 , BIBREF31 . We built upon this research to set out to detect both negation variants in the tweets using regular expressions. We are namely interested in the rate of usage of the standard negation (featuring both negative particles) across users: ","$$L^u_{\mathrm {cn}}=\frac{n^u_{\mathrm {cn}}}{n^u_{\mathrm {cn}}+n^u_{\mathrm {incn}}} \hspace{14.45377pt} \mbox{and} \hspace{14.45377pt} \overline{L}^{i}_{\mathrm {cn}}=\frac{\sum _{u\in i}L^u_{\mathrm {cn}}}{N_i},$$   (Eq. 18) ","In written French, adjectives and nouns are marked as being plural by generally adding the letters s or x at the end of the word. Because these endings are mute (without counterpart in spoken French), their omission is the most frequent spelling error in adults BIBREF32 . Moreover, studies showed correlations between standard spelling and social status of the writers, in preteens, teens and adults BIBREF33 , BIBREF32 , BIBREF34 . We then set to estimate the use of standard plural across users: ",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Does the fact that people are active during the day time define their SEC?,Sample Answer,1804.01155-Introduction-1,1804.01155-Introduction-2,1804.01155-Introduction-3,1804.01155-Introduction-4,1804.01155-Standard usage of plural ending of written words-0,"Languages and communication systems of several animal species vary in time, geographical space, and along social dimensions. Varieties are shared by individuals frequenting the same space or belonging to the same group. The use of vocal variants is flexible. It changes with the context and the communication partner and functions as ""social passwords"" indicating which individual is a member of the local group BIBREF0 . Similar patterns can be found in human languages if one considers them as evolving and dynamical systems that are made of several social or regional varieties, overlapping or nested into each other. Their emergence and evolution result from their internal dynamics, contact with each other, and link formation within the social organization, which itself is evolving, composite and multi-layered BIBREF1 , BIBREF2 .","The strong tendency of communication systems to vary, diversify and evolve seems to contradict their basic function: allowing mutual intelligibility within large communities over time. Language variation is not counter adaptive. Rather, subtle differences in the way others speak provide critical cues helping children and adults to organize the social world BIBREF3 . Linguistic variability contributes to the construction of social identity, definition of boundaries between social groups and the production of social norms and hierarchies.","Sociolinguistics has traditionally carried out research on the quantitative analysis of the so-called linguistic variables, i.e. points of the linguistic system which enable speakers to say the same thing in different ways, with these variants being ""identical in reference or truth value, but opposed in their social [...] significance"" BIBREF4 . Such variables have been described in many languages: variable pronunciation of -ing as [in] instead of [iŋ] in English (playing pronounced playin'); optional realization of the first part of the French negation (je (ne) fume pas, ""I do not smoke""); optional realization of the plural ending of verb in Brazilian Portuguese (eles disse(ram), ""they said""). For decades, sociolinguistic studies have showed that hearing certain variants triggers social stereotypes BIBREF5 . The so-called standard variants (e.g. [iŋ], realization of negative ne and plural -ram) are associated with social prestige, high education, professional ambition and effectiveness. They are more often produced in more formal situation. Non-standard variants are linked to social skills, solidarity and loyalty towards the local group, and they are produced more frequently in less formal situation.","It is therefore reasonable to say that the sociolinguistic task can benefit from the rapid development of computational social science BIBREF6 : the similarity of the online communication and face-to-face interaction BIBREF7 ensures the validity of the comparison with previous works. In this context, the nascent field of computational sociolinguistics found the digital counterparts of the sociolinguistic patterns already observed in spoken interaction. However a closer collaboration between computer scientists and sociolinguists is needed to meet the challenges facing the field BIBREF8 :","In written French, adjectives and nouns are marked as being plural by generally adding the letters s or x at the end of the word. Because these endings are mute (without counterpart in spoken French), their omission is the most frequent spelling error in adults BIBREF32 . Moreover, studies showed correlations between standard spelling and social status of the writers, in preteens, teens and adults BIBREF33 , BIBREF32 , BIBREF34 . We then set to estimate the use of standard plural across users: ",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How did they define standard language?,Sample Answer,1804.01155-Related Work-1,1804.01155-INSEE dataset: socioeconomic features-4,1804.01155-Combined dataset: individual socioeconomic features-0,1804.01155-Conclusions-2,1804.01155-4-Figure1-1.png,"Recently, the advent of social media and publicly available communication platforms has opened up a new gate to access individual information at a massive scale. Among all available social platforms, Twitter has been regarded as the choice by default, namely thanks to the intrinsic nature of communications taking place through it and the existence of data providers that are able to supply researchers with the volume of data they require. Work previously done on demographic variation is now relying increasingly on corpora from this social media platform as evidenced by the myriad of results showing that this resource reflects not only morpholexical variation of spoken language but also geographical BIBREF12 , BIBREF13 .","Despite the inherent biases of the selected socioeconomic indicators, in general we found weak but significant pairwise correlations between these three variables as shown in the upper diagonal panels in Fig. 1 b (in red), with values in Table 1 . We observed that while $S_\mathrm {inc}^{i}$ income and $S_\mathrm {own}^{i}$ owner ratio are positively correlated ( $R=0.24$ , $p<10^{-2}$ ), and the $S_\mathrm {own}^{i}$ and $S_\mathrm {den}^{i}$ population density are negatively correlated ( $R=-0.23$ , $p<10^{-2}$ ), $S_\mathrm {inc}^{i}$ and $S_\mathrm {den}^{i}$ appeared to be very weakly correlated ( $S_\mathrm {own}^{i}$0 , $S_\mathrm {own}^{i}$1 ). This nevertheless suggested that high average income, high owner ratio, and low population density are consistently indicative of high socioeconomic status in the dataset. [subfigure]justification=justified,singlelinecheck=false","Data collected from Twitter provides a large variety of information about several users including their tweets, which disclose their interests, vocabulary, and linguistic patterns; their direct mentions from which their social interactions can be inferred; and the sequence of their locations, which can be used to infer their representative location. However, no information is directly available regarding their socioeconomic status, which can be pivotal to understand the dynamics and structure of their personal linguistic patterns.","Despite these findings, one has to acknowledge the multiple limitations affecting this work: First of all, although Twitter is a broadly adopted service in most technologically enabled societies, it commonly provides a biased sample in terms of age and socioeconomic status as older or poorer people may not have access to this technology. In addition, home locations inferred for lower activity users may induced some noise in our inference method. Nevertheless, we demonstrated that our selected Twitter users are quite representative in terms of spatial, temporal, and socioeconomic distributions once compared to census data. Other sources of bias include the ""homogenization"" performed by INSEE to ensure privacy rights are upheld as well as the proxies we devised to approximate users' home location and social network. Currently, a sample survey of our set of geolocated users is being conducted so as to bootstrap socioeconomic data to users and definitely validate our inference results. Nonetheless, this INSEE dataset provides still the most comprehensive available information on socioeconomic status over the whole country. For limiting such risk of bias, we analyzed the potential effect of the confounding variables on distribution and cross-correlations of SES indicators. Acknowledging possible limitations of this study, we consider it as a necessary first step in analyzing income through social media using datasets orders of magnitude larger than in previous research efforts.","Figure 1: Distributions and correlations of socioeconomic indicators. (a) Spatial distribution of average income in France with 200m × 200m resolution. (b) Distribution of socioeconomic indicators (in the diag.) and their pairwise correlations measured in the INSEE (upper diag. panels) and Twitter geotagged (lower diag. panels) datasets. Contour plots assign the equidensity lines of the scatter plots, while solid lines are the corresponding linear regression values. Population density in log.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How much improvement do the introduced model achieve compared to the previous models?,Sample Answer,1706.02222-Gated Recurrent Neural Network-1,1706.02222-Recursive Neural Tensor Network-6,1706.02222-Optimizing Tensor Weight using Backpropagation Through Time-6,1706.02222-Optimizing Tensor Weight using Backpropagation Through Time-12,1706.02222-3-Figure4-1.png,A Long Short Term Memory (LSTM) BIBREF9 is a gated RNN with three gating layers and memory cells. The gating layers are used by the LSTM to control the existing memory by retaining the useful information and forgetting the unrelated information. Memory cells are used for storing the information across time. The LSTM hidden layer at time $t$ is defined by the following equations BIBREF17 : ,"where $W_{tsr}^{[1:d]} \in \mathbb {R}^{2d \times 2d \times d}$ is the tensor weight to map the tensor product between two children vectors. Each slice $W_{tsr}^{[i]}$ is a matrix $\mathbb {R}^{2d \times 2d}$ . For more details, we visualize the calculation for $p_1$ in Fig. 5 .",$$\frac{\partial E(\theta )}{\partial W_{tsr}^{[1:d]}} &=& \sum _{i=1}^{T} \frac{\partial E_i(\theta )}{\partial W_{tsr}^{[1:d]}} \nonumber $$   (Eq. 26) ,"$$f^{\prime }(a_j) =
{\left\lbrace \begin{array}{ll}
(1-f(a_j)^2), & \text{if } f(\cdot ) \text{ is $\tanh $ function} \\
f(a_j)(1-f(a_j)), & \text{if } f(\cdot ) \text{ is sigmoid function}
\end{array}\right.} \nonumber $$   (Eq. 29) ","Fig. 4. Computation for parent values p1 and p2 was done in a bottom-up fashion. Visible node leaves x1, x2, and x3 are processed based on the given binary tree structure.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the performance difference of using a generated summary vs. a user-written one?,Sample Answer,1911.02711-Introduction-1,1911.02711-Introduction-3,1911.02711-Related Work-1,1911.02711-Method ::: Hierarchically-Refined Review Encoder ::: Attention Inference Layer-4,1911.02711-Experiments ::: Baselines ::: HSSC @!START@BIBREF6@!END@.-0,"To this end, some recent work BIBREF6, BIBREF7 exploits joint modeling. The model structure can be illustrated by Figure FIGREF4. In particular, given a review input, a model is trained to simultaneously predict the sentiment and summary. As a result, both summary information and review information are integrated in the review encoder through back-propagation training. However, one limitation of this method is that it does not explicitly encode a summary during test time.","To address this issue, we further investigate a joint encoder for review and summary, which is demonstrated in Figure FIGREF4. The model works by jointly encoding the review and the summary in a multi-layer structure, incrementally updating the representation of the review by consulting the summary representation at each layer. As shown in Figure FIGREF5, our model consists of a summary encoder, a hierarchically-refined review encoder and an output layer. The review encoder is composed of multiple attention layers, each consisting of a sequence encoding layer and an attention inference layer. Summary information is integrated into the representation of the review content at each attention layer, thus, a more abstract review representation is learned in subsequent layers based on a lower-layer representation. This mechanism allows the summary to better guide the representation of the review in a bottom-up manner for improved sentiment classification.","In particular, attention-based models have been widely explored, which assign attention weights to hidden states to generate a representation of the input sequence. A hierarchical model with two levels of attention mechanisms was proposed for document classification BIBREF12. Self-attention mechanism has also been used in sentiment analysis BIBREF13, BIBREF14. However, BIBREF15 empirically showed that self-attention mechanism does not consistently agree with the most salient features, which means that self-attention models may suffer from attending on explicit but irrelevant sentimental words.","According to the equations of standard LSTM and Equation DISPLAY_FORM13, tokens of the original review that are the most relevant to the summary are focused on more by consulting summary representation. The hidden states $\mathbf {H}^{w,s}$ are thus a representation matrix of the review text that encompass key features of summary representation. Multi-head attention mechanism ensures that multi-faced semantic dependency features can be captured during the process, which is beneficial for scenarios where several key points exist in one review.","This model adopts encoder parameter sharing for jointly sentiment classification and summarization. It predicts the sentiment label using a highway layer, concatenating the hidden state in summary decoder and the original text representation in encoder.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the source of the dataset?,Sample Answer,1910.09295-Methods ::: ULMFiT-2,1910.09295-Results and Discussion ::: Language Model Finetuning Significance-2,1910.09295-Ablation Studies ::: Attention Head Effects-3,1910.09295-Ablation Studies ::: Attention Head Effects-4,1910.09295-Stylometric Tests-2,"First, the language model is finetuned to the text of the target task to adapt to the task syntactically. Second, a classification layer is appended to the model and is finetuned to the classification task conservatively. During finetuning, multiple different techniques are introduced to prevent catastrophic forgetting.","In this finetuning stage, the model is said to “adapt to the idiosyncracies of the task it is solving” BIBREF5. Given that our techniques rely on linguistic cues and features to make accurate predictions, having the model adapt to the stylometry or “writing style” of an article will therefore improve performance.","While increasing the number of attention heads improves performance, keeping on adding extra heads will not result to an equivalent boost as the performance plateaus after a number of heads.","As shown in Figure FIGREF35, the performance boost of the model plateaus after 10 attention heads, which was the default used in the study. While the performance of 16 heads is greater than 10, it is only a marginal improvement, and does not justify the added costs to training with more attention heads.","When looking at the y-axis, there is a big difference in word count. The fake news corpora has twice the amount of words as the real news corpora. This means that fake news articles are at average lengthier than real news articles. The only differences seen in the x-axis is the order of appearance of word lengths 6, 7, and 1. The characteristic curves also exhibit differences in trend. While the head and tail look similar, the body show different trends. When graphing the corpora by news category, the heads and tails look similar to the general real and fake news characteristic curve but the body exhibits a trend different from the general corpora. This difference in trend may be attributed to either a lack of text data to properly represent real and fake news or the existence of a stylistic difference between real and fake news.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What were the baselines?,Sample Answer,1910.09295-Methods ::: BERT-2,1910.09295-Methods ::: Multitask Finetuning-1,1910.09295-Ablation Studies-0,1910.09295-7-Table5-1.png,1910.09295-7-Table6-1.png,"where $d_{k}$ is the dimensions of the key matrix $K$. Attention allows the Transformer to refer to multiple positions in a sequence for context at any given time regardless of distance, which is an advantage over Recurrent Neural Networks (RNN).","Motivated by recent advancements in multitask learning, we finetune the model to the stylometry of the target task at the same time as we finetune the classifier, instead of setting it as a separate step. This produces two losses to be optimized together during training, and ensures that no task (stylometric adaptation or classification) will be prioritized over the other. This concept has been proposed and explored to improve the performance of transfer learning in multiple language tasks BIBREF9, BIBREF10.",Several ablation studies are performed to establish causation between the model architectures and the performance boosts in the study.,Table 5: An ablation study on the effects of pretraining for multitasking-based and standard GPT-2 finetuning. Results show that pretraining greatly accounts for almost half of performance on both finetuning techniques. “Acc. Inc.” refers to the boost in performance contributed by the pretraining step. “% of Perf.” refers to the percentage of the total performance that the pretraining step contributes.,"Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. “Effect” refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What rouge score do they achieve?,Sample Answer,1908.08345-Introduction-2,1908.08345-Introduction-3,1908.08345-Fine-tuning Bert for Summarization ::: Summarization Encoder-0,1908.08345-Fine-tuning Bert for Summarization ::: Extractive Summarization-0,1908.08345-3-Figure1-1.png,"We explore the potential of Bert for text summarization under a general framework encompassing both extractive and abstractive modeling paradigms. We propose a novel document-level encoder based on Bert which is able to encode a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers to capture document-level features for extracting sentences. Our abstractive model adopts an encoder-decoder architecture, combining the same pretrained Bert encoder with a randomly-initialized Transformer decoder BIBREF3. We design a new training schedule which separates the optimizers of the encoder and the decoder in order to accommodate the fact that the former is pretrained while the latter must be trained from scratch. Finally, motivated by previous work showing that the combination of extractive and abstractive objectives can help generate better summaries BIBREF4, we present a two-stage approach where the encoder is fine-tuned twice, first with an extractive objective and subsequently on the abstractive summarization task.","We evaluate the proposed approach on three single-document news summarization datasets representative of different writing conventions (e.g., important information is concentrated at the beginning of the document or distributed more evenly throughout) and summary styles (e.g., verbose vs. more telegraphic; extractive vs. abstractive). Across datasets, we experimentally show that the proposed models achieve state-of-the-art results under both extractive and abstractive settings. Our contributions in this work are three-fold: a) we highlight the importance of document encoding for the summarization task; a variety of recently proposed techniques aim to enhance summarization performance via copying mechanisms BIBREF5, BIBREF6, BIBREF7, reinforcement learning BIBREF8, BIBREF9, BIBREF10, and multiple communicating encoders BIBREF11. We achieve better results with a minimum-requirement model without using any of these mechanisms; b) we showcase ways to effectively employ pretrained language models in summarization under both extractive and abstractive settings; we would expect any improvements in model pretraining to translate in better summarization in the future; and c) the proposed models can be used as a stepping stone to further improve summarization performance as well as baselines against which new proposals are tested.","Although Bert has been used to fine-tune various NLP tasks, its application to summarization is not as straightforward. Since Bert is trained as a masked-language model, the output vectors are grounded to tokens instead of sentences, while in extractive summarization, most models manipulate sentence-level representations. Although segmentation embeddings represent different sentences in Bert, they only apply to sentence-pair inputs, while in summarization we must encode and manipulate multi-sentential inputs. Figure FIGREF2 illustrates our proposed Bert architecture for Summarization (which we call BertSum).","Let $d$ denote a document containing sentences $[sent_1, sent_2, \cdots , sent_m]$, where $sent_i$ is the $i$-th sentence in the document. Extractive summarization can be defined as the task of assigning a label $y_i \in \lbrace 0, 1\rbrace $ to each $sent_i$, indicating whether the sentence should be included in the summary. It is assumed that summary sentences represent the most important content of the document.","Figure 1: Architecture of the original BERT model (left) and BERTSUM (right). The sequence on top is the input document, followed by the summation of three kinds of embeddings for each token. The summed vectors are used as input embeddings to several bidirectional Transformer layers, generating contextual vectors for each token. BERTSUM extends BERT by inserting multiple [CLS] symbols to learn sentence representations and using interval segmentation embeddings (illustrated in red and green color) to distinguish multiple sentences.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the new initialization method proposed in this paper?,Sample Answer,1706.09147-Introduction-0,1706.09147-The WikilinksNED Dataset:             Entity Mentions in the Web-0,1706.09147-Model Architecture-2,1706.09147-Training-1,1706.09147-WikilinksNED-1,"Named Entity Disambiguation (NED) is the task of linking mentions of entities in text to a given knowledge base, such as Freebase or Wikipedia. NED is a key component in Entity Linking (EL) systems, focusing on the disambiguation task itself, independently from the tasks of Named Entity Recognition (detecting mention bounds) and Candidate Generation (retrieving the set of potential candidate entities). NED has been recognized as an important component in NLP tasks such as semantic parsing BIBREF0 .","We introduce WikilinksNED, a large-scale NED dataset based on text fragments from the web. Our dataset is derived from the Wikilinks corpus BIBREF14 , which was constructed by crawling the web and collecting hyperlinks (mentions) linking to Wikipedia concepts (entities) and their surrounding text (context). Wikilinks contains 40 million mentions covering 3 million entities, collected from over 10 million web pages.","The ARNN unit is composed from an RNN and an attention mechanism. Equation 10 represents the general semantics of an RNN unit. An RNN reads a sequence of vectors $\lbrace v_t\rbrace $ and maintains a hidden state vector $\lbrace h_t\rbrace $ . At each step a new hidden state is computed based on the previous hidden state and the next input vector using some function $f$ , and an output is computed using $g$ . This allows the RNN to “remember” important signals while scanning the context and to recognize signals spanning multiple words. ",Near-Misses: Sampling out of the candidate set of each mention. We have found this to be more effective where the training data reliably reflects the test-set distribution.,"To isolate the effect of candidate generation algorithms, we used the following simple method for all systems: given a mention $m$ , consider all candidate entities $e$ that appeared as the ground-truth entity for $m$ at least once in the training corpus. This simple method yields $97\%$ ground-truth recall on the test set.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How was a quality control performed so that the text is noisy but the annotations are accurate?,Sample Answer,1706.09147-Introduction-0,1706.09147-Introduction-3,1706.09147-The WikilinksNED Dataset:             Entity Mentions in the Web-0,1706.09147-Model Architecture-4,1706.09147-WikilinksNED-5,"Named Entity Disambiguation (NED) is the task of linking mentions of entities in text to a given knowledge base, such as Freebase or Wikipedia. NED is a key component in Entity Linking (EL) systems, focusing on the disambiguation task itself, independently from the tasks of Named Entity Recognition (detecting mention bounds) and Candidate Generation (retrieving the set of potential candidate entities). NED has been recognized as an important component in NLP tasks such as semantic parsing BIBREF0 .",“I had no choice but to experiment with other indoor games. I was born in Atlantic City so the obvious next choice was Monopoly. I played until I became a successful Captain of Industry.”,"We introduce WikilinksNED, a large-scale NED dataset based on text fragments from the web. Our dataset is derived from the Wikilinks corpus BIBREF14 , which was constructed by crawling the web and collecting hyperlinks (mentions) linking to Wikipedia concepts (entities) and their surrounding text (context). Wikilinks contains 40 million mentions covering 3 million entities, collected from over 10 million web pages.","Our implementation uses a standard GRU unit BIBREF19 as an RNN. We fit the RNN unit with an additional attention mechanism, commonly used with state-of-the-art encoder-decoder models BIBREF20 , BIBREF21 . Since our model lacks a decoder, we use the entity embedding as a control signal for the attention mechanism.","Finally, we include the Most Probable Sense (MPS) baseline, which selects the entity that was seen most with the given mention during training.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
"How are customer satisfaction, customer frustration and overall problem resolution data collected?",Sample Answer,1709.05413-Related Work-1,1709.05413-Data Collection-5,1709.05413-Experiments-5,1709.05413-Experiments-6,1709.05413-Classifying Problem Outcomes-5,"Previous work has explored speech act modeling in different domains (as a predecessor to dialogue act modeling). Zhang et al. present work on recognition of speech acts on Twitter, following up with a study on scalable speech act recognition given the difficulty of obtaining labeled training data BIBREF9 . They use a simple taxonomy of four main speech acts (Statement, Question, Suggestion, Comment, and a Miscellaneous category). More recently, Vosoughi et al. develop BIBREF10 a speech act classifier for Twitter, using a modification of the taxonomy defined by Searle in 1975, including six acts they observe to commonly occur on Twitter: Assertion, Recommendation Expression, Question, Request, again plus a Miscellaneous category. They describe good features for speech act classification and the application of such a system to detect stories on social media BIBREF11 . In this work, we are interested in the dialogic characteristics of Twitter conversations, rather than speech acts in stand-alone tweets.","It is clear that the lines differentiating these acts are not very well defined, and that segmentation would not necessarily aid in clearly separating out each intent. For these reasons, and due to the overall brevity of tweets in general, we choose to avoid the overhead of requiring annotators to provide segment boundaries, and instead ask for all appropriate dialogue acts.","From this experiment, we observe that our sequential SVM-HMM outperforms each non-sequential baseline, for each of the four class sets. We select the sequential SVM-HMM model for our preferred model for subsequent experiments. We observe that while performance may be expected to drop as the number of classes increases, we instead get a spike in performance for the 10-Class (Easy) setting. This increase occurs due to the addition of the lexically well-defined classes of Statement Apology and Statement Thanks, which are much simpler for our model to predict. Their addition results in a performance boost, comparable to that of the simpler 6-Class problem. When we remove the two well-defined classes and add in the next two broader dialogue act classes of Statement Offer and Question Open (as defined by the 10-Class (Hard) set), we observe a drop in performance, and an overall result comparable to our 8-Class problem. This result is still strong, since the number of classes has increased, but the overall performance does not drop.","We also observe that while NB and LinearSVC have the same performance trend for the smaller number of classes, Linear SVC rapidly improves in performance as the number of classes increases, following the same trend as SVM-HMM. The smallest margin of difference between SVM-HMM and Linear SVC also occurs at the 10-Class (Easy) setting, where the addition of highly-lexical classes makes for a more differentiable set of turns.","In more detail, we note interesting differences comparing the performance of the small set of dialogue act features that ""summarize"" the large, sparse set of best features for each label, as a form of data-driven feature selection. For satisfaction, we see that the best feature set outperforms the dialogue acts for each class set except for 10-Class (Easy), where the dialogue acts are more effective. The existence of the very lexically well-defined Social Act Thanking and Social Act Apology classes makes the dialogue acts ideal for summarization. In the case of problem resolution, we see that the performance of the dialogue acts approaches that of the best feature set as the number of classes increases, showing that the dialogue features are able to express the full intent of the turns well, even at more difficult class settings. Finally, for the frustration experiment, we observe negligible different between the best features and dialogue act features, and very high classification results overall.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the tasks that this method has shown improvements?,Sample Answer,1808.08780-Introduction-0,1808.08780-Introduction-2,1808.08780-Cross-lingual embeddings training-1,1808.08780-Experiments-5,1808.08780-Conclusions and Future Work-0,"Word embeddings are one of the most widely used resources in NLP, as they have proven to be of enormous importance for modeling linguistic phenomena in both supervised and unsupervised settings. In particular, the representation of words in cross-lingual vector spaces (henceforth, cross-lingual word embeddings) is quickly gaining in popularity. One of the main reasons is that they play a crucial role in transferring knowledge from one language to another, specifically in downstream tasks such as information retrieval BIBREF0 , entity linking BIBREF1 and text classification BIBREF2 , while at the same time providing improvements in multilingual NLP problems such as machine translation BIBREF3 .","These alignments are generally modeled as linear transformations, which are constrained such that the structure of the initial monolingual spaces is left unchanged. This can be achieved by imposing an orthogonality constraint on the linear transformation BIBREF6 , BIBREF7 . Our hypothesis in this paper is that such approaches can be further improved, as they rely on the assumption that the internal structure of the two monolingual spaces is identical. In reality, however, this structure is influenced by language-specific phenomena, e.g., the fact that Spanish distinguishes between masculine and feminine nouns BIBREF8 as well as the specific biases of the different corpora from which the monolingual spaces were learned. Because of this, monolingual embedding spaces are not isomorphic BIBREF9 , BIBREF10 . On the other hand, simply dropping the orthogonality constraints leads to overfitting, and is thus not effective in practice.","Monolingual embeddings. The monolingual word embeddings are trained with the Skipgram model from FastText BIBREF31 on the corpora described above. The dimensionality of the vectors was set to 300, with the default FastText hyperparameters.","We perform experiments on both monolingual and cross-lingual word similarity. In monolingual similarity, models are tested in their ability to determine the similarity between two words in the same language, whereas in cross-lingual similarity the words belong to different languages. While in the monolingual setting the main objective is to test the quality of the monolingual subsets of the bilingual vector space, the cross-lingual setting constitutes a straightforward benchmark to test the quality of bilingual embeddings.","We have shown how to refine bilingual word embeddings by applying a simple transformation which moves cross-lingual synonyms closer towards their average representation. Before applying this strategy, we start by aligning the monolingual embeddings of the two languages of interest. For this initial alignment, we have considered two state-of-the-art methods from the literature, namely VecMap BIBREF11 and MUSE BIBREF12 , which also served as our baselines. Our approach is motivated by the fact that these alignment methods do not change the structure of the individual monolingual spaces. However, the internal structure of embeddings is, at least to some extent, language-specific, and is moreover affected by biases of the corpus from which they are trained, meaning that after the initial alignment significant gaps remain between the representations of cross-lingual synonyms. We tested our approach on a wide array of datasets from different tasks (i.e., bilingual dictionary induction, word similarity and cross-lingual hypernym discovery) with state-of-the-art results.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Why does the model improve in monolingual spaces as well? ,Sample Answer,1808.08780-Introduction-4,1808.08780-Related Work-1,1808.08780-Meeting in the middle-2,1808.08780-Meeting in the middle-4,1808.08780-Experiments-1,"Our experimental results show that the proposed additional transformation does not only benefit cross-lingual evaluation tasks, but, perhaps surprisingly, also monolingual ones. In particular, we perform an extensive set of experiments on standard benchmarks for bilingual dictionary induction and monolingual and cross-lingual word similarity, as well as on an extrinsic task: cross-lingual hypernym discovery.","Another branch of research exploits pre-trained monolingual embeddings with weak signals such as bilingual lexicons for learning bilingual embeddings BIBREF4 , BIBREF5 , BIBREF24 , BIBREF7 . mikolov2013exploiting was one of the first attempts into this line of research, applying a linear transformation in order to map the embeddings from one monolingual space into another. They also noted that more sophisticated approaches, such as using multilayer perceptrons, do not improve with respect to their linear counterparts. xing2015normalized built upon this work by normalizing word embeddings during training and adding an orthogonality constraint. In a complementary direction, faruqui2014improving put forward a technique based on canonical correlation analysis to obtain linear mappings for both monolingual embedding spaces into a new shared space. artetxe2016learning proposed a similar linear mapping to mikolov2013exploiting, generalizing it and providing theoretical justifications which also served to reinterpret the methods of faruqui2014improving and xing2015normalized. smith2017offline further showed how orthogonality was required to improve the consistency of bilingual mappings, making them more robust to noise. Finally, a more complete generalization providing further insights on the linear transformations used in all these models can be found in artetxe2018generalizing.","$$E=\sum _{(w,w^{\prime }) \in D} \Vert X\vec{w}-\vec{\mu }_ {w,w^{\prime }}\Vert ^2$$   (Eq. 6) ","It is worth pointing out that we experimented with several variants of this linear regression formulation. For example, we also tried using a multilayer perceptron to learn non-linear mappings, and we experimented with several regularization terms to penalize mappings that deviate too much from the identity mapping. None of these variants, however, were found to improve on the much simpler formulation in ( 6 ), which can be solved exactly and efficiently. Furthermore, one may wonder whether the initial alignment is actually needed, since e.g., coates2018frustratingly obtained high-quality meta-embeddings without such an alignment set. However, when applying our approach directly to the initial monolingual non-aligned embedding spaces, we obtained results which were competitive but slightly below the two considered alignment strategies.","The dictionary induction task consists in automatically generating a bilingual dictionary from a source to a target language, using as input a list of words in the source language.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Ngrams of which length are aligned using PARENT?,Sample Answer,1906.01081-Correlation Comparison-2,1906.01081-5-Table1-1.png,1906.01081-6-Table2-1.png,1906.01081-7-Figure4-1.png,1906.01081-8-Table3-1.png,"Correlations are higher for the systems category than the hyperparams category. The latter is a more difficult setting since very similar models are compared, and hence the variance of the correlations is also high. Commonly used metrics which only rely on the reference (BLEU, ROUGE, METEOR, CIDEr) have only weak correlations with human judgments. In the hyperparams category, these are often negative, implying that tuning models based on these may lead to selecting worse models. BLEU performs the best among these, and adding n-grams from the table as references improves this further (BLEU-T).","Table 1: Models used for WikiBio, with the human evaluation scores for these model outputs and the reference texts. PG-Net: Pointer-Generator network. Human scores computed using Thurstone’s method (Tsukida and Gupta, 2011).",Table 2: Correlation of metrics with human judgments on WikiBio. A superscript of C/W indicates that the correlation is significantly lower than that of PARENTC/W using a bootstrap confidence test for α = 0.1.,Figure 4: Correlation of the metrics to human judgment as the percentage of entailed examples in WikiBio is varied.,Table 3: Accuracy on making the same judgments as humans between pairs of generated texts. p < 0.01∗/0.05†/0.10‡: accuracy is significantly higher than the next best accuracy to the left using a paired McNemar’s test.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How many people participated in their evaluation study of table-to-text models?,Sample Answer,1906.01081-Data & Models-0,1906.01081-Compared Metrics-1,1906.01081-Correlation Comparison-2,1906.01081-Correlation Comparison-3,1906.01081-Related Work-1,"Our main experiments are on the WikiBio dataset BIBREF0 , which is automatically constructed and contains many divergent references. In § SECREF47 we also present results on the data released as part of the WebNLG challenge.","Information Extraction based: We compare the CS, RG and RG-F metrics discussed in § SECREF4 .","Correlations are higher for the systems category than the hyperparams category. The latter is a more difficult setting since very similar models are compared, and hence the variance of the correlations is also high. Commonly used metrics which only rely on the reference (BLEU, ROUGE, METEOR, CIDEr) have only weak correlations with human judgments. In the hyperparams category, these are often negative, implying that tuning models based on these may lead to selecting worse models. BLEU performs the best among these, and adding n-grams from the table as references improves this further (BLEU-T).","Among the extractive evaluation metrics, CS, which also only relies on the reference, has poor correlation in the hyperparams category. RG-F, and both variants of the PARENT metric achieve the highest correlation for both settings. There is no significant difference among these for the hyperparams category, but for systems, PARENT-W is significantly better than the other two. While RG-F needs a full information extraction pipeline in its implementation, PARENT-C only relies on co-occurrence counts, and PARENT-W can be used out-of-the-box for any dataset. To our knowledge, this is the first rigorous evaluation of using information extraction for generation evaluation.","Hallucination BIBREF38 , BIBREF39 refers to when an NLG system generates text which mentions extra information than what is present in the source from which it is generated. Divergence can be viewed as hallucination in the reference text itself. PARENT deals with hallucination by discounting n-grams which do not overlap with either the reference or the table.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?,Sample Answer,1906.01081-Evaluation via Information Extraction-0,1906.01081-Evaluation via Information Extraction-2,1906.01081-WebNLG Dataset-3,1906.01081-9-Figure5-1.png,1906.01081-11-Figure6-1.png," BIBREF1 proposed to use an auxiliary model, trained to extract structured records from text, for evaluation. However, the extraction model presented in that work is limited to the closed-domain setting of basketball game tables and summaries. In particular, they assume that each table has exactly the same set of attributes for each entity, and that the entities can be identified in the text via string matching. These assumptions are not valid for the open-domain WikiBio dataset, and hence we train our own extraction model to replicate their evaluation scheme.","Given this information extraction system, we consider the following metrics for evaluation, along the lines of BIBREF1 . Content Selection (CS): F-score for the (attribute, value) pairs extracted from the generated text compared to those extracted from the reference. Relation Generation (RG): Precision for the (attribute, value) pairs extracted from the generated text compared to those in the ground truth table. RG-F: Since our task emphasizes the recall of information from the table as well, we consider another variant which computes the F-score of the extracted pairs to those in the table. We omit the content ordering metric, since our extraction system does not align records to the input text.","The INLINEFORM0 parameter in the calculation of PARENT decides whether to compute recall against the table or the reference (Eq. EQREF22 ). Figure FIGREF50 shows the distribution of the values taken by INLINEFORM1 using the heuristic described in § SECREF3 for instances in both WikiBio and WebNLG. For WikiBio, the recall of the references against the table is generally low, and hence the recall of the generated text relies more on the table. For WebNLG, where the references are elicited from humans, this recall is much higher (often INLINEFORM2 ), and hence the recall of the generated text relies more on the reference.","Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference.","Figure 6: An input-output pair for the information extraction system. <R> and <C> are special symbols used to separate (attribute, value) pairs and attributes from values, respectively.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are causal attribution networks?,Sample Answer,1812.06038-Graph fusion-2,1812.06038-Capture-recapture-6,1812.06038-Fusing causal networks-3,1812.06038-9-Table2-1.png,1812.06038-12-Figure4-1.png,"To fuse $G_1$ and $G_2$ into $G$ , first compute $F = \lbrace f(u,v) \mid u,v \in V_1 \cup V_2 \rbrace $ . One can interpret $F$ as (the edges of) a fusion indicator graph defined over the combined node sets of $G_1$ and $G_2$ . Each connected component in $F$ then corresponds to a subset of $V_1 \cup V_2$ that should be combined into a single node in $V$ . (One can also take a stricter view and combine nodes corresponding to completely dense connected components of $G_2$0 instead of any connected components, but this strictness can also be incorporated by making $G_2$1 more strict.) Let $G_2$2 indicate the connected component of $G_2$3 containing node $G_2$4 . Abusing notation, one can also consider $G_2$5 as representing the node in $G_2$6 that the unfused node $G_2$7 maps onto. Lastly, we define the edges $G_2$8 of the fused graph based on the neighborhoods of nodes in $G_2$9 . The neighborhood $G$0 of each node $G$1 in the fused graph is the union of the neighborhoods of the nodes connected to $G$2 in $G$3 : for any node $G$4 , let $G$5 and $G$6 Then the neighborhood $G$7 defines the edges incident on $G$8 in the fused graph and $G$9 may now be computed. Notice by this procedure that if an edge already exists in $F = \lbrace f(u,v) \mid u,v \in V_1 \cup V_2 \rbrace $0 and/or $F = \lbrace f(u,v) \mid u,v \in V_1 \cup V_2 \rbrace $1 between two nodes $F = \lbrace f(u,v) \mid u,v \in V_1 \cup V_2 \rbrace $2 and $F = \lbrace f(u,v) \mid u,v \in V_1 \cup V_2 \rbrace $3 that share a connected component in $F = \lbrace f(u,v) \mid u,v \in V_1 \cup V_2 \rbrace $4 , then a self-loop is created in $F = \lbrace f(u,v) \mid u,v \in V_1 \cup V_2 \rbrace $5 when $F = \lbrace f(u,v) \mid u,v \in V_1 \cup V_2 \rbrace $6 and $F = \lbrace f(u,v) \mid u,v \in V_1 \cup V_2 \rbrace $7 are combined. For our purposes these self-loops are meaningful, but otherwise they can be discarded.","with $n_{12} > 1$ , allowing us to assess our estimate uncertainty. Equations ( 6 ) and ( 7 ) are approximations when assuming a flat prior on $N$ but are exact when assuming an almost-flat prior on $N$ that slightly favors larger populations $N$ over smaller BIBREF30 .","To apply NetFUSES with our semantic similarity function (Sec. ""Data and Methods"" ) requires determining a single parameter, the similarity threshold $t$ . One can identify a value of $t$ using an independent analysis of text, but we argue for a simple indicator of its value given the networks: growth in the number of self-loops as $t$ is varied. If two nodes $i$ and $j$ that are connected before fusion are combined into a single node $u$ by NetFUSES, then the edge $i\rightarrow j$ becomes the self-loop $u \rightarrow u$ . Yet the presence of the original edge $i \rightarrow j$ generally implies that those nodes are not equivalent, and so it is more plausible that combining them is a case of over-fusion than it would have been if $i$ and $t$0 were not connected. Of course, in networks such as the causal attribution networks we study, a self-loop is potentially meaningful, representing a positive feedback where a cause is its own effect. But these self-loops are quite rare (Table 1 ) and we argue that creating additional self-loops via NetFUSES is more likely to be over-fusion than the identification of such feedback. Thus we can study the growth in the number of self-loops as we vary the threshold $t$1 to determine as an approximate value for $t$2 the point at which new self-loops start to form.",Table 2: Text statistics across each dataset.,"Figure 4: Statistics of fused Wikidata–ConceptNet networks across semantic similarity threshold values. Monitoring the number of self-loops, we observe a relatively clear onset of over-fusion at a threshold of t ≈ 0.95. At this threshold, we observe a 4.95% reduction in the number of nodes and a 1.43% reduction in the number of edges compared with t ≥ 1.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the highest accuracy score achieved?,Sample Answer,1607.06025-Introduction-3,1607.06025-Introduction-8,1607.06025-Recurrent Neural Networks-2,1607.06025-Preliminary evaluation-7,1607.06025-Conclusion-0,"But what is a good stream of hypotheses? We argue that a good stream contains diverse, comprehensible, accurate and non-trivial hypotheses. A hypothesis is comprehensible if it is grammatical and semantically makes sense. It is accurate if it clearly expresses the relationship (signified by the label) with the premise. Finally, it is non-trivial if it is not trivial to determine the relationship (label) between the hypothesis and premise. For instance, given a premise ”A man drives a red car” and label entailment, the hypothesis ”A man drives a car” is more trivial than ”A person is sitting in a red vehicle”.","The generative models learn solely from the original training set to regenerate the dataset. Thus, the model learns the distribution of the original dataset. Furthermore, the generated dataset is just a random sample from the estimated distribution. To determine how well did the generative model learn the distribution, we observe how close does the accuracy of the classifier trained on the generated dataset approach the accuracy of classifier trained on the original dataset.","The mLSTM is an attention-based model with two input sequences – premise and hypothesis in case of NLI. Each word of the premise is matched against each word of the hypothesis to find the soft alignment between the sentences. The mLSTM is based on LSTM in such a way that it remembers the important matches and forgets the less important. The input to the LSTM inside the mLSTM at each time step is INLINEFORM0 , where INLINEFORM1 is an attention vector that represents the weighted sum of premise sequence, where the weights present the degree to which each token of the premise is aligned with the INLINEFORM2 -th token of the hypothesis INLINEFORM3 , and INLINEFORM4 is the concatenation operator. More details about mLSTM are presented in BIBREF2 .","The next metric is the log-likelihood of hypotheses in the development set. This metric is the negative of the training loss function. The log-likelihood improves with dimensionality since it is easier to fit the hypotheses in the training step having more dimensions. Consequently, the hypothesis in the generating step are more confident – they have lower log-likelihood.","In this paper, we have proposed several generative neural networks for generating hypothesis using NLI dataset. To evaluate these models we propose the accuracy of classifier trained on the generated dataset as the main metric. The best model achieved INLINEFORM0 accuracy, which is only INLINEFORM1 less than the accuracy of the classifier trained on the original human written dataset, while the best dataset combined with the original dataset has achieved the highest accuracy. This model learns a decoder and a mapping embedding for each training example. It outperforms the more standard encoder-decoder networks. Although more parameters are needed to be trained, less are updated on each batch. We have also shown that the attention mechanism improves the model. The analysis has confirmed our hypothesis that a good dataset contains accurate, non-trivial and comprehensible examples. To further examine the quality of generated hypothesis, they were compared against the original human written hypotheses. The discriminative evaluation shows that in INLINEFORM2 of cases the human evaluator incorrectly distinguished between the original and the generated hypothesis. The discriminative model was actually better in distinguishing. We have also compared the accuracy of classifier to other metrics. The standard text generation metrics ROUGE and METEOR do not indicate if a generated dataset is good for training a classifier.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Which datasets are used?,Sample Answer,1901.09755-Background-3,1901.09755-System-2,1901.09755-Error Analysis-5,1901.09755-Error Analysis-9,1901.09755-Error Analysis-12,"Finally, it should be noted that there is a closely related task, namely, the SemEval 2016 task on Stance Detection. Stance detection is related to ABSA, but there is a significant difference. In ABSA the task is to determine whether a piece of text is positive, negative, or neutral with respect to an aspect and a given target (which in Stance Detection is called “author's favorability” towards a given target). However, in Stance Detection the text may express opinion or sentiment about some other target, not mentioned in the given text, and the targets are predefined, whereas in ABSA the targets are open-ended.","The clustering features look for the cluster class of the incoming token in one or more of the clustering lexicons induced following the three methods listed above. If found, then the class is added as feature (“not found” otherwise). As we work on a 5 token window, for each token and clustering lexicon at least 5 features are generated. For Brown, the number of features generated depend on the number of nodes found in the path for each token and clustering lexicon used.",Example (2): this place is a keeper!,"Finally, type (c) errors are usually caused by lack of generalization of our system to deal with unknown targets. Example (4-7) contain various mentions to the “Ray's” restaurant, which is in the top 5 errors for the English 2016 test set.",Example (5): We were only in Seattle for one night and I'm so glad we picked Rays for dinner!,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How did they obtain the tweets?,Sample Answer,1907.04072-Blackmarket Services-0,1907.04072-Proposed Model-4,1907.04072-Proposed Model-5,1907.04072-Baseline Methods-1,1907.04072-Experimental Results-1,"blackAs studied in BIBREF0 , there are two prevalent models of blackmarket services, namely premium and freemium. Premium services are only available upon payment from customers, whereas freemium services offer both paid and unpaid options. The unpaid services are available to the users when they contribute to the blackmarket by providing appraisals for other users' content. Here, we mainly concentrate on freemium services. The freemium services can be further divided into three categories: (i) social-share services (request customers to spread the content on social media), (ii) credit-based services (customers earn credits by providing appraisals, and can then use the credits earned to gain appraisals for their content), and (iii) auto-time retweet services (customers need to provide their Twitter access tokens, upon which their content is retweeted 10-20 times for each 15-minute window).","As shown in Figure FIGREF21 , the inputs are fed into separate fully connected (FC) layers with cross-stitch units stacked between successive layers. The cross-stitch units find the best shared representations using linear combinations, and learn the optimal linear combinations for a given set of tasks. The cross-stitch units essentially allow us to unify two separate networks for two separate tasks into a single network wherein each layer of the network shares the parameters from the other network using linear combinations. The network also employs batch-normalization and dropout to avoid overfitting.","The output layer of the first task classifies tweets as blackmarket or genuine using a cross entropy loss function. The output layer of the second task predicts the numerical values for the number of retweets and likes that a tweet will gain after five days of being posted by using a Mean Squared Error (MSE) loss. Note that the performance of the secondary task is not of importance to us, however, the secondary task helps the primary task. Therefore, we focus on the performance of the model in the primary task during training and evaluation.","Spam Detection 1: We use the Twitter spam detection method proposed by Wu et al. BIBREF4 . It uses the Word2Vec and Doc2Vec models to encode the tweets into a vector representation, which is fed to a MLP classifier in order to classify the tweets as spam or not-spam. We use the same methodology to classify tweets in our dataset as blackmarket or genuine.","blackWe analyse the false negatives generated by our model to find which type of tweets the model finds difficult to classify. The percentage of each class in the false negatives is as follows: Promotional - 23.29%, Politics - 10.96%, Entertainment - 21.92%, News - 9.59%, Spam - 5.48%, and Others - 28.77%. We observe that the tweets belonging to the category Others are difficult to classify since they are similar to genuine tweets in terms of content. The results also indicate that our model is robust while classifying blackmarket tweets belonging to the following categories – News, Spam and Politics.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What baseline do they compare to?,Sample Answer,1907.04072-Introduction-0,1907.04072-Blackmarket Services-0,1907.04072-Baseline Methods-1,1907.04072-Experimental Results-1,1907.04072-Conclusion-0,"Twitter is an important medium for people and companies to promote their products, ideologies, or to reach out and connect with other people in the form of micro-conversations. Twitter provides users with multiple ways of showing their support towards a tweet in the form of Likes, Retweets and Quotes. These content-level appraisals help in spreading the content further and act as a measure of users' agreement on the value of the content. The count of these content-level appraisals therefore determines the influence of a particular tweet and its author. This has led to the creation of certain blackmarket services such as FreeFollowers (https://www.freefollowers.io/), Like4Like (https://like4like.org/), YouLikeHits (https://www.youlikehits.com/), JustRetweet (http://justretweet.com), which allow users to post their tweets in order to gain inorganic appraisals in the form of Likes, Retweets and Quotes BIBREF0 , BIBREF1 .","blackAs studied in BIBREF0 , there are two prevalent models of blackmarket services, namely premium and freemium. Premium services are only available upon payment from customers, whereas freemium services offer both paid and unpaid options. The unpaid services are available to the users when they contribute to the blackmarket by providing appraisals for other users' content. Here, we mainly concentrate on freemium services. The freemium services can be further divided into three categories: (i) social-share services (request customers to spread the content on social media), (ii) credit-based services (customers earn credits by providing appraisals, and can then use the credits earned to gain appraisals for their content), and (iii) auto-time retweet services (customers need to provide their Twitter access tokens, upon which their content is retweeted 10-20 times for each 15-minute window).","Spam Detection 1: We use the Twitter spam detection method proposed by Wu et al. BIBREF4 . It uses the Word2Vec and Doc2Vec models to encode the tweets into a vector representation, which is fed to a MLP classifier in order to classify the tweets as spam or not-spam. We use the same methodology to classify tweets in our dataset as blackmarket or genuine.","blackWe analyse the false negatives generated by our model to find which type of tweets the model finds difficult to classify. The percentage of each class in the false negatives is as follows: Promotional - 23.29%, Politics - 10.96%, Entertainment - 21.92%, News - 9.59%, Spam - 5.48%, and Others - 28.77%. We observe that the tweets belonging to the category Others are difficult to classify since they are similar to genuine tweets in terms of content. The results also indicate that our model is robust while classifying blackmarket tweets belonging to the following categories – News, Spam and Politics.","In this paper, we presented a novel multitask learning approach to solve the problem of identification of tweets that are submitted to blackmarket services, without the use of any temporal features. To sum up, our contributions are three-fold: (i) Characterization: We proposed 12 tweet content based features that are useful in the task of identifying blackmarket tweets, (ii) Classification: We developed a novel Multitask Learning based model to classify tweets as blackmarket tweets or genuine tweets, (iii) Dataset: We collected a dataset consisting of tweets that have been submitted to blackmarket services in order to gain inorganic appraisals.",1.0,1.0,1.0,1.0,1.0,0.22222222222222224,0.25,0.2
What language is explored in this paper?,Sample Answer,1907.04072-Introduction-0,1907.04072-Introduction-4,1907.04072-Analysis of Blackmarket Tweets-0,1907.04072-Baseline Methods-1,1907.04072-Experimental Results-1,"Twitter is an important medium for people and companies to promote their products, ideologies, or to reach out and connect with other people in the form of micro-conversations. Twitter provides users with multiple ways of showing their support towards a tweet in the form of Likes, Retweets and Quotes. These content-level appraisals help in spreading the content further and act as a measure of users' agreement on the value of the content. The count of these content-level appraisals therefore determines the influence of a particular tweet and its author. This has led to the creation of certain blackmarket services such as FreeFollowers (https://www.freefollowers.io/), Like4Like (https://like4like.org/), YouLikeHits (https://www.youlikehits.com/), JustRetweet (http://justretweet.com), which allow users to post their tweets in order to gain inorganic appraisals in the form of Likes, Retweets and Quotes BIBREF0 , BIBREF1 .","We show that our multitask learning approach outperforms Twitter spam detection approaches, as well as state-of-the-art classifiers by 14.1% (in terms of F1-score), achieving an F1-score of 0.89 on our dataset. In short, the contributions of the paper are threefold: a new dataset, characterization of blackmarket tweets, and a novel multitask learning framework to detect tweets posted on blackmarket services.","To further understand the purpose of the collusive users behind the usage of blackmarket services, we annotated blackmarket tweets in our test set into a few discrete categories. The statistics of the categories are as follows: Promotional - 43.75%, Entertainment - 15.89%, Spam - 13.57%, News - 7.86%, Politics - 4.82%, and Others - 14.11%. We considered a tweet as Promotional only if the tweet attempts to promote a website/product. Most of the tweets in the Others category include personal tweets without any call to action or promotion, but this also can be considered as self-promotion. We further noticed that there were about 5% of normal tweets on concerning issues such as “pray for ..."", indicating that blackmarket services are also being used for non-business purposes. 99% of tweets other than the tweets from Others class included at least one URL, and 100% of the URLs in the blackmarket tweets were shortened.","Spam Detection 1: We use the Twitter spam detection method proposed by Wu et al. BIBREF4 . It uses the Word2Vec and Doc2Vec models to encode the tweets into a vector representation, which is fed to a MLP classifier in order to classify the tweets as spam or not-spam. We use the same methodology to classify tweets in our dataset as blackmarket or genuine.","blackWe analyse the false negatives generated by our model to find which type of tweets the model finds difficult to classify. The percentage of each class in the false negatives is as follows: Promotional - 23.29%, Politics - 10.96%, Entertainment - 21.92%, News - 9.59%, Spam - 5.48%, and Others - 28.77%. We observe that the tweets belonging to the category Others are difficult to classify since they are similar to genuine tweets in terms of content. The results also indicate that our model is robust while classifying blackmarket tweets belonging to the following categories – News, Spam and Politics.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
By how much do they improve on domain classification?,Sample Answer,2003.03728-Shortlister Model-2,2003.03728-Shortlister Model ::: Deriving Pseudo Labels-5,2003.03728-Shortlister Model ::: Self-distillation-1,2003.03728-Shortlister Model ::: Combined Loss-1,2003.03728-4-Table1-1.png,,,,,"Table 1. Evaluation results on various metrics (%). pseudo, neg feed, and self dist denote using derived pseudo labels, negative feedback, and self-distillation, respectively.",1.0,1.0,1.0,1.0,1.0,0.28571428571428575,0.5,0.2
Which dataset do they evaluate on?,Sample Answer,2003.03728-Shortlister Model-1,2003.03728-Shortlister Model ::: Deriving Pseudo Labels-5,2003.03728-Shortlister Model ::: Self-distillation-1,2003.03728-Shortlister Model ::: Combined Loss-1,2003.03728-4-Table2-1.png,"Given an input utterance and its target label, binary cross entropy is used as the baseline loss function as follows:",,,,Table 2. Examples of additional pseudo labels.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How do they decide by how much to decrease confidences of incorrectly predicted domains?,Sample Answer,2003.03728-Introduction-0,2003.03728-Shortlister Model-0,2003.03728-Shortlister Model ::: Deriving Pseudo Labels-0,2003.03728-2-Figure1-1.png,2003.03728-4-Table2-1.png,"Domain classification is a task that predicts the most relevant domain given an input utterance BIBREF0. It is becoming more challenging since recent conversational interaction systems such as Amazon Alexa, Google Assistant, and Microsoft Cortana support more than thousands of domains developed by external developers BIBREF3, BIBREF2, BIBREF4. As they are independently and rapidly developed without a centralized ontology, multiple domains have overlapped capabilities that can process the same utterances. For example, “make an elephant sound” can be processed by AnimalSounds, AnimalNoises, and ZooKeeper domains.","Our shortlister architecture is shown in Figure FIGREF3. The words of an input utterance are represented as contextualized word vectors by bidirectional long short-term memory (BiLSTM) on top of the word embedding layer BIBREF14. Then, the concatenation of the last outputs of the forward LSTM and the backward LSTM is used to represent the utterance as a vector. Following BIBREF2 and BIBREF17, we leverage the domain enablement information through attention mechanism BIBREF18, where the weighted sum of enabled domain vectors followed by sigmoid activation is concatenated to the utterance vector for representing a personalized utterance. On top of the personalized utterance vector, a feed-forward neural network followed by sigmoid activation is used to obtain $n$-dimensional output vector $o$, where the prediction confidence of each domain is represented as a scalar value between 0 and 1.","We hypothesize that the outputs repeatedly predicted with the highest confidences are indeed correct labels in many cases in multi-label PU learning setting. This approach is closely related to pseudo labeling BIBREF7 or self-training BIBREF8, BIBREF9, BIBREF10 in semi-supervised learning since our model is supervised with additional pseudo labels, but differs in that our approach assigns pseudo labels to singly labeled train sets rather than unlabeled data sets.","Fig. 1. Shortlister architecture: an input utterance is represented as a concatenation of the utterance vector from BiLSTM and the weighted sum of domain enablement vectors through domain enablement attention mechanism. Then, a feed-forward neural network followed by sigmoid activation represents the n-dimensional output vector.",Table 2. Examples of additional pseudo labels.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What languages are represented in the dataset?,Sample Answer,1910.06748-Introduction-1,1910.06748-Proposed Model ::: ngam-regional CNN Model-0,1910.06748-Proposed Model ::: ngam-regional CNN Model-1,1910.06748-Proposed Model ::: ngam-regional CNN Model-2,1910.06748-Experimental Results ::: Analysis-3,"Given its massive scale, multilingual nature, and popularity, Twitter has naturally attracted the attention of the LID research community. Several attempts have been made to construct LID datasets from that resource. However, a major challenge is to assign each tweet in the dataset to the correct language among the more than 70 languages used on the platform. The three commonly-used approaches are to rely on human labeling BIBREF4, BIBREF5, machine detection BIBREF5, BIBREF6, or user geolocation BIBREF3, BIBREF7, BIBREF8. Human labeling is an expensive process in terms of workload, and it is thus infeasible to apply it to create a massive dataset and get the full benefit of Twitter's scale. Automated LID labeling of this data creates a noisy and imperfect dataset, which is to be expected since the purpose of these datasets is to create new and better LID algorithms. And user geolocation is based on the assumption that users in a geographic region use the language of that region; an assumption that is not always correct, which is why this technique is usually paired with one of the other two. Our first contribution in this paper is to propose a new approach to build and automatically label a Twitter LID dataset, and to show that it scales up well by building a dataset of over 18 million labeled tweets. Our hope is that our new Twitter dataset will become a benchmarking standard in the LID literature.","To begin, we present a traditional CNN with an ngam-regional constrain as our baseline. CNNs have been widely used in both image processing BIBREF15 and NLP BIBREF16. The convolution operation of a filter with a region size $m$ is parameterized by a weight matrix $\mathbf {W}_{cnn} \in \mathbb {R}^{d_{cnn}\times md}$ and a bias vector $\mathbf {b}_{cnn} \in \mathbb {R}^{d_{cnn}}$, where $d_{cnn}$ is the dimension of the CNN. The inputs are a sequence of $m$ consecutive input columns in $\mathbf {X}$, represented by a concatenated vector $\mathbf {X}[i:i+m-1] \in \mathbb {R}^{md}$. The region-based feature vector $\mathbf {c}_i$ is computed as follows:",where $\oplus $ denotes a concatenation operation and $g$ is a non-linear function. The region filter is slid from the beginning to the end of $\mathbf {X}$ to obtain a convolution matrix $\mathbf {C}$:,"The first novelty of our CNN is that we add a zero-padding constrain at both sides of $\mathbf {X}$ to ensure that the number of columns in $\mathbf {C}$ is equal to the number of columns in $\mathbf {X}$. Consequently, each $\mathbf {c}_i$ feature vector corresponds to an $\mathbf {x}_i$ input vector at the same index position $i$, and is learned from concatenating the surrounding $m$-gram embeddings. Particularly:","The last two lines of Table TABREF33 report the results of our basic CNN and our attention CNN LID systems. It can be seen that both of them outperform the benchmark systems in accuracy, precision, recall, and F1 score in all experiments. Moreover, the attention CNN outperforms the basic CNN in every metric (we will explore the benefit of the attention mechanism in the next subsection). In terms of processing speed, only the CLD2 system surpasses ours, but it does so at the cost of a 10% drop in accuracy and F1 score. Looking at the choice of datasets, it can be seen that training with the large-scale dataset leads to a nearly 1% improvement compared to the medium-sized dataset, which also gives a 1% improvement compared to the small-scale dataset. While it is expected that using more training data will lead to a better system and better results, the small improvement indicates that even our small-scale dataset has sufficient messages to allow the network training to converge.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is task success rate achieved? ,Sample Answer,1911.11744-Introduction-1,1911.11744-Introduction ::: Problem Statement:-0,1911.11744-Background-0,1911.11744-Conclusion and Future Work-0,1911.11744-3-Figure1-1.png,"In this paper, we present an imitation learning approach that combines language, vision, and motion in order to synthesize natural language-conditioned control policies that have strong generalization capabilities while also capturing the semantics of the task. We argue that such a multi-modal teaching approach enables robots to acquire complex policies that generalize to a wide variety of environmental conditions based on descriptions of the intended task. In turn, the network produces control parameters for a lower-level control policy that can be run on a robot to synthesize the corresponding motion. The hierarchical nature of our approach, i.e., a high-level policy generating the parameters of a lower-level policy, allows for generalization of the trained task to a variety of spatial, visual and contextual changes.","In order to outline our problem statement, we contrast our approach to Imitation learning BIBREF0 which considers the problem of learning a policy $\mathbf {\pi }$ from a given set of demonstrations ${\cal D}=\lbrace \mathbf {d}^0,.., \mathbf {d}^m\rbrace $. Each demonstration spans a time horizon $T$ and contains information about the robot's states and actions, e.g., demonstrated sensor values and control inputs at each time step. Robot states at each time step within a demonstration are denoted by $\mathbf {x}_t$. In contrast to other imitation learning approaches, we assume that we have access to the raw camera images of the robot $_t$ at teach time step, as well as access to a verbal description of the task in natural language. This description may provide critical information about the context, goals or objects involved in the task and is denoted as $\mathbf {s}$. Given this information, our overall objective is to learn a policy $\mathbf {\pi }$ which imitates the demonstrated behavior, while also capturing semantics and important visual features. After training, we can provide the policy $\mathbf {\pi }(\mathbf {s},)$ with a different, new state of the robot and a new verbal description (instruction) as parameters. The policy will then generate the control signals needed to perform the task which takes the new visual input and semantic context int o account.","A fundamental challenge in imitation learning is the extraction of policies that do not only cover the trained scenarios, but also generalize to a wide range of other situations. A large body of literature has addressed the problem of learning robot motor skills by imitation BIBREF6, learning functional BIBREF1 or probabilistic BIBREF7 representations. However, in most of these approaches, the state vector has to be carefully designed in order to ensure that all necessary information for adaptation is available. Neural approaches to imitation learning BIBREF8 circumvent this problem by learning suitable feature representations from rich data sources for each task or for a sequence of tasks BIBREF9, BIBREF10, BIBREF11. Many of these approaches assume that either a sufficiently large set of motion primitives is already available or that a taxonomy of the task is available, i.e., semantics and motions are not trained in conjunction. The importance of maintaining this connection has been shown in BIBREF12, allowing the robot to adapt to untrained variations of the same task. To learn entirely new tasks, meta-learning aims at learning policy parameters that can quickly be fine-tuned to new tasks BIBREF13. While very successful in dealing with visual and spatial information, these approaches do not incorporate any semantic or linguistic component into the learning process. Language has shown to successfully generate task descriptions in BIBREF14 and several works have investigated the idea of combining natural language and imitation learning: BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19. However, most approaches do not utilize the inherent connection between semantic task descriptions and low-level motions to train a model.","In this work, we presented an imitation learning approach combining language, vision, and motion. A neural network architecture called Multimodal Policy Network was introduced which is able to learn the cross-modal relationships in the training data and achieve high generalization and disambiguation performance as a result. Our experiments showed that the model is able to generalize towards different locations and sentences while maintaining a high success rate of delivering an object to a desired bowl. In addition, we discussed an extensions of the method that allow us to obtain uncertainty information from the model by utilizing stochastic network outputs to get a distribution over the belief.","Figure 1: Network architecture overview. The network consists of two parts, a high-level semantic network and a low-level control network. Both networks are working seamlessly together and are utilized in an End-to-End fashion.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Does proposed end-to-end approach learn in reinforcement or supervised learning manner?,Sample Answer,1911.11744-Introduction-0,1911.11744-Background-0,1911.11744-Multimodal Policy Generation via Imitation-5,1911.11744-Results-2,1911.11744-3-Figure1-1.png,"A significant challenge when designing robots to operate in the real world lies in the generation of control policies that can adapt to changing environments. Programming such policies is a labor and time-consuming process which requires substantial technical expertise. Imitation learning BIBREF0, is an appealing methodology that aims at overcoming this challenge – instead of complex programming, the user only provides a set of demonstrations of the intended behavior. These demonstrations are consequently distilled into a robot control policy by learning appropriate parameter settings of the controller. Popular approaches to imitation, such as Dynamic Motor Primitives (DMPs) BIBREF1 or Gaussian Mixture Regression (GMR) BIBREF2 largely focus on motion as the sole input and output modality, i.e., joint angles, forces or positions. Critical semantic and visual information regarding the task, such as the appearance of the target object or the type of task performed, is not taken into account during training and reproduction. The result is often a limited generalization capability which largely revolves around adaptation to changes in the object position. While imitation learning has been successfully applied to a wide range of tasks including table-tennis BIBREF3, locomotion BIBREF4, and human-robot interaction BIBREF5 an important question is how to incorporate language and vision into a differentiable end-to-end system for complex robot control.","A fundamental challenge in imitation learning is the extraction of policies that do not only cover the trained scenarios, but also generalize to a wide range of other situations. A large body of literature has addressed the problem of learning robot motor skills by imitation BIBREF6, learning functional BIBREF1 or probabilistic BIBREF7 representations. However, in most of these approaches, the state vector has to be carefully designed in order to ensure that all necessary information for adaptation is available. Neural approaches to imitation learning BIBREF8 circumvent this problem by learning suitable feature representations from rich data sources for each task or for a sequence of tasks BIBREF9, BIBREF10, BIBREF11. Many of these approaches assume that either a sufficiently large set of motion primitives is already available or that a taxonomy of the task is available, i.e., semantics and motions are not trained in conjunction. The importance of maintaining this connection has been shown in BIBREF12, allowing the robot to adapt to untrained variations of the same task. To learn entirely new tasks, meta-learning aims at learning policy parameters that can quickly be fine-tuned to new tasks BIBREF13. While very successful in dealing with visual and spatial information, these approaches do not incorporate any semantic or linguistic component into the learning process. Language has shown to successfully generate task descriptions in BIBREF14 and several works have investigated the idea of combining natural language and imitation learning: BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19. However, most approaches do not utilize the inherent connection between semantic task descriptions and low-level motions to train a model.","where $f_G()$ and $f_H()$ are multilayer-perceptrons that use $$ after being processed in a single perceptron with weight $_G$ and bias $_G$. These parameters are then used in the third part of the network, which is a DMP BIBREF0, allowing us leverage a large body of research regarding their behavior and stability, while also allowing other extensions of DMPs BIBREF5, BIBREF24, BIBREF25 to be incorporated to our framework.","The generated parameters of the low-level DMP controller – the weights and goal position – must be sufficiently accurate in order to successfully deliver the object to the specified bin. On the right side of Figure FIGREF4, the generated weights for the DMP are shown for two tasks in which the target is close and far away from the robot, located at different sides of the table, indicating the robots ability to generate differently shaped trajectories. The accuracy of the goal position can be seen in Figure FIGREF4(left) which shows another aspect of our approach: By using stochastic forward passes BIBREF26 the model can return an estimate for the validity of a requested task in addition to the predicted goal configuration. The figure shows that the goal position of a red bowl has a relatively small distribution independently of the used sentence or location on the table, where as an invalid target (green) produces a significantly larger distribution, indicating that the requested task may be invalid.","Figure 1: Network architecture overview. The network consists of two parts, a high-level semantic network and a low-level control network. Both networks are working seamlessly together and are utilized in an End-to-End fashion.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How do they obtain word lattices from words?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How do they match annotators to instances?,Sample Answer,1905.07791-Application Domain-1,1905.07791-Quantifying Task Difficulty-5,1905.07791-Experimental Setup and Results-3,1905.07791-Removing Difficult Examples-0,1905.07791-Re-weighting by Difficulty-0,"Table 1 shows an example of difficult and easy examples according to our definition of difficulty. The underlined text demarcates the (consensus) reference label provided by domain experts. In the difficult examples, crowd workers marked text distinct from these reference annotations; whereas in the easy cases they reproduced them with reasonable fidelity. The difficult sentences usually exhibit complicated structure and feature jargon.","We show the distribution of the automatically labeled scores for sentences that do contain spans in Figure 1 . The mean of the Population (p) sentence scores is significantly lower than that for other types of sentences (i and o), again indicating that they are easier on average to annotate. This aligns with a previous finding that annotating Interventions and Outcomes is more difficult than annotating Participants BIBREF5 .","Table 3 reports Pearson correlation coefficients between the predictions with each of the neural models and the ground truth difficulty scores. Rows 1-4 correspond to individual models, and row 5 reports the ensemble performance. Columns correspond to label type. Results from all models outperform the baseline SVR model: Pearson's correlation coefficients range from 0.550 to 0.622. The regression correlations are the lowest.","We first evaluate changes in performance induced by training the sequence labeling model using less data by removing difficult sentences prior to training. The hypothesis here is that these difficult instances are likely to introduce more noise than signal. We used a cross-fold approach to predict sentence difficulties, training on 9/10ths of the data and scoring the remaining 1/10th at a time. We then sorted sentences by predicted difficulty scores, and experimented with removing increasing numbers of these (in order of difficulty) prior to training the LSTM-CRF-Pattern model.","We showed above that removing a small number of the most difficult sentences does not harm, and in fact modestly improves, medical IE model performance. However, using the available data we are unable to test if this will be useful in practice, as we would need additional data to determine how many difficult sentences should be dropped.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How much data is needed to train the task-specific encoder?,Sample Answer,1905.07791-Introduction-2,1905.07791-Quantifying Task Difficulty-5,1905.07791-Experimental Setup and Results-5,1905.07791-Removing Difficult Examples-1,1905.07791-Re-weighting by Difficulty-2,"Can we predict item difficulty? We define a training instance as difficult if a lay annotator or an automated model disagree on its labeling. We show that difficulty can be predicted, and that it is distinct from inter-annotator agreement. Further, such predictions can be used during training to improve information extraction models.","We show the distribution of the automatically labeled scores for sentences that do contain spans in Figure 1 . The mean of the Population (p) sentence scores is significantly lower than that for other types of sentences (i and o), again indicating that they are easier on average to annotate. This aligns with a previous finding that annotating Interventions and Outcomes is more difficult than annotating Participants BIBREF5 .","For all models, correlations for Intervention and Outcomes are higher than for Population, which is expected given the difficulty distributions in Figure 1 . In these, the sentences are more uniformly distributed, with a fair number of difficult and easier sentences. By contrast, in Population there are a greater number of easy sentences and considerably fewer difficult sentences, which makes the difficulty ranking task particularly challenging.","Figure 3 shows the results achieved by the LSTM-CRF-Pattern model after discarding increasing amounts of the training data: the $x$ and $y$ axes correspond to the the percentage of data removed and F1 scores, respectively. We contrast removing sentences predicted to be difficult with removing them (a) randomly (i.i.d.), and, (b) in inverse order of predicted inter-annotator agreement. The agreement prediction model is trained exactly the same like difficult prediction model, with simply changing the difficult score to annotation agreement. F1 scores actually improve (marginally) when we remove the most difficult sentences, up until we drop 4% of the data for Population and Interventions, and 6% for Outcomes. Removing training points at i.i.d. random degrades performance, as expected. Removing sentences in order of disagreement seems to have similar effect as removing them by difficulty score when removing small amount of the data, but the F1 scores drop much faster when removing more data. These findings indicate that sentences predicted to be difficult are indeed noisy, to the extent that they do not seem to provide the model useful signal.","Table 4 reports the precision, recall and F1 achieved both with and without sentence re-weighting. Re-weighting improves all metrics modestly but consistently. All F1 differences are statistically significant under a sign test ( $p<0.01$ ). The model with best precision is different for Patient, Intervention and Outcome labels. However re-weighting by difficulty does consistently yield the best recall for all three extraction types, with the most notable improvement for i and o, where recall improved by 10 percentage points. This performance increase translated to improvements in F1 across all types, as compared to the base model and to re-weighting by agreement.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the hyperparameters of the bi-GRU?,Sample Answer,1907.09369-Introduction-2,1907.09369-Related Work-1,1907.09369-Related Work-3,1907.09369-Data and preparation-2,1907.09369-Model-0,"By adding the complexity of language and the fact that emotion expressions are very complex and context dependant BIBREF6 , BIBREF7 , BIBREF8 , we can see why detecting emotions in textual data is a challenging task. This difficulty can be seen when human annotators try to assign emotional labels to the text, but using various techniques the annotation task can be accomplished with desirable agreement among the annotators BIBREF9 .","Considering the complexity of the task, and the fact that these models lose a lot of information by using simpler models such as the bag of words model (BOW) or lexicon features, these attempts lead to methods which are not reusable and generalizable. Further improvement in classification algorithms, and trying out new paths is necessary in order to improve the performance of emotion detection methods. Some suggestions that were less present in the literature, are to develop methods that go above lexical representations and consider the flow of the language.","In this work, we argue that creating a model that can better capture the context and sequential nature of text , can significantly improve the performance in the hard task of emotion detection. We show this by using a recurrent neural network-based classifier that can learn to create a more informative latent representation of the target text as a whole, and we show that this can improve the final performance significantly. Based on that, we suggest focusing on methodologies that increase the quality of these latent representations both contextually and emotionally, can improve the performance of these models. Based on this assumption we propose a deep recurrent neural network architecture to detect discrete emotions in a tweet dataset. The code can be accessed at GitHub [https://github.com/armintabari/Emotion-Detection-RNN].","As Twitter is against polishing this many tweets, Wang et al. provided the tweet ids along with their label. For our experiment, we retrieved the tweets in Wang et al.'s dataset by tweet IDs. As the dataset is from 7 years ago We could only download over 1.3 million tweets from around 2.5M tweet IDs in the dataset. The distribution of the data can be seen in Table TABREF5 .","In this section, we introduce the deep neural network architecture that we used to classify emotions in the tweets dataset. Emotional expressions are more complex and context-dependent even compared to other forms of expressions based mostly on the complexity and ambiguity of human emotions and emotional expressions and the huge impact of context on the understanding of the expressed emotion. These complexities are what led us to believe lexicon-based features like is normally used in conventional machine learning approaches are unable to capture the intricacy of emotional expressions.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the size of the second dataset?,Sample Answer,1909.07734-Introduction-1,1909.07734-Datasets-3,1909.07734-Challenge Details-1,1909.07734-Submissions ::: AlexU-0,1909.07734-Evaluation & Discussion ::: Deep Learning Models.-0,"Detecting and recognizing emotions is a difficult task for machines. Nevertheless, following the successful use of computational linguistics to analyze sentiment in texts, there is growing interest in the more difficult task of the automatic detection and classification of emotions in texts.","The labeled emotion is calculated using the absolute majority of votes. Thus, if a specific emotion received three or more votes, then that utterance is labeled with that emotion. If there is no majority vote, the utterance is labeled with “non-neutral” label. In addition to the utterance, annotation, and label, each line in each dialogue includes the speaker's name (in the case of EmotionPush, a speaker ID was used). The emotion distribution for Friends and EmotionPush, for both training and evaluation data, is shown in Table .","The label distribution of emotions in our data are highly unbalanced, as can be seen in Figure FIGREF6. Due to the small number of three of the labels, participants were instructed to use only four emotions for labels: joy, sadness, anger, and neutral. Evaluation of submissions was done using only utterances with these four labels. Utterances with labels other than the above four (i.e., surprise, disgust, fear or non-neutral) were discarded and not used in the evaluation.","BIBREF13 The classifier uses a pre-trained BERT model followed by a feed-forward neural network with a softmax output. Due to the overwhelming presence of the neutral label, a classifying cascade is employed, where the majority classifier is first used to decide whether the utterance should be classified with “neutral” or not. A second classifier is used to focus on the other emotions (joy, sadness, and anger). Dealing with the imbalanced classes is done through the use of a weighted loss function.","Most of the submissions used deep learning models. Five of the models were based on the BERT architecture, with some using pre-trained BERT. Some of the submissions enhanced the model by adding context and speaker related encoding to improve performance. We also received submissions using more traditional networks such as CNN, as well as machine learning classics such as SVM. The results demonstrate that domain knowledge, feature engineering, and careful application of existing methodologies is still paramount for building successful machine learning models.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How large is the first dataset?,Sample Answer,1909.07734-Introduction-1,1909.07734-Datasets-3,1909.07734-Submissions ::: AlexU-0,1909.07734-Evaluation & Discussion-0,1909.07734-Evaluation & Discussion ::: Emotional Model and Annotation Challenges.-0,"Detecting and recognizing emotions is a difficult task for machines. Nevertheless, following the successful use of computational linguistics to analyze sentiment in texts, there is growing interest in the more difficult task of the automatic detection and classification of emotions in texts.","The labeled emotion is calculated using the absolute majority of votes. Thus, if a specific emotion received three or more votes, then that utterance is labeled with that emotion. If there is no majority vote, the utterance is labeled with “non-neutral” label. In addition to the utterance, annotation, and label, each line in each dialogue includes the speaker's name (in the case of EmotionPush, a speaker ID was used). The emotion distribution for Friends and EmotionPush, for both training and evaluation data, is shown in Table .","BIBREF13 The classifier uses a pre-trained BERT model followed by a feed-forward neural network with a softmax output. Due to the overwhelming presence of the neutral label, a classifying cascade is employed, where the majority classifier is first used to decide whether the utterance should be classified with “neutral” or not. A second classifier is used to focus on the other emotions (joy, sadness, and anger). Dealing with the imbalanced classes is done through the use of a weighted loss function.","An evaluation summary of the submissions is available in Tables and . We only present the teams that submitted technical reports. A full leaderboard that includes all the teams is available on the challenge website. This section highlights some observations related to the challenge. Identical utterances can convey different emotions in different contexts. A few of the models incorporated the dialogue context into the model, such as the models proposed by teams IDEA and KU.","The discrete 6-emotion model and similar models are often used in emotion detection tasks. However, such 1-out-of-n models are limited in a few ways: first, expressed emotions are often not discrete but mixed (for example, surprise and joy or surprise and anger are often manifested in the same utterance). This leads to more inter-annotator disagreement, as annotators can only select one emotion. Second, there are additional emotional states that are not covered by the basic six emotions but are often conveyed in speech and physical expressions, such as desire, embarrassment, relief, and sympathy. This is reflected in feedback we received from one of the AMT workers: “I am doing my best on your HITs. However, the emotions given (7 of them) are a lot of times not the emotion I'm reading (such as questioning, happy, excited, etc). Your emotions do not fit them all...”.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Who was the top-scoring team?,Sample Answer,1909.07734-Datasets-3,1909.07734-Challenge Details-1,1909.07734-Submissions ::: Podlab-0,1909.07734-Submissions ::: CYUT-0,1909.07734-Evaluation & Discussion ::: Emotional Model and Annotation Challenges.-0,"The labeled emotion is calculated using the absolute majority of votes. Thus, if a specific emotion received three or more votes, then that utterance is labeled with that emotion. If there is no majority vote, the utterance is labeled with “non-neutral” label. In addition to the utterance, annotation, and label, each line in each dialogue includes the speaker's name (in the case of EmotionPush, a speaker ID was used). The emotion distribution for Friends and EmotionPush, for both training and evaluation data, is shown in Table .","The label distribution of emotions in our data are highly unbalanced, as can be seen in Figure FIGREF6. Due to the small number of three of the labels, participants were instructed to use only four emotions for labels: joy, sadness, anger, and neutral. Evaluation of submissions was done using only utterances with these four labels. Utterances with labels other than the above four (i.e., surprise, disgust, fear or non-neutral) were discarded and not used in the evaluation.",BIBREF12 A support vector machine (SVM) was used for classification. Words are ranked using a per-emotion TF-IDF score. Experiments were performed to verify whether the previous utterance would improve classification performance. Input to the Linear SVM was done using one-hot-encoding of top ranking words.,"BIBREF16 A word embedding layer followed by a bi-directional GRU-based RNN. Output from the RNN was fed into a single-node classifier. The augmented dataset was used for training the model, but “neutral”-labeled utterances were filtered to deal with class imbalance.","The discrete 6-emotion model and similar models are often used in emotion detection tasks. However, such 1-out-of-n models are limited in a few ways: first, expressed emotions are often not discrete but mixed (for example, surprise and joy or surprise and anger are often manifested in the same utterance). This leads to more inter-annotator disagreement, as annotators can only select one emotion. Second, there are additional emotional states that are not covered by the basic six emotions but are often conveyed in speech and physical expressions, such as desire, embarrassment, relief, and sympathy. This is reflected in feedback we received from one of the AMT workers: “I am doing my best on your HITs. However, the emotions given (7 of them) are a lot of times not the emotion I'm reading (such as questioning, happy, excited, etc). Your emotions do not fit them all...”.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what were the baselines?,Sample Answer,1904.03288-Jasper Architecture-1,1904.03288-Jasper Architecture-2,1904.03288-Jasper Architecture-4,1904.03288-Language Model-1,1904.03288-NovoGrad-1,"Each block input is connected directly into the last sub-block via a residual connection. The residual connection is first projected through a 1x1 convolution to account for different numbers of input and output channels, then through a batch norm layer. The output of this batch norm layer is added to the output of the batch norm layer in the last sub-block. The result of this sum is passed through the activation function and dropout to produce the output of the sub-block.","The sub-block architecture of Jasper was designed to facilitate fast GPU inference. Each sub-block can be fused into a single GPU kernel: dropout is not used at inference-time and is eliminated, batch norm can be fused with the preceding convolution, ReLU clamps the result, and residual summation can be treated as a modified bias term in this fused operation.","We also build a variant of Jasper, Jasper Dense Residual (DR). Jasper DR follows DenseNet BIBREF15 and DenseRNet BIBREF16 , but instead of having dense connections within a block, the output of a convolution block is added to the inputs of all the following blocks. While DenseNet and DenseRNet concatenates the outputs of different layers, Jasper DR adds them in the same way that residuals are added in ResNet. As explained below, we find addition to be as effective as concatenation.","We experiment with statistical N-gram language models BIBREF23 and neural Transformer-XL BIBREF11 models. Our best results use acoustic and word-level N-gram language models to generate a candidate list using beam search with a width of 2048. Next, an external Transformer-XL LM rescores the final list. All LMs were trained on datasets independently from acoustic models. We show results with the neural LM in our Results section. We observed a strong correlation between the quality of the neural LM (measured by perplexity) and WER as shown in Figure FIGREF20 .","At each step INLINEFORM0 , NovoGrad computes the stochastic gradient INLINEFORM1 following the regular forward-backward pass. Then the second-order moment INLINEFORM2 is computed for each layer INLINEFORM3 similar to ND-Adam BIBREF27 : DISPLAYFORM0 ",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what competitive results did they obtain?,Sample Answer,1904.03288-Introduction-0,1904.03288-Jasper Architecture-2,1904.03288-Language Model-0,1904.03288-NovoGrad-1,1904.03288-2-Table1-1.png,"Conventional automatic speech recognition (ASR) systems typically consist of several independently learned components: an acoustic model to predict context-dependent sub-phoneme states (senones) from audio, a graph structure to map senones to phonemes, and a pronunciation model to map phonemes to words. Hybrid systems combine hidden Markov models to model state dependencies with neural networks to predict states BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . Newer approaches such as end-to-end (E2E) systems reduce the overall complexity of the final system.","The sub-block architecture of Jasper was designed to facilitate fast GPU inference. Each sub-block can be fused into a single GPU kernel: dropout is not used at inference-time and is eliminated, batch norm can be fused with the preceding convolution, ReLU clamps the result, and residual summation can be treated as a modified bias term in this fused operation.","A language model (LM) is a probability distribution over arbitrary symbol sequences INLINEFORM0 such that more likely sequences are assigned high probabilities. LMs are frequently used to condition beam search. During decoding, candidates are evaluated using both acoustic scores and LM scores. Traditional N-gram LMs have been augmented with neural LMs in recent work BIBREF20 , BIBREF21 , BIBREF22 .","At each step INLINEFORM0 , NovoGrad computes the stochastic gradient INLINEFORM1 following the regular forward-backward pass. Then the second-order moment INLINEFORM2 is computed for each layer INLINEFORM3 similar to ND-Adam BIBREF27 : DISPLAYFORM0 ","Table 1: Jasper 10x5: 10 blocks, each consisting of 5 1Dconvolutional sub-blocks, plus 4 additional blocks.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the size of the new dataset?,Sample Answer,1902.09666-Introduction-1,1902.09666-Related Work-4,1902.09666-Level C: Offensive Language Target Identification-0,1902.09666-Level C: Offensive Language Target Identification-2,1902.09666-Level C: Offensive Language Target Identification-3,"Recently, Waseem et. al. ( BIBREF12 ) acknowledged the similarities among prior work and discussed the need for a typology that differentiates between whether the (abusive) language is directed towards a specific individual or entity or towards a generalized group and whether the abusive content is explicit or implicit. Wiegand et al. ( BIBREF11 ) followed this trend as well on German tweets. In their evaluation, they have a task to detect offensive vs not offensive tweets and a second task for distinguishing between the offensive tweets as profanity, insult, or abuse. However, no prior work has explored the target of the offensive language, which is important in many scenarios, e.g., when studying hate speech with respect to a specific target.",Bullying detection: Several studies have been published on bullying detection. One of them is the one by xu2012learning which apply sentiment analysis to detect bullying in tweets. xu2012learning use topic models to to identify relevant topics in bullying. Another related study is the one by dadvar2013improving which use user-related features such as the frequency of profanity in previous messages to improve bullying detection.,"Level C categorizes the targets of insults and threats as individual (IND), group (GRP), and other (OTH).","Individual (IND): Posts targeting an individual. It can be a a famous person, a named individual or an unnamed participant in the conversation. Insults and threats targeted at individuals are often defined as cyberbulling.","Group (GRP): The target of these offensive posts is a group of people considered as a unity due to the same ethnicity, gender or sexual orientation, political affiliation, religious belief, or other common characteristic. Many of the insults and threats targeted at a group correspond to what is commonly understood as hate speech.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What kinds of offensive content are explored?,Sample Answer,1902.09666-Introduction-5,1902.09666-Related Work-11,1902.09666-Level B: Categorization of Offensive Language-0,1902.09666-Experiments and Evaluation-1,1902.09666-2-Table1-1.png,"Using this annotation model, we create a new large publicly available dataset of English tweets. The key contributions of this paper are as follows:",,Level B categorizes the type of offense and two labels are used: targeted (TIN) and untargeted (INT) insults and threats.,"Our models are trained on the training data, and evaluated by predicting the labels for the held-out test set. The distribution is described in Table TABREF15 . We evaluate and compare the models using the macro-averaged F1-score as the label distribution is highly imbalanced. Per-class Precision (P), Recall (R), and F1-score (F1), also with other averaged metrics are also reported. The models are compared against baselines of predicting all labels as the majority or minority classes.","Table 1: Four tweets from the dataset, with their labels for each level of the annotation schema.",1.0,1.0,1.0,1.0,1.0,0.14285714285714285,0.1111111111111111,0.2
How long is the dataset for each step of hierarchy?,Sample Answer,1902.09666-Related Work-4,1902.09666-Level A: Offensive language Detection-3,1902.09666-Level B: Categorization of Offensive Language-0,1902.09666-Level B: Categorization of Offensive Language-2,1902.09666-Level C: Offensive Language Target Identification-3,Bullying detection: Several studies have been published on bullying detection. One of them is the one by xu2012learning which apply sentiment analysis to detect bullying in tweets. xu2012learning use topic models to to identify relevant topics in bullying. Another related study is the one by dadvar2013improving which use user-related features such as the frequency of profanity in previous messages to improve bullying detection.,"Offensive (OFF): We label a post as offensive if it contains any form of non-acceptable language (profanity) or a targeted offense, which can be veiled or direct. This category includes insults, threats, and posts containing profane language or swear words.",Level B categorizes the type of offense and two labels are used: targeted (TIN) and untargeted (INT) insults and threats.,"Untargeted (UNT): Posts containing non-targeted profanity and swearing. Posts with general profanity are not targeted, but they contain non-acceptable language.","Group (GRP): The target of these offensive posts is a group of people considered as a unity due to the same ethnicity, gender or sexual orientation, political affiliation, religious belief, or other common characteristic. Many of the insults and threats targeted at a group correspond to what is commonly understood as hate speech.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How is module that analyzes behavioral state trained?,Sample Answer,1909.00107-Introduction-2,1909.00107-Introduction-3,1909.00107-Experimental Setup ::: Data ::: Behavior Related Corpora-1,1909.00107-Experimental Setup ::: Data ::: Behavior Related Corpora-2,1909.00107-Results ::: Penn Tree Bank Corpus ::: Previous state-of-the-art architectures-0,"The use of topic information to provide semantic context to language models has also been studied extensively BIBREF9, BIBREF10, BIBREF11, BIBREF12. Topic models are useful for extracting high level semantic structure via latent topics which can aid in better modeling of longer documents.","Recently, however, empirical studies involving investigation of different network architectures, hyper-parameter tuning, and optimization techniques have yielded better performance than the addition of contextual information BIBREF13, BIBREF14. In contrast to the majority of work that focus on improving the neural network aspects of RNNLM, we introduce psycholinguistic signals along with linguistic units to improve the fundamental language model.","Couples Therapy Corpus: This corpus comprises of dyadic conversations between real couples seeking marital counseling. The dataset consists of audio, video recordings along with their transcriptions. Each speaker is rated by multiple annotators over 33 behaviors. The dataset comprises of approximately 0.83 million words with 10,000 unique entries of which 0.5 million is used for training (0.24m for dev and 88k for test).","Cancer Couples Interaction Dataset: This dataset was gathered as part of a observational study of couples coping with advanced cancer. Advanced cancer patients and their spouse caregivers were recruited from clinics and asked to interact with each other in two structured discussions: neutral discussion and cancer related. Interactions were audio-recorded using small digital recorders worn by each participant. Manually transcribed audio has approximately 230,000 word tokens with a vocabulary size of 8173.","Finally we apply behavior gating on a previous state-of-the-art architecture, one that is most often used as a benchmark over various recent works. Specifically, we employ the AWD-LSTM proposed by BIBREF2 with QRNN BIBREF25 instead of LSTM. We observe positive results with AWD-LSTM augmented with behavior-gating providing a relative improvement of (1.42% on valid) 0.66% in perplexity (Table TABREF17).",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Which of the two ensembles yields the best performance?,Sample Answer,1911.03681-Introduction-2,1911.03681-LAMA-1,1911.03681-LAMA ::: LAMA-UHN-0,1911.03681-LAMA ::: LAMA-UHN-2,1911.03681-E-BERT ::: E-BERT.-3,"In §SECREF3, we propose E-BERT, a simple mapping-based extension of BERT that replaces entity mentions with wikipedia2vec entity embeddings BIBREF3. In §SECREF4, we show that E-BERT rivals BERT and the recently proposed entity-enhanced ERNIE model BIBREF2 on LAMA. E-BERT has a substantial lead over both baselines on LAMA-UHN; furthermore, ensembles of E-BERT and BERT outperform all baselines on original LAMA.","The LAMA probing task follows this schema: Given a KB triple of the form (S, R, O), the object is elicited with a relation-specific cloze-style question, e.g., (Jean_Marais, native-language, French) becomes: “The native language of Jean Marais is [MASK].” The LM predicts a distribution over a limited vocabulary to replace [MASK], which is evaluated against the known gold answer.","It is often possible to guess properties of an entity from its name, with zero factual knowledge of the entity itself. This is because entities are often named according to implicit or explicit rules (e.g., the cultural norms involved in naming a child, copyright laws for industrial products, or simply a practical need for descriptive names). LAMA makes guessing even easier by its limited vocabulary, which may only contain a few candidates for a particular entity type.","Filter 1: The string match filter deletes all KB triples where the correct answer (e.g., Apple) is a case-insensitive substring of the subject entity name (e.g., Apple Watch). This simple heuristic deletes up to 81% of triples from individual relations (see Appendix for statistics and examples).",E-BERT replaces the subwords that correspond to the entity mention with the symbolic entity: The native language of Jean_Marais is [MASK] .,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what preprocessing method is introduced?,Sample Answer,1611.00440-Model Accuracy Test-4,1611.00440-Prediction Accuracy Test-4,1611.00440-Discussion-0,1611.00440-Discussion-1,1611.00440-Conclusion-0,"The test results do not show exact effect of training data and the model accuracy. Models with smaller number of training data (e.g. Huckabee's, Santorum's) achieve higher accuracy than models with larger number of training data (e.g. Trump's, Clinton's), while the lowest accuracy is achieved by Kasich's, which is trained with small number of training data. The undefined value of INLINEFORM0 and INLINEFORM1 scores on Christie's, Gilmore's, and Santorum's model shows extreme predictions on these models.","Overall prediction accuracy can be calculated by subtracting one with the average result of error rate division on each party by number of its remaining candidates. We achieve 0.548 prediction accuracy, which is not good enough BIBREF1 . The model accuracy is mainly affected by the large error rate on Democratic candidates (1.33 from 2 candidates).","Using simple preprocessed data, our Naive Bayesian model successfully achieves 95.8% accuracy on 10-fold cross validation and gets 54.8% accuracy on predicting the poll result. The model predicts Ted Cruz and Bernie Sanders as the nominee of Republican and Democratic Party respectively. Based on the positive predictions, it predicts that Bernie Sanders will be elected as the 2016 U.S. President.","Although it has 95.8% accuracy during the model test, the model's prediction does not represent the poll. Table III shows that model's accuracy is not dependent of its number of training data. Model with less training data (e.g. Mike Huckabee's) can perform perfectly during the model test and only misses a rank on the prediction, whereas model with more training data (e.g. Donald Trump's) can have worse performance.","We built Naive Bayesian predictive models for 2016 U.S. Presidential Election. We use the official hashtag and simple preprocessing method to prepare the data without modifying its meaning. Our model achieves 95.8% accuracy during the model test and predicts the poll with 54.8% accuracy. The model predicts that Bernie Sanders and Ted Cruz will become the nominees of Democratic and Republican Party respectively, and the election will be won by Bernie Sanders.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Which dataset has been used in this work?,Sample Answer,1806.03125-Introduction-0,1806.03125-Introduction-3,1806.03125-Conventional text classification methods-19,1806.03125-TF weighted word subspace-0,1806.03125-Discussion-1,"Text classification has become an indispensable task due to the rapid growth in the number of texts in digital form available online. It aims to classify different texts, also called documents, into a fixed number of predefined categories, helping to organize data, and making easier for users to find the desired information. Over the past three decades, many methods based on machine learning and statistical models have been applied to perform this task, such as latent semantic analysis (LSA), support vector machines (SVM), and multinomial naive Bayes (MNB).","To solve these problems, neural networks have been employed to learn vector representations of words BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 . In particular, the word2vec representation BIBREF8 has gained attention. Given a training corpus, word2vec can generate a vector for each word in the corpus that encodes its semantic information. These word vectors are distributed in such a way that words from similar contexts are represented by word vectors with high correlation, while words from different contexts are represented by word vectors with low correlation.","Despite being robust tools for text classification, both these models depend directly on the bag-of-words features and do not naturally work with representations such as word2vec.","The word subspace formulation presented in Section ""Word subspace"" is a practical and compact way to represent sets of word vectors, retaining most of the variability of features. However, as seen in the BOW features, the frequency of words is relevant information that can improve the characterization of a text. To incorporate this information into the word subspace modeling, we propose an extension of the word subspace, called the term-frequency (TF) weighted word subspace.","The results from the text classification experiment showed that subspace-based methods performed better than the text classification methods discussed in this work. Ultimately, our proposed TF weighted word subspace with MSM surpassed all the other methods. word2vec features are reliable tools to represent the semantic meaning of the words and when treated as sets of word vectors, they are capable of representing the content of texts. However, despite the fact that word vectors can be treated separately, conventional methods such as SVM and LSA may not be suitable for text classification using word vectors.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What can word subspace represent?,Sample Answer,1806.03125-Conventional text classification methods-9,1806.03125-Conventional text classification methods-36,1806.03125-TF weighted word subspace-10,1806.03125-Experimental Evaluation-0,1806.03125-Discussion-2,"As for the posterior $P(d_i|c_j)$ , different calculations are performed for each model. For MVB, it is defined as: ","If the training data is linearly separable, we can select two hyperplanes in a way that there are no points between them and then try to maximize the distance. In other words, minimize $\Vert {w}\Vert $ subject to $c_i({w}\cdot {x}_u-b) \ge 1, i=\lbrace 1,2,...,n\rbrace $ . If the training data is not linearly separable, the kernel trick can be applied, where every dot product is replaced by a non-linear kernel function.","Text classification with TF weighted word subspace can also be performed under the framework of MSM. In this paper, we will refer to MSM with TF weighted word subspace as TF-MSM.","In this section we describe the experiments performed to demonstrate the validity of our proposed method and its extension. We used the Reuters-8 dataset without stop words from BIBREF27 aiming at single-label classification, which is a preprocessed format of the Reuters-21578. Words in the texts were considered as they appeared, without performing stemming or typo correction. This database has eight different classes with the number of samples varying from 51 to over 3000 documents, as can be seen in Table 1 .","Among the conventional methods, LSA and SVM achieved about 86% and 89%, respectively, when using bag-of-words features. Interestingly, both methods had better performance when using binary weights. For LSA, we can see that despite the slight differences in the performance, tfidfBOW required approximations with smaller dimensions. SVM had the lowest accuracy rate when using the tfidfBOW features. One possible explanation for this is that TF-IDF weights are useful when rare words and very frequent words exist in the corpus, giving higher weights for rare words and lower weights for common words. Since we removed the stop words, the most frequent words among the training documents were not considered and, therefore, using TF-IDF weights did not improve the results.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Which neural architecture do they use as a base for their attention conflict mechanisms?,Sample Answer,1906.08593-Conflict model-2,1906.08593-Task 1: Quora Duplicate Question Pair Detection-0,1906.08593-Task 2: Ranking questions in Bing's People Also Ask-0,1906.08593-Qualitative Comparison-2,1906.08593-Qualitative Comparison-6,where INLINEFORM0 INLINEFORM1 INLINEFORM2 is a parameter that we introduce to provide a weight for the pair. The two word representations INLINEFORM3 and INLINEFORM4 are projected to a space where their element wise difference can be used to model their dissimilarity and softmax applied on them can produce high probability to more dissimilar word pairs.,"The dataset includes pairs of questions labelled as 1 or 0 depending on whether a pair is duplicate or not respectively. This is a popular pair-level classification task on which extensive work has already been done before like BIBREF7 , BIBREF8 . For this task, we make the output layer of our model to predict two probabilities for non-duplicate and duplicate. We sample the data from the original dataset so that it contains equal positive and negative classes. Original dataset has some class imbalance but for sake simplicity we don't consider it. The final data that we use has roughly 400,000 question pairs and we split this data into train and test using 8:2 ratio.","People Also Ask is a feature in Bing search result page where related questions are recommended to the user. User may click on a question to view the answer. Clicking is a positive feedback that shows user's interest in the question. We use this click logs to build a question classifier using the same model in Figure 3. The problem statement is very similar to BIBREF10 where they use logistic regression to predict whether an user would click on ad. Our goal is to classify if a question is potential high-click question or not for a given query. For this, we first create a labelled data set using the click logs where any question having CTR lower than 0.3 is labelled as 0 and a question having CTR more than 0.7 as 1.",Sequence 2: How do I learn french genders ?,Sequence 1: How do I prevent breast cancer ?,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
By how much does their model outperform existing methods?,Sample Answer,1705.03261-Introduction-0,1705.03261-Introduction-2,1705.03261-Related Work-2,1705.03261-Word Level Attention-0,1705.03261-2-Figure1-1.png,"Drug-drug interaction (DDI) is a situation when one drug increases or decreases the effect of another drug BIBREF0 . Adverse drug reactions may cause severe side effect, if two or more medicines were taken and their DDI were not investigated in detail. DDI is a common cause of illness, even a cause of death BIBREF1 . Thus, DDI databases for clinical medication decisions are proposed by some researchers. These databases such as SFINX BIBREF2 , KEGG BIBREF3 , CredibleMeds BIBREF4 help physicians and pharmacists avoid most adverse drug reactions.","There has been many efforts to automatically extract DDIs from natural language BIBREF0 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , mainly medical literature and clinical records. These works can be divided into the following categories:","Drug-drug interaction extraction is a relation extraction task of natural language processing. Relation extraction aims to determine the relation between two given entities in a sentence. In recent years, attention mechanism and various neural networks are applied to relation extraction BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 . Convolutional deep neural network are utilized for extracting sentence level features in BIBREF19 . Then the sentence level features are concatenated with lexical level features, which are obtained by NLP toolkit WordNet BIBREF22 , followed by a multilayer perceptron (MLP) to classify the entities' relation. A fixed work is proposed by Nguyen et al. BIBREF21 . The convolutional kernel is set various size to capture more n-gram features. In addition, the word and position embedding are trained automatically instead of keeping constant as in BIBREF19 . Wang et al. BIBREF20 introduce multi-level attention mechanism to CNN in order to emphasize the keywords and ignore the non-critical words during relation detection. The attention CNN model outperforms previous state-of-the-art methods.","The purpose of word level attention layer is to extract sentence representation (also known as feature vector) from encoded matrix. We use word level attention instead of max pooling, since attention mechanism can determine the importance of individual encoded word in each row of INLINEFORM0 . Let INLINEFORM1 denotes the attention vector (column vector), INLINEFORM2 denotes the filter that gives each element in the row of INLINEFORM3 a weight. The following equations shows the attention operation, which is also illustrated in figure FIGREF15 . DISPLAYFORM0 DISPLAYFORM1 ",Fig. 1. Partial records in DDI corpus,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the performance of their model?,Sample Answer,1705.03261-Introduction-0,1705.03261-Introduction-1,1705.03261-Word Level Attention-0,1705.03261-DDI Prediction-1,1705.03261-Experimental Results-1,"Drug-drug interaction (DDI) is a situation when one drug increases or decreases the effect of another drug BIBREF0 . Adverse drug reactions may cause severe side effect, if two or more medicines were taken and their DDI were not investigated in detail. DDI is a common cause of illness, even a cause of death BIBREF1 . Thus, DDI databases for clinical medication decisions are proposed by some researchers. These databases such as SFINX BIBREF2 , KEGG BIBREF3 , CredibleMeds BIBREF4 help physicians and pharmacists avoid most adverse drug reactions.","Traditional DDI databases are manually constructed according to clinical records, scientific research and drug specifications. For instance, The sentence “With combined use, clinicians should be aware, when phenytoin is added, of the potential for reexacerbation of pulmonary symptomatology due to lowered serum theophylline concentrations BIBREF5 ”, which is from a pharmacotherapy report, describe the side effect of phenytoin and theophylline's combined use. Then this information on specific medicines will be added to DDI databases. As drug-drug interactions have being increasingly found, manually constructing DDI database would consume a lot of manpower and resources.","The purpose of word level attention layer is to extract sentence representation (also known as feature vector) from encoded matrix. We use word level attention instead of max pooling, since attention mechanism can determine the importance of individual encoded word in each row of INLINEFORM0 . Let INLINEFORM1 denotes the attention vector (column vector), INLINEFORM2 denotes the filter that gives each element in the row of INLINEFORM3 a weight. The following equations shows the attention operation, which is also illustrated in figure FIGREF15 . DISPLAYFORM0 DISPLAYFORM1 ","The DDI prediction follows the procedure described in Section SECREF6 - SECREF26 . The given sentence is eventually represented by feature vector INLINEFORM0 . Then INLINEFORM1 is classified to a specific DDI type with a softmax classifier. In next section, we will evaluate our model's DDI prediction performance and see the advantages and shortcomings of our model.","Whether a dynamic or static word embedding is better for a DDI extraction task is under consideration. Nguyen et al. BIBREF21 shows that updating word embedding at the time of other parameters being trained makes a better performance in relation extraction task. We let the embedding be static when training, while other conditions are all the same. The “RNN + static word embedding + 2ATT” curve shows this case. We can draw a conclusion that updating the initialized word embedding trains more suitable word vectors for the task, which promotes the performance.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How do they generate the synthetic dataset?,Sample Answer,1710.10609- Model-3,1710.10609- Model-6,1710.10609-SimCluster Algorithm-9,1710.10609-Evaluation and Results-1,1710.10609-Evaluation and Results-3,"We define the induced centroids INLINEFORM0 as the arithmetic means of the points INLINEFORM1 s such that INLINEFORM2 's have the same cluster assigned to them. Similarly, we define INLINEFORM3 as the arithmetic means of INLINEFORM4 s such that INLINEFORM5 s have the same cluster assigned to them. More formally, we define these induced centroids as:- INLINEFORM6 ",Then the matching INLINEFORM0 is defined to be the bijective function which maximizes INLINEFORM1 . We consider a term in the cost function corresponding to the sum of distances between the original centroids and the matched induced centroids. Our overall cost function is now given by:- INLINEFORM2 ,"Finally we update the matching between the clusters. To do so, we need to find a bipartite matching match on the cluster indices so as to maximize INLINEFORM0 . We use Hungarian algorithm BIBREF13 to perform the same i.e. we define a bipartite graph with vertices consisting of cluster indices in the two domains. There is an edge from vertex representing cluster indices j (in domain 1) and j' in domain 2, with weight N(j,j'). We find a maximum weight bipartite matching in this graph.","ARI (Adjusted Rand Index): Standard Rand Index is a metric used to check the clustering quality against a given standard set of clusters by comparing the pairwise clustering decisions. It is defined as INLINEFORM0 , where a is the number of true positive pairs, b is the number of true negative pairs, c is the number of false positive pairs and d is the number of false negative pairs. Adjusted rand index corrects the standard rand index for chance and is defined as INLINEFORM1 BIBREF15 .",F1 scores: We also report F1 scores for the pairwise clustering decisions. In the above notation we considered the pair-precision as INLINEFORM0 and recall as INLINEFORM1 . The F1 measure is the Harmonic mean given as INLINEFORM2 .,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How are their changes evaluated?,Sample Answer,1907.05403-Introduction-0,1907.05403-The RASA NLU Pipeline-1,1907.05403-Incrementalizing RASA Components-1,1907.05403-Incrementalizing RASA Components-3,1907.05403-Appendix-3,"There is no shortage of services that are marketed as natural language understanding (nlu) solutions for use in chatbots, digital personal assistants, or spoken dialogue systems (sds). Recently, Braun2017 systematically evaluated several such services, including Microsoft LUIS, IBM Watson Conversation, API.ai, wit.ai, Amazon Lex, and RASA BIBREF0 . More recently, Liu2019b evaluated LUIS, Watson, RASA, and DialogFlow using some established benchmarks. Some nlu services work better than others in certain tasks and domains with a perhaps surprising pattern: RASA, the only fully open-source nlu service among those evaluated, consistently performs on par with the commercial services.","Figure FIGREF7 shows a schematic of a pipeline for three components. The context (i.e., training data) is passed to Component A which performs its training, then persists a trained model for that component. Then the data is passed through Component A as input for Component B which also trains and persists, and so on for Component C. During runtime, the persisted models are loaded into memory and together form the nlu module.","The Message class in RASA nlu is the main message bus between components in the pipeline. Message follows a blackboard approach to passing information between components. For example, in a pipeline containing a tokenizer, intent classifier, and entity extractor, each of the components would store the tokens, intent class, and entities in the Message object, respectively. Our modifications to Message were minimal; we simply used it to store ius and corresponding edit types (i.e., ADD or REVOKE).","The Interpreter class in RASA nlu is the main interface between user input (e.g., asr) and the series of components in the pipeline. On training, the Interpreter prepares the training data, and serially calls train on each of the components in the pipeline. Similarly, to process input, one uses the Interpreter’s parse method, where the Interpreter prepares the input (i.e., the ongoing utterance) and serially calls process on the components in the pipeline (analgous to left buffer updates in the iu framework). As a result of its design, we were able to leverage the Interpreter class for incremental processing, notably because of its use of a persistent Message object as a bus of communication between Components.","- name: ""intent_..._tensorflow_embedding""",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How many samples did they generate for the artificial language?,Sample Answer,"1906.00180-Zero-shot, compositional generalization-1",1906.00180-Unseen words-4,1906.00180-Unseen words-9,1906.00180-Discussion & Conclusions-0,1906.00180-12-Figure5-1.png,"In this section, we report more positive results on compositional reasoning of our Siamese networks. We focus on zero-shot generalization: correct classification of examples of a type that has not been observed before. Provided that atomic constituents and production rules are understood, compositionality does not require that abundantly many instances embodying a semantic category are observed. We will consider in turn what set-up is required to demonstrate zero-shot generalization to unseen lengths, and to generalization to sentences composed of novel words.","Examples of ontological twins in the taxonomy of nouns $\mathcal {N}^{\mathcal {L}}$ are `Romans' and `Venetians' . This can easily be verified in the Venn diagram of Figure 1 by replacing `Romans' with `Venetians' and observing that the same hierarchy applies. The same holds for e.g. `Germans' and `Polish' or for `children' and `students'. For several such word-twin pairs the GRU is evaluated with respect to the fragment of the test data containing the original word $w$ , and with respect to that same fragment after replacing the original word with ontological twin $t(w)$ . Results are shown in Table 7 .","What happens when we consider ontologies that have the same structure, but are thematically very different from the original ontology? Three such alternative hierarchies are considered: $r_{animals}$ , $r_{religion}$ and $r_{America}$ . Each of these functions relocalizes the noun ontology in a totally different domain of discourse, as indicated by their names. Table 9 specifies the functions and their effect.","We established that our Siamese recurrent networks (with SRN, GRU or LSTM cells) are able to recognize logical entailment relations without any a priori cues about syntax or semantics of the input expressions. Indeed, some of the recurrent set-ups even outperform tree-shaped networks, whose topology is specifically designed to deal with such tasks. This indicates that recurrent networks can develop representations that can adequately process a formal language with a nontrivial hierarchical structure. The formal language we defined did not exploit the full expressive power of first-order predicate logic; nevertheless by using standard first-order predicate logic, a standard theorem prover, and a set-up where the training set only covers a tiny fraction of the space of possible logical expressions, our experiments avoid the problems observed in earlier attempts to demonstrate logical reasoning in recurrent networks.","Figure 5: Confusion matrices of the best-performing GRU with respect to the test set. Rows represent targets, columns predictions. (a) row-normalized results for all test instances. (b) unnormalized results for misclassified test instances. Clearly, most errors are due to unrecognized or wrongly attributed independence.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How were the ngram models used to generate predictions on the data?,Sample Answer,1704.08390-Introduction-0,1704.08390-Introduction-1,1704.08390-Corpus Preparation and Pre-processing-2,1704.08390-Discussion and Future Work-2,1704.08390-Discussion and Future Work-3,"Humor is an expression of human uniqueness and intelligence and has drawn attention in diverse areas such as linguistics, psychology, philosophy and computer science. Computational humor draws from all of these fields and is a relatively new area of study. There is some history of systems that are able to generate humor (e.g., BIBREF0 , BIBREF1 ). However, humor detection remains a less explored and challenging problem (e.g., BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 ).","SemEval-2017 Task 6 BIBREF6 also focuses on humor detection by asking participants to develop systems that learn a sense of humor from the Comedy Central TV show, @midnight with Chris Hardwick. Our system ranks tweets according to how funny they are by training N-gram language models on two different corpora. One consisting of funny tweets provided by the task organizers, and the other on a freely available research corpus of news data. The funny tweet data is made up of tweets that are intended to be humorous responses to a hashtag given by host Chris Hardwick during the program.",Pre-processing consists of two steps: filtering and tokenization. The filtering step was only for the tweet training corpus. We experimented with various filtering and tokenziation combinations during the development stage to determine the best setting.,"These results suggest that there are only slight differences between bigram and trigram models, and that the type and quantity of corpora used to train the models is what really determines the results.","The task description paper BIBREF6 reported system by system results for each hashtag. We were surprised to find that our performance on the hashtag file #BreakUpIn5Words in the evaluation stage was significantly better than any other system on both Subtask A (with accuracy of 0.913) and Subtask B (with distance score of 0.636). While we still do not fully understand the cause of these results, there is clearly something about the language used in this hashtag that is distinct from the other hashtags, and is somehow better represented or captured by a language model. Reaching a better understanding of this result is a high priority for future work.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What rank did the language model system achieve in the task evaluation?,Sample Answer,1704.08390-Introduction-1,1704.08390-Corpus Preparation and Pre-processing-0,1704.08390-Corpus Preparation and Pre-processing-3,1704.08390-Tweet Prediction-0,1704.08390-Tweet Prediction-1,"SemEval-2017 Task 6 BIBREF6 also focuses on humor detection by asking participants to develop systems that learn a sense of humor from the Comedy Central TV show, @midnight with Chris Hardwick. Our system ranks tweets according to how funny they are by training N-gram language models on two different corpora. One consisting of funny tweets provided by the task organizers, and the other on a freely available research corpus of news data. The funny tweet data is made up of tweets that are intended to be humorous responses to a hashtag given by host Chris Hardwick during the program.","The tweet data was provided by the task organizers. It consists of 106 hashtag files made up of about 21,000 tokens. The hashtag files were further divided into a development set trial_dir of 6 hashtags and a training set of 100 hashtags train_dir. We also obtained 6.2 GB of English news data with about two million tokens from the News Commentary Corpus and the News Crawl Corpus from 2008, 2010 and 2011. Each tweet and each sentence from the news data is found on a single line in their respective files.","Filtering removes the following elements from the tweets: URLs, tokens starting with the “@” symbol (Twitter user names), and tokens starting with the “#” symbol (Hashtags).","The system sorts all the tweets for each hashtag and orders them based on their log probability score, where the funniest tweet should be listed first. If the scores are based on the tweet language model then they are sorted in ascending order since the log probability value closest to 0 indicates the tweet that is most like the (funny) tweets model. However, if the log probability scores are based on the news data then they are sorted in descending order since the largest value will have the smallest probability associated with it and is therefore least like the (unfunny) news model.","For Subtask A, the system goes through the sorted list of tweets in a hashtag file and compares each pair of tweets. For each pair, if the first tweet was funnier than the second, the system would output the tweet_ids for the pair followed by a “1”. If the second tweet is funnier it outputs the tweet_ids followed by a “0”. For Subtask B, the system outputs all the tweet_ids for a hashtag file starting from the funniest.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the models evaluated on?,Sample Answer,1908.10449-Introduction-2,1908.10449-iMRC: Making MRC Interactive ::: Query Types-0,1908.10449-iMRC: Making MRC Interactive ::: Query Types-2,1908.10449-Baseline Agent ::: Model Structure ::: Action Generator-1,1908.10449-6-Table4-1.png,"The key idea behind our proposed interactive MRC (iMRC) is to restrict the document context that a model observes at one time. Concretely, we split a supporting document into its component sentences and withhold these sentences from the model. Given a question, the model must issue commands to observe sentences in the withheld set; we equip models with actions such as Ctrl+F (search for token) and stop for searching through partially observed documents. A model searches iteratively, conditioning each command on the input question and the sentences it has observed previously. Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL).","Given the question “When is the deadline of AAAI?”, as a human, one might try searching “AAAI” on a search engine, follow the link to the official AAAI website, then search for keywords “deadline” or “due date” on the website to jump to a specific paragraph. Humans have a deep understanding of questions because of their significant background knowledge. As a result, the keywords they use to search are not limited to what appears in the question.","One token from the question: the setting with smallest action space. Because iMRC deals with Ctrl+F commands by exact string matching, there is no guarantee that all sentences are accessible from question tokens only.","Here, the size of $L_{shared} \in \mathbb {R}^{95 \times 150}$; $L_{action}$ has an output size of 4 or 2 depending on the number of actions available; the size of $L_{ctrlf}$ is the same as the size of a dataset's vocabulary size (depending on different query type settings, we mask out words in the vocabulary that are not query candidates). The overall Q-value is simply the sum of the two components:","Table 4: Experimental results on test set. #Action 4 denotes the settings as described in the action space section, #Action 2 indicates the setting where only Ctrl+F and stop are available. F1info indicates an agent’s F1 score iff sufficient information is in its observation.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what dataset was used for training?,Sample Answer,1903.02930-Introduction-0,1903.02930-Introduction-5,1903.02930-Model-0,1903.02930-Combining the text and video modalities-1,1903.02930-Location of combination-0, INLINEFORM0 Work performed while the author was an intern at Google.,Our work is distinguishable from previous work with respect to three dimensions:,A language model assigns to a sentence INLINEFORM0 the probability: INLINEFORM1 ,"In addition to concatenating the word and visual embedding, we explore two variants of our model that allow for a finer-grained integration of the two modalities:",We explore three locations for fusing visual features in an RNNLM (Figure ). Our Early Fusion strategy merges the text and the visual features at the input to the LSTM cells. This embodies the intuition that it is best to do feature combination at the earliest possible stage. The Middle Fusion merges the visual features at the output of the 1st LSTM layer while the Late Fusion strategies merges the two features after the final LSTM layer. The idea behind the Middle and Late fusion is that we would like to minimize changes to the regular RNNLM architecture at the early stages and still be able to benefit from the visual features.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what is the size of the training data?,Sample Answer,1903.02930-Introduction-1,1903.02930-Introduction-3,1903.02930-Combining the text and video modalities-1,1903.02930-Combining the text and video modalities-4,1903.02930-Location of combination-0,"Language models are vital components of a wide variety of systems for Natural Language Processing (NLP) including Automatic Speech Recognition, Machine Translation, Optical Character Recognition, Spelling Correction, etc. However, most language models are trained and applied in a manner that is oblivious to the environment in which human language operates BIBREF0 . These models are typically trained only on sequences of words, ignoring the physical context in which the symbolic representations are grounded, or ignoring the social context that could inform the semantics of an utterance.","In this paper, we examine the problem of incorporating temporal visual context into a recurrent neural language model (RNNLM). Multimodal Neural Language Models were introduced in BIBREF12 , where log-linear LMs BIBREF13 were conditioned to handle both image and text modalities. Notably, this work did not use the recurrent neural model paradigm which has now become the de facto way of implementing neural LMs.","In addition to concatenating the word and visual embedding, we explore two variants of our model that allow for a finer-grained integration of the two modalities:","Here, we apply the intuition that some words could provide information as to whether or not the visual context is helpful. In a simplistic example, if the word history is the article “the,"" then the visual context could provide relevant information needed for predicting the next word. For other word histories, though, the visual context might not be needed or be even irrelevant for the next word prediction: if the previous word is “carpe"", the next word is very likely to be “diem"", regardless of visual context. We implement a simple weighting mechanism that learns a scalar weight for the visual embedding prior to concatenation with the word embedding. The input to the RNNLM is now INLINEFORM0 , where: INLINEFORM1 ",We explore three locations for fusing visual features in an RNNLM (Figure ). Our Early Fusion strategy merges the text and the visual features at the input to the LSTM cells. This embodies the intuition that it is best to do feature combination at the earliest possible stage. The Middle Fusion merges the visual features at the output of the 1st LSTM layer while the Late Fusion strategies merges the two features after the final LSTM layer. The idea behind the Middle and Late fusion is that we would like to minimize changes to the regular RNNLM architecture at the early stages and still be able to benefit from the visual features.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what features were derived from the videos?,Sample Answer,1903.02930-Introduction-3,1903.02930-Combining the text and video modalities-0,1903.02930-Combining the text and video modalities-2,1903.02930-Location of combination-0,1903.02930-4-Table3-1.png,"In this paper, we examine the problem of incorporating temporal visual context into a recurrent neural language model (RNNLM). Multimodal Neural Language Models were introduced in BIBREF12 , where log-linear LMs BIBREF13 were conditioned to handle both image and text modalities. Notably, this work did not use the recurrent neural model paradigm which has now become the de facto way of implementing neural LMs.","There are several options for combining the text and video modalities. We opt for the simplest strategy, which concatenates the representations. For a word embedding INLINEFORM0 and corresponding visual representation INLINEFORM1 , the input to our RNNLM will be the concatenated vector INLINEFORM2 . For the examples where we were unable to compute visual features (see Section § SECREF3 ), we set INLINEFORM3 to be a zero-vector.","In this case, the RNNLM is given as input a vector INLINEFORM0 that is a weighted sum of the two embeddings: INLINEFORM1 ",We explore three locations for fusing visual features in an RNNLM (Figure ). Our Early Fusion strategy merges the text and the visual features at the input to the LSTM cells. This embodies the intuition that it is best to do feature combination at the earliest possible stage. The Middle Fusion merges the visual features at the output of the 1st LSTM layer while the Late Fusion strategies merges the two features after the final LSTM layer. The idea behind the Middle and Late fusion is that we would like to minimize changes to the regular RNNLM architecture at the early stages and still be able to benefit from the visual features.,Table 3: Two sentences from YouCook2 with wordpiece-level negative log likelihood scores. Most gains (high-,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What private companies are members of consortium?,Sample Answer,2003.09244-Other European LT Programmes ::: Spain-1,2003.09244-Core Projects ::: Language Resources-3,2003.09244-Core Projects ::: Language Resources-5,2003.09244-Core Projects ::: NLP Tools-0,2003.09244-Core Projects ::: Machine Translation-9,"The plan was to facilitate the development of tools and linguistic resources. Examples of tools are named entity recognisers, word-sense disambiguation, tools for computing semantic similarity and text classification, automatic summarisation and MT. Examples of linguistic resources to be developed in the programme are parallel corpora, lists of proper nouns, terminology lists and dictionaries.","Treebanks. The largest of the syntactically parsed treebanks that exist is the Icelandic Parsed Historical Corpus (IcePaHC; Wallenberg et al., 2011; Rögnvaldsson et al., 2011, 2012), which contains one million words from the 12th to the 21st century. The scheme used for the syntactic annotation is based on the Penn Parsed Corpora of Historical English BIBREF24, BIBREF25. On the other hand, no Universal Dependencies (UD)-treebanks are available for Icelandic. Within the programme, a UD-treebank will by built, based on IcePaHC, and extended with new material.","Hyphenation tool. Hyphenation from one language to another often seems rather idiosyncratic but within one and the same language, such as Icelandic, such rules are often reasonably clear. A list of more than 200,000 Icelandic words with permissible hyphenations is available in the language resources repository. It will be expanded based on words from the DIM. A new hyphenation tool, trained on the extended list, will be built in the programme. The tool makes a suggestion for correct hyphenation possibilities of words that are not found on the hyphenation list.","A wide variety of NLP tools are to be developed or improved upon within the programme. It is of vital importance to develop quality NLP tools, as many tools often form a pipeline that analyses data and delivers the results to tools used by end users, and, in the pipeline, errors can accumulate and perpetuate.","Pre- and postprocessing. Preprocessing in MT is the task of changing the training corpus/source text in some manner for the purpose of making the translation task easier or mark particular words/phrases that should not be translated. Postprocessing is then the task of restoring the generated target language to its normal form. An example of pre- and postprocessing in our project is the handling of named entities (NEs). NEs are found and matched within source and target sentence pairs in the training corpus, and replaced by placeholders with information about case and singular/plural number. NE-to-placeholder substitution is implemented in the input and placeholder-to-NE substitution in the output pipelines of the translation system.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What concrete software is planned to be developed by the end of the programme?,Sample Answer,2003.09244-Introduction-2,2003.09244-Other European LT Programmes ::: Estonia-0,2003.09244-Core Projects ::: NLP Tools-0,2003.09244-Core Projects ::: Machine Translation-6,2003.09244-Core Projects ::: Machine Translation-9,"Icelandic is an example of a language with almost a negligible number of speakers, in terms of market size, since only about 350,000 people speak Icelandic as their native language. Icelandic is therefore seldom on the list of supported languages in LT software and applications.","Regarding LT, the Estonian situation is, in many ways, similar to that of Iceland: It has too few users for companies to see opportunities in embarking on development of (costly) LT, but on the other hand society is technologically advanced – people use, or want to be able to use, LT software. In Estonia, the general public wants Estonian to maintain its status, and like Icelandic, the language has a complex inflection system and very active word generation. The problems faced by Estonia are therefore not unlike those that Iceland faces.","A wide variety of NLP tools are to be developed or improved upon within the programme. It is of vital importance to develop quality NLP tools, as many tools often form a pipeline that analyses data and delivers the results to tools used by end users, and, in the pipeline, errors can accumulate and perpetuate.","Back-translation. In order to augment the training data, back-translated texts will be used. Monolingual Icelandic texts will be selected and translated to English with one of the baseline system (see below). By doing so, more training data can be obtained for the en$\rightarrow $is direction. An important part of using back-translated texts during training is filtering out translations that may otherwise lead to poor quality of the augmented part.","Pre- and postprocessing. Preprocessing in MT is the task of changing the training corpus/source text in some manner for the purpose of making the translation task easier or mark particular words/phrases that should not be translated. Postprocessing is then the task of restoring the generated target language to its normal form. An example of pre- and postprocessing in our project is the handling of named entities (NEs). NEs are found and matched within source and target sentence pairs in the training corpus, and replaced by placeholders with information about case and singular/plural number. NE-to-placeholder substitution is implemented in the input and placeholder-to-NE substitution in the output pipelines of the translation system.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
When did language technology start in Iceland?,Sample Answer,2003.09244-Introduction-6,2003.09244-Other European LT Programmes ::: Spain-2,2003.09244-Other European LT Programmes ::: Estonia-2,2003.09244-Core Projects ::: NLP Tools-5,2003.09244-Core Projects ::: Machine Translation-9,The third pillar of the programme is the revival of the joint Master's programme in LT at Reykjavik University (RU) and the University of Iceland (UI). The goal is further to increase the number of PhD students and to build strong knowledge centres for sustainable LT development in Iceland.,"The estimated total cost of the programme was 90 million euros. As the programme had just recently started when the Icelandic programme was being planned, we did not have any information on what went well and what could have been done better.","The National Programme for Estonian Language Technology was launched in 2006. The first phase ran from 2006 to 2010. All results of this first phase, language resources and software prototypes, were released as public domain. All such resources and tools are preserved long term and available from the Center of Estonian Language Resources. 33 projects were funded, which included the creation of reusable language resources and development of essential linguistic software, as well as bringing the relevant infrastructure up to date BIBREF5. The programme managed to significantly improve upon existing Estonian language resources, both in size, annotation and standardisation. In creating software, most noticeable results were in speech technology. Reporting on the results of the programme BIBREF5 stress that the first phase of the programme created favourable conditions for LT development in Estonia. According to an evaluation of the success of the programme, at least 84% of the projects had satisfactory results. The total budged for this first phase was 3.4 million euros.","PoS tagger. Precise PoS-tagging is important in many LT projects because information on word class or morphological features is often needed in later stages of an NLP pipeline. Improved tagging accuracy, thus often results in an improvement in the overall quality of LT software.","Pre- and postprocessing. Preprocessing in MT is the task of changing the training corpus/source text in some manner for the purpose of making the translation task easier or mark particular words/phrases that should not be translated. Postprocessing is then the task of restoring the generated target language to its normal form. An example of pre- and postprocessing in our project is the handling of named entities (NEs). NEs are found and matched within source and target sentence pairs in the training corpus, and replaced by placeholders with information about case and singular/plural number. NE-to-placeholder substitution is implemented in the input and placeholder-to-NE substitution in the output pipelines of the translation system.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the 12 languages covered?,Sample Answer,2003.04866-Lexical Semantic Similarity ::: Similarity and Language Variation: Semantic Typology-1,2003.04866-Monolingual Evaluation of Representation Learning Models ::: Models in Comparison-15,2003.04866-Monolingual Evaluation of Representation Learning Models ::: Results and Discussion-0,2003.04866-Monolingual Evaluation of Representation Learning Models ::: Results and Discussion-12,2003.04866-Conclusion and Future Work-13,"In general, semantic typology studies the variation in lexical semantics across the world's languages. According to BIBREF68, the ways languages categorize concepts into the lexicon follow three main axes: 1) granularity: what is the number of categories in a specific domain?; 2) boundary location: where do the lines marking different categories lie?; 3) grouping and dissection: what are the membership criteria of a category; which instances are considered to be more prototypical? Different choices with respect to these axes lead to different lexicalization patterns. For instance, distinct senses in a polysemous word in English, such as skin (referring to both the body and fruit), may be assigned separate words in other languages such as Italian pelle and buccia, respectively BIBREF70. We later analyze whether similarity scores obtained from native speakers also loosely follow the patterns described by semantic typology.","In other words, we treat each pretrained encoder enc as a black-box function to encode a single word or a multi-word expression $x$ in each language into a $d$-dimensional contextualized representation $\mathbf {x}_{\textsc {enc}} \in \mathbb {R}^d = \textsc {enc}(x)$ (e.g., $d=768$ with bert). As multilingual pretrained encoders, we experiment with the multilingual bert model (m-bert) BIBREF29 and xlm BIBREF30. m-bert is pretrained on monolingual Wikipedia corpora of 102 languages (comprising all Multi-SimLex languages) with a 12-layer Transformer network, and yields 768-dimensional representations. Since the concept pairs in Multi-SimLex are lowercased, we use the uncased version of m-bert. m-bert comprises all Multi-SimLex languages, and its evident ability to perform cross-lingual transfer BIBREF12, BIBREF125, BIBREF126 also makes it a convenient baseline model for cross-lingual experiments later in §SECREF8. The second multilingual model we consider, xlm-100, is pretrained on Wikipedia dumps of 100 languages, and encodes each concept into a $1,280$-dimensional representation. In contrast to m-bert, xlm-100 drops the next-sentence prediction objective and adds a cross-lingual masked language modeling objective. For both encoders, the representations of each concept are computed as averages over the last $H=4$ hidden layers in all experiments, as suggested by Wu:2019arxiv.","The results we report are Spearman's $\rho $ coefficients of the correlation between the ranks derived from the scores of the evaluated models and the human scores provided in each Multi-SimLex dataset. The main results with static and contextualized word vectors for all test languages are summarized in Table TABREF43. The scores reveal several interesting patterns, and also pinpoint the main challenges for future work.","Differences across Languages. Naturally, the results from Tables TABREF43 and TABREF46 also reveal that there is variation in performance of both static word embeddings and pretrained encoders across different languages. Among other causes, the lowest absolute scores with ft are reported for languages with least resources available to train monolingual word embeddings, such as Kiswahili, Welsh, and Estonian. The low performance on Welsh is especially indicative: Figure FIGREF20 shows that the ratings in the Welsh dataset match up very well with the English ratings, but we cannot achieve the same level of correlation in Welsh with Welsh ft word embeddings. Difference in performance between two closely related languages, est (low-resource) and fin (high-resource), provides additional evidence in this respect.","Another important follow-up analysis might involve the comparison of the performance of representation learning models on multilingual datasets for both word-level semantic similarity and sentence-level Natural Language Understanding. In particular, Multi-SimLex fills a gap in available resources for multilingual NLP and might help understand how lexical and compositional semantics interact if put alongside existing resources such as XNLI BIBREF84 for natural language inference or PAWS-X BIBREF89 for cross-lingual paraphrase identification. Finally, the Multi-SimLex annotation could turn out to be a unique source of evidence to study the effects of polysemy in human judgments on semantic similarity: for equivalent word pairs in multiple languages, are the similarity scores affected by how many senses the two words (or multi-word expressions) incorporate?",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How is unstability defined?,Sample Answer,1804.09692-Methodology-2,1804.09692-Algorithm Properties-0,1804.09692-Lessons Learned: What Contributes to the Stability of an Embedding-8,1804.09692-2-Table1-1.png,1804.09692-4-Table2-1.png,"The goodness of fit of a regression model is measured using the coefficient of determination INLINEFORM0 . This measures how much variance in the dependent variable INLINEFORM1 is captured by the independent variables INLINEFORM2 . A model that always predicts the expected value of INLINEFORM3 , regardless of the input features, will receive an INLINEFORM4 score of 0. The highest possible INLINEFORM5 score is 1, and the INLINEFORM6 score can be negative.","In addition to word and data properties, we encode features about the embedding algorithms. These include the different algorithms being used, as well as the different parameter settings of these algorithms. Here, we consider three embedding algorithms, word2vec, GloVe, and PPMI. The choice of algorithm is represented in our feature vector as a bag-of-words.","Figure FIGREF19 also shows that “All NYT"" generalizes across the other NYT domains better than Europarl, but not as well as in-domain data (“All NYT"" encompasses data from US, NY, Business, Arts, and Sports). This is true even though Europarl is much larger than “All NYT"".",Table 1: Top ten most similar words for the word international in three randomly intialized word2vec models trained on the NYT Arts Domain. Words in all three lists are in bold; words in only two of the lists are italicized.,"Table 2: Consider the word international in two embedding spaces. Suppose embedding spaceA is trained using word2vec (embedding dimension 100) on the NYT Arts domain, and embedding space B is trained using PPMI (embedding dimension 100) on Europarl. This table summarizes the resulting features for this word across the two embedding spaces.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Does the paper report F1-scores with and without post-processing for the second task?,Sample Answer,1908.06493-Introduction-0,1908.06493-Related Work-0,1908.06493-Related Work-3,1908.06493-Data and Methodology ::: System Definition ::: Hierarchical Classifier ::: Scikit Learn Hierarchical-0,1908.06493-Experiments ::: Preliminary Experiments on Development Set-4,"Hierarchical Multi-label Classification (HMC) is an important task in Natural Language Processing (NLP). Several NLP problems can be formulated in this way, such as patent, news articles, books and movie genres classification (as well as many other classification tasks like diseases, gene function prediction). Also, many tasks can be formulated as hierarchical problem in order to cope with a large amount of labels to assign to the sample, in a divide and conquer manner (with pseudo meta-labels). A theoretical survey exists BIBREF0 discussing on how the task can be engaged, several approaches and the prediction quality measures. Basically, the task in HMC is to assign a sample to one or many nodes of a Directed Acyclic Graph (DAG) (in special cases a tree) based on features extracted from the sample. In the case of possible multiple parent, the evaluation of the prediction complicates heavily, for once since several paths can be taken, but only in a joining node must be considered.","The dataset released by BIBREF4 enabled a major boost in HMC on text. This was a seminating dataset since not only was very large (800k documents) but the hierarchies were large (103 and 364). Many different versions were used in thousands of papers. Further, the label density BIBREF5 was considerably high allowing also to be treated as multi-label, but not too high as to be disregarded as a common real-world task. Some other datasets were also proposed (BIBREF6, BIBREF7), which were far more difficult to classify. This means consequently that a larger mature and varied collection of methods were developed, from which we cannot cover much in this paper.","Still, a less considered problem in HMC is the number of predicted labels, especially regarding the post-processing of the predictions. We discussed this thoroughly in BIBREF1. The main two promising approaches were proposed by BIBREF9 and BIBREF10. The former focuses on column and row based methods for estimating the appropriate threshold to convert a prediction confidence into a label prediction. BIBREF10 used the label cardinality (BIBREF5), which is the mean average label per sample, of the training set and change the threshold globally so that the test set achieved similar label cardinality.","Scikit Learn Hierarchical (Hsklearn) was forked and improved to deal with multi-labels for the task, especially, allowing each node to perform its own preprocessing. This guaranteed that the performance of our own implementation was surpassed and that a contribution for the community was made. This ensured as well that the results are easily reproducible.","Although in Fig. FIGREF26, the curve fitted is parabolic, in the interval between -0.2 and 0, the score is almost linear (and strongly monotone decreasing) giving a good indication that at least -0.2 should be a good threshold to produce a higher F-1 score without any loss.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What does post-processing do to the output?,Sample Answer,1908.06493-Related Work-3,1908.06493-Data and Methodology ::: Task Definition and Data Description-2,1908.06493-Data and Methodology ::: System Definition ::: Post-processing: Threshold-1,1908.06493-Experiments ::: Preliminary Experiments on Development Set-4,1908.06493-Experiments ::: Subtask B-1,"Still, a less considered problem in HMC is the number of predicted labels, especially regarding the post-processing of the predictions. We discussed this thoroughly in BIBREF1. The main two promising approaches were proposed by BIBREF9 and BIBREF10. The former focuses on column and row based methods for estimating the appropriate threshold to convert a prediction confidence into a label prediction. BIBREF10 used the label cardinality (BIBREF5), which is the mean average label per sample, of the training set and change the threshold globally so that the test set achieved similar label cardinality.","The label cardinality of the training dataset was about 1.070 (train: 1.069, dev: 1.072) in the root nodes, pointing to a clearly low multi-label problem, although there were samples with up to 4 root nodes assigned. This means that the traditional machine learning systems would promote single label predictions. Subtask B has a label cardinality of 3.107 (train: 3.106, dev: 3.114), with 1 up to 14 labels assigned per sample. Table TABREF4 shows a short dataset summary by task.","As described in BIBREF1, Read and Pfahringer BIBREF10 introduce a method (referred hereinafter to as LCA) to estimate the threshold globally. Their method chooses the threshold that minimizes the difference between the label cardinality of the training set and the predicted set.","Although in Fig. FIGREF26, the curve fitted is parabolic, in the interval between -0.2 and 0, the score is almost linear (and strongly monotone decreasing) giving a good indication that at least -0.2 should be a good threshold to produce a higher F-1 score without any loss.","The threshold set to -0.25 shown also to produce better results with micro F-1, in contrast to the simple average between recall and precision. This can be seen also by checking the average value between recall and precision, by checking the sum, our approach produced 0.7072+0.6487 = 1.3559 whereas the second team had 0.7377+0.6174 = 1.3551, so the harmonic mean gave us a more comfortable winning marge.",1.0,1.0,1.0,1.0,1.0,0.22222222222222224,0.25,0.2
What size filters do they use in the convolution layer?,Sample Answer,1808.04122-Introduction-1,1808.04122-Experimental setup-12,1808.04122-Main experimental results-1,1808.04122-Main results-1,1808.04122-Related work-1,"Triple modeling is applied not only to the KG completion, but also for other tasks which can be formulated as a triple-based prediction problem. An example is in search personalization, one would aim to tailor search results to each specific user based on the user's personal interests and preferences BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 . Here the triples can be formulated as (submitted query, user profile, returned document) and used to re-rank documents returned to a user given an input query, by employing an existing KG embedding method such as TransE BIBREF3 , as proposed by BIBREF12 . Previous studies have shown the effectiveness of modeling triple for either KG completion or search personalization. However, there has been no single study investigating the performance on both tasks.","In addition, as mentioned by BIBREF9 , the more recently clicked document expresses more about the user current search interest. Hence, we make use of the user clicked documents in the training set with the temporal weighting scheme proposed by BIBREF11 to initialize user profile embeddings for the three embedding models.","Following BIBREF3 , for each relation INLINEFORM0 in FB15k-237, we calculate the averaged number INLINEFORM1 of head entities per tail entity and the averaged number INLINEFORM2 of tail entities per head entity. If INLINEFORM3 1.5 and INLINEFORM4 1.5, INLINEFORM5 is categorized one-to-one (1-1). If INLINEFORM6 1.5 and INLINEFORM7 1.5, INLINEFORM8 is categorized one-to-many (1-M). If INLINEFORM9 1.5 and INLINEFORM10 1.5, INLINEFORM11 is categorized many-to-one (M-1). If INLINEFORM12 1.5 and INLINEFORM13 1.5, INLINEFORM14 is categorized many-to-many (M-M). As a result, 17, 26, 81 and 113 relations are labelled 1-1, 1-M, M-1 and M-M, respectively. And 0.9%, 6.3%, 20.5% and 72.3% of the test triples in FB15k-237 contain 1-1, 1-M, M-1 and M-M relations, respectively.","To illustrate our training progress, we plot performances of CapsE on the validation set over epochs in Figure FIGREF18 . We observe that the performance is improved with the increase in the number of filters since capsules can encode more useful properties for a large embedding size.","For search tasks, unlike classical methods, personalized search systems utilize the historical interactions between the user and the search system, such as submitted queries and clicked documents to tailor returned results to the need of that user BIBREF7 , BIBREF8 . That historical information can be used to build the user profile, which is crucial to an effective search personalization system. Widely used approaches consist of two separated steps: (1) building the user profile from the interactions between the user and the search system; and then (2) learning a ranking function to re-rank the search results using the user profile BIBREF9 , BIBREF33 , BIBREF10 , BIBREF11 . The general goal is to re-rank the documents returned by the search system in such a way that the more relevant documents are ranked higher. In this case, apart from the user profile, dozens of other features have been proposed as the input of a learning-to-rank algorithm BIBREF9 , BIBREF33 . Alternatively, BIBREF12 modeled the potential user-oriented relationship between the submitted query and the returned document by applying TransE to reward higher scores for more relevant documents (e.g., clicked documents). They achieved better performances than the standard ranker as well as competitive search personalization baselines BIBREF27 , BIBREF9 , BIBREF11 .",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
By how much do they outperform state-of-the-art models on knowledge graph completion?,Sample Answer,1808.04122-Introduction-8,1808.04122-The proposed CapsE-1,1808.04122-The proposed CapsE-3,1808.04122-Main experimental results-1,1808.04122-Search personalization application-0, INLINEFORM0 We restate the prospective strategy of expanding the triple embedding models to improve the ranking quality of the search personalization systems. We adapt our model to search personalization and evaluate on SEARCH17 BIBREF12 – a dataset of the web search query logs. Experimental results show that our CapsE achieves the new state-of-the-art results with significant improvements over strong baselines.,"We denote INLINEFORM0 , INLINEFORM1 and INLINEFORM2 as the INLINEFORM3 -dimensional embeddings of INLINEFORM4 , INLINEFORM5 and INLINEFORM6 , respectively. In our proposed CapsE, we follow BIBREF15 to view each embedding triple [ INLINEFORM7 , INLINEFORM8 , INLINEFORM9 ] as a matrix INLINEFORM10 , and denote INLINEFORM11 as the INLINEFORM12 -th row of INLINEFORM13 . We use a filter INLINEFORM14 operated on the convolution layer. This filter INLINEFORM15 is repeatedly operated over every row of INLINEFORM16 to generate a feature map INLINEFORM17 , in which INLINEFORM18 where INLINEFORM19 denotes a dot product, INLINEFORM20 is a bias term and INLINEFORM21 is a non-linear activation function such as ReLU. Our model uses multiple filters INLINEFORM22 to generate feature maps. We denote INLINEFORM23 as the set of filters and INLINEFORM24 as the number of filters, thus we have INLINEFORM25 INLINEFORM26 -dimensional feature maps, for which each feature map can capture one single characteristic among entries at the same dimension.","The first capsule layer consists of INLINEFORM0 capsules, for which each capsule INLINEFORM1 has a vector output INLINEFORM2 . Vector outputs INLINEFORM3 are multiplied by weight matrices INLINEFORM4 to produce vectors INLINEFORM5 which are summed to produce a vector input INLINEFORM6 to the capsule in the second layer. The capsule then performs the non-linear squashing function to produce a vector output INLINEFORM7 : DISPLAYFORM0 ","Following BIBREF3 , for each relation INLINEFORM0 in FB15k-237, we calculate the averaged number INLINEFORM1 of head entities per tail entity and the averaged number INLINEFORM2 of tail entities per head entity. If INLINEFORM3 1.5 and INLINEFORM4 1.5, INLINEFORM5 is categorized one-to-one (1-1). If INLINEFORM6 1.5 and INLINEFORM7 1.5, INLINEFORM8 is categorized one-to-many (1-M). If INLINEFORM9 1.5 and INLINEFORM10 1.5, INLINEFORM11 is categorized many-to-one (M-1). If INLINEFORM12 1.5 and INLINEFORM13 1.5, INLINEFORM14 is categorized many-to-many (M-M). As a result, 17, 26, 81 and 113 relations are labelled 1-1, 1-M, M-1 and M-M, respectively. And 0.9%, 6.3%, 20.5% and 72.3% of the test triples in FB15k-237 contain 1-1, 1-M, M-1 and M-M relations, respectively.","Given a user, a submitted query and the documents returned by a search system for that query, our approach is to re-rank the returned documents so that the more relevant documents should be ranked higher. Following BIBREF12 , we represent the relationship between the submitted query, the user and the returned document as a (s, r, o)-like triple (query, user, document). The triple captures how much interest a user puts on a document given a query. Thus, we can evaluate the effectiveness of our CapsE for the search personalization task.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How better does new approach behave than existing solutions?,Sample Answer,2001.08868-Methodology ::: Phase 2: Generalization-2,2001.08868-Experiments ::: Games and experiments setup-2,2001.08868-Appendix ::: Text-Games ::: CoinCollector-0,2001.08868-Appendix ::: Text-Games ::: CookingWorld-8,2001.08868-16-Figure10-1.png,"where $W \in \mathbb {R}^{2d \times |V|}$ is a matrix that maps the decoder hidden state, concatenated with the context vector, into a vocabulary size vector. During training, the parameters of the model are trained by minimizing:","CoinCollector BIBREF8 is a class of text-based games where the objective is to find and collect a coin from a specific location in a given set of connected rooms . The agent wins the game after it collects the coin, at which point (for the first and only time) a reward of +1 is received by the agent. The environment parses only five admissible commands (go north, go east, go south, go west, and take coin) made by two worlds;","In the hard setting (mode 2), each room on the path to the coin has two distractor rooms, and the level (e.g. 30) indicates the shortest path from the starting point to the coin room.","Thus the hardest game would be a recipe with 3 ingredients, which must all be picked up somewhere across 12 locations and then need to be cut and cooked, and to get access to some locations, several doors or objects need to be opened. The handicap of a limited capacity in the inventory makes the game more difficult by requiring the agent to drop an object and later on take it again if needed. The grammar used for the text-based games is the following:",Figure 10: Joint all the games,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How are the three different forms defined in this work?,Sample Answer,1911.10401-Introduction-0,1911.10401-Introduction-1,1911.10401-Literature Review-0,1911.10401-Literature Review ::: Irony and Sarcasm Detection-1,1911.10401-Methodology: A hybrid Recurrent Convolution Transformer Approach ::: The background: Transfer Learning-0,"In the networked-world era the production of (structured or unstructured) data is increasing with most of our knowledge being created and communicated via web-based social channels BIBREF1. Such data explosion raises the need for efficient and reliable solutions for the management, analysis and interpretation of huge data sizes. Analyzing and extracting knowledge from massive data collections is not only a big issue per-se, but also challenges the data analytics state-of-the-art BIBREF2, with statistical and machine learning methodologies paving the way, and deep learning (DL) taking over and presenting highly accurate solutions BIBREF3. Relevant applications in the field of social media cover a wide spectrum, from the categorization of major disasters BIBREF4 and the identification of suggestions BIBREF5 to inducing users’ appeal to political parties BIBREF6.","The raising of computational social science BIBREF7, and mainly its social media dimension BIBREF8, challenge contemporary computational linguistics and text-analytics endeavors. The challenge concerns the advancement of text analytics methodologies towards the transformation of unstructured excerpts into some kind of structured data via the identification of special passage characteristics, such as its emotional content (e.g., anger, joy, sadness) BIBREF9. In this context, Sentiment Analysis (SA) comes into play, targeting the devise and development of efficient algorithmic processes for the automatic extraction of a writer’s sentiment or emotion as conveyed in text excerpts. Relevant efforts focus on tracking the sentiment polarity of single utterances, which in most cases is loaded with a lot of subjectivity and a degree of vagueness BIBREF10. Contemporary research in the field utilizes data from social media resources (e.g., Facebook, Twitter) as well as other short text references in blogs, forums etc BIBREF11. However, users of social media tend to violate common grammar and vocabulary rules and even use various figurative language forms to communicate their message. In such situations, the sentiment inclination underlying the literal content of the conveyed concept may significantly differ from its figurative context, making SA tasks even more puzzling. Evidently, single turn text lack in detecting sentiment polarity on sarcastic and ironic expressions, as already signified in the relevant “SemEval-2014 Sentiment Analysis task 9” BIBREF12. Moreover, lacking of facial expressions and voice tone require context aware approaches to tackle such a challenging task and overcome its ambiguities BIBREF13. As sentiment is the emotion behind customer engagement, SA finds its realization in automated customer aware services, elaborating over user’s emotional intensities BIBREF14. Most of the related studies utilize single turn texts from topic specific sources, such as Twitter, Amazon, IMDB etc. Hand crafted and sentiment-oriented features, indicative of emotion polarity, are utilized to represent respective excerpt cases. The formed data are then fed traditional machine learning classifiers (e.g. SVM, Random Forest, multilayer perceptrons) or DL techniques and respective complex neural architectures, in order to induce analytical models that are able to capture the underlying sentiment content and polarity of passages BIBREF15, BIBREF16, BIBREF17.","Although the NLP community have researched all aspects of FL independently, none of the proposed systems were evaluated on more than one type. Related work on FL detection and classification tasks could be categorized into two main categories, according to the studied task: (a) irony and sarcasm detection, and (b) sentiment analysis of FL excerpts. Even if sarcasm and irony are not identical phenomenons, we will present those types together, as they appear in the literature.","Approaches based on unexpectedness and contradictory factors. Reyes et al. BIBREF42, BIBREF43 were the first that attempted to capture irony and sarcasm in social media. They introduced the concepts of unexpectedness and contradiction that seems to be frequent in FL expressions. The unexpectedness factor was also adopted as a key concept in other studies as well. In particular, Barbieri et al. BIBREF44 compared tweets with sarcastic content with other topics such as, #politics, #education, #humor. The measure of unexpectedness was calculated using the American National Corpus Frequency Data source as well as the morphology of tweets, using Random Forests (RF) and Decision Trees (DT) classifiers. In the same direction, Buschmeir et al. BIBREF45 considered unexpectedness as an emotional imbalance between words in the text. Ghosh et al. BIBREF46 identified sarcasm using Support Vector Machines (SVM) using as features the identified contradictions within each tweet.","Due to the limitations of annotated datasets and the high cost of data collection, unsupervised learning approaches tend to be an easier way towards training networks. Recently, transfer learning approaches, i.e., the transfer of already acquired knowledge to new conditions, are gaining attention in several domain adaptation problems BIBREF72. In fact, pre-trained embeddings representations, such as GloVe, ElMo and USE, coupled with transfer learning architectures were introduced and managed to achieve state-of-the-art results on various NLP tasks BIBREF73. In this chapter we review on these methodologies in order to introduce our approach. In this chapter we will summarize those methods and introduce our proposed transfer learning system. Model specifications used for the state-of-the-art models compared can be found in Appendix SECREF6.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
"In the proposed metric, how is content relevance measured?",Sample Answer,1604.00400-Summarization evaluation by Rouge-2,1604.00400-Summarization Evaluation by Relevance Analysis (Sera)-12,1604.00400-Annotations-3,1604.00400-Sera-0,1604.00400-4-Table1-1.png,"In the summarization literature, despite the large number of variants of Rouge, only one or very few of these variants are often chosen (arbitrarily) for evaluation of the quality of the summarization approaches. When Rouge was proposed, the original variants were only recall-oriented and hence the reported correlation results BIBREF1 . The later extension of Rouge family by precision were only reflected in the later versions of the Rouge toolkit and additional evaluation of its effectiveness was not reported. Nevertheless, later published work in summarization adopted this toolkit for its ready implementation and relatively efficient performance.","We also experiment with different query (re)formulation approaches. Query reformulation is a method in Information Retrieval that aims to refine the query for better retrieval of results. Query reformulation methods often consist of removing ineffective terms and expressions from the query (query reduction) or adding terms to the query that help the retrieval (query expansion). Query reduction is specially important when queries are verbose. Since we use the summaries as queries, the queries are usually long and therefore we consider query reductions.","In the above example, the underlined expressions are the content units that convey the main meaning of the text. We call these small units, nuggets which are phrases or concepts that are the main contributors to the content quality of the summary.","The results of our proposed method (Sera) are shown in the bottom part of Table TABREF23 . In general, Sera shows better correlation with pyramid scores in comparison with Rouge. We observe that the Pearson correlation of Sera with cut-off point of 5 (shown by Sera-5) is 0.823 which is higher than most of the Rouge variants. Similarly, the Spearman and Kendall correlations of the Sera evaluation score is 0.941 and 0.857 respectively, which are higher than all Rouge correlation values. This shows the effectiveness of the simple variant of our proposed summarization evaluation metric.",Table 1: Example of nugget annotation for Pyramid scores. The pyramid tier represents the number of occurrences of the nugget in all the human written gold summaries.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What different correlations result when using different variants of ROUGE scores?,Sample Answer,1604.00400-Summarization Evaluation by Relevance Analysis (Sera)-12,1604.00400-Annotations-3,1604.00400-Summarization approaches-1,1604.00400-Summarization approaches-7,1604.00400-Sera-1,"We also experiment with different query (re)formulation approaches. Query reformulation is a method in Information Retrieval that aims to refine the query for better retrieval of results. Query reformulation methods often consist of removing ineffective terms and expressions from the query (query reduction) or adding terms to the query that help the retrieval (query expansion). Query reduction is specially important when queries are verbose. Since we use the summaries as queries, the queries are usually long and therefore we consider query reductions.","In the above example, the underlined expressions are the content units that convey the main meaning of the text. We call these small units, nuggets which are phrases or concepts that are the main contributors to the content quality of the summary.","LexRank BIBREF9 : LexRank finds the most important (central) sentences in a document by using random walks in a graph constructed from the document sentences. In this graph, the sentences are nodes and the similarity between the sentences determines the edges. Sentences are ranked according to their importance. Importance is measured in terms of centrality of the sentence — the total number of edges incident on the node (sentence) in the graph. The intuition behind LexRank is that a document can be summarized using the most central sentences in the document that capture its main aspects.","Summarization using citation-context and discourse structure BIBREF16 : In this method, the set of citations to the article are used to find the article sentences that directly reflect those citations (citation-contexts). In addition, the scientific discourse of the article is utilized to capture different aspects of the article. The scientific discourse usually follows a structure in which the authors first describe their hypothesis, then the methods, experiment, results and implications. Sentence selection is based on finding the most important sentences in each of the discourse facets of the document using the MMR heuristic.","Table TABREF23 also shows the results of other Sera variants including discounting and query reformulation methods. Some of these variants are the result of applying query reformulation in the process of document retrieval which are described in section SECREF3 As illustrated, the Noun Phrases (NP) query reformulation at cut-off point of 5 (shown as Sera-np-5) achieves the highest correlations among all the Sera variants ( INLINEFORM0 = INLINEFORM1 , INLINEFORM2 = INLINEFORM3 = INLINEFORM4 ). In the case of Keywords (KW) query reformulation, without using discounting, we can see that there is no positive gain in correlation. However, keywords when applied on the discounted variant of Sera, result in higher correlations.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What tasks are used for evaluation?,Sample Answer,1909.00015-Adaptively Sparse Transformers with @!START@$\alpha $@!END@-entmax ::: Different @!START@$\alpha $@!END@ per head.-0,1909.00015-Experiments-1,1909.00015-Analysis ::: Identifying Head Specializations ::: Positional heads.-1,1909.00015-Conclusion and Future Work-1,1909.00015-7-Figure6-1.png,"Unlike LSTM-based seq2seq models, where $\alpha $ can be more easily tuned by grid search, in a Transformer, there are many attention heads in multiple layers. Crucial to the power of such models, the different heads capture different linguistic phenomena, some of them isolating important words, others spreading out attention across phrases BIBREF0. This motivates using different, adaptive $\alpha $ values for each attention head, such that some heads may learn to be sparser, and others may become closer to softmax. We propose doing so by treating the $\alpha $ values as neural network parameters, optimized via stochastic gradients along with the other weights.","1.5-entmax: a Transformer with sparse entmax attention with fixed $\alpha =1.5$ for all heads. This is a novel model, since 1.5-entmax had only been proposed for RNN-based NMT models BIBREF14, but never in Transformers, where attention modules are not just one single component of the seq2seq model but rather an integral part of all of the model components.","For position $+1$, the models each dedicate one head, with confidence around $95\%$, slightly higher for entmax. The adaptive model sets $\alpha =1.96$ for this head.","In particular, we analyzed how the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence. Our adaptivity strategy relies only on gradient-based optimization, side-stepping costly per-head hyper-parameter searches. Further speed-ups are possible by leveraging more parallelism in the bisection algorithm for computing $\alpha $-entmax.","Figure 6: Jensen-Shannon Divergence between heads at each layer. Measures the disagreement between heads: the higher the value, the more the heads are disagreeing with each other in terms of where to attend. Models using sparse entmax have more diverse attention than the softmax baseline.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
HOw does the method perform compared with baselines?,Sample Answer,1909.00015-Analysis ::: High-Level Statistics ::: What kind of @!START@$\alpha $@!END@ values are learned?-0,1909.00015-Analysis ::: High-Level Statistics ::: Attention weight density when translating.-3,1909.00015-Jacobian of @!START@$\alpha $@!END@-entmax w.r.t. the shape parameter @!START@$\alpha $@!END@: Proof of Proposition @!START@UID22@!END@ ::: Solving the indetermination for @!START@$\alpha =1$@!END@-2,1909.00015-8-Figure9-1.png,1909.00015-9-Figure10-1.png,"Figure FIGREF37 shows the learning trajectories of the $\alpha $ parameters of a selected subset of heads. We generally observe a tendency for the randomly-initialized $\alpha $ parameters to decrease initially, suggesting that softmax-like behavior may be preferable while the model is still very uncertain. After around one thousand steps, some heads change direction and become sparser, perhaps as they become more confident and specialized. This shows that the initialization of $\alpha $ does not predetermine its sparsity level or the role the head will have throughout. In particular, head 8 in the encoder self-attention layer 2 first drops to around $\alpha =1.3$ before becoming one of the sparsest heads, with $\alpha \approx 2$.","Teasing this down into separate layers, Figure FIGREF41 shows the average (sorted) density of each head for each layer. We observe that $\alpha $-entmax is able to learn different sparsity patterns at each layer, leading to more variance in individual head behavior, to clearly-identified dense and sparse heads, and overall to different tendencies compared to the fixed case of $\alpha =1.5$.","To solve this indetermination, we will need to apply L'Hôpital's rule twice. Let us first compute the derivative of $\tilde{p}_i(\alpha )$ with respect to $\alpha $. We have","Figure 9: Interrogation-detecting heads in the three models. The top sentence is interrogative while the bottom one is declarative but includes the interrogative word “what”. In the top example, these interrogation heads assign a high probability to the question mark in the time step of the interrogative word (with ≥ 97.0% probability), while in the bottom example since there is no question mark, the same head does not assign a high probability to the last token in the sentence during the interrogative word time step. Surprisingly, this head prefers a low α = 1.05, as can be seen from the dense weights. This allows the head to identify the noun phrase “Armani Polo” better.","Figure 10: Example of two sentences of similar length where the same head (α = 1.33) exhibits different sparsity. The longer phrase in the example on the right “a sexually transmitted disease” is handled with higher confidence, leading to more sparsity.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Can the new position representation be generalized to other tasks?,Sample Answer,1803.02155-Transformer-0,1803.02155-Efficient Implementation-2,1803.02155-Experimental Setup-1,1803.02155-4-Table1-1.png,1803.02155-4-Table2-1.png,"The Transformer BIBREF3 employs an encoder-decoder structure, consisting of stacked encoder and decoder layers. Encoder layers consist of two sublayers: self-attention followed by a position-wise feed-forward layer. Decoder layers consist of three sublayers: self-attention followed by encoder-decoder attention, followed by a position-wise feed-forward layer. It uses residual connections around each of the sublayers, followed by layer normalization BIBREF10 . The decoder uses masking in its self-attention to prevent a given output position from incorporating information about future output positions during training.","The Transformer computes self-attention efficiently for all sequences, heads, and positions in a batch using parallel matrix multiplication operations BIBREF3 . Without relative position representations, each $e_{ij}$ can be computed using $bh$ parallel multiplications of $n \times d_z$ and $d_z \times n$ matrices. Each matrix multiplication computes $e_{ij}$ for all sequence positions, for a particular head and sequence. For any sequence and head, this requires sharing the same representation for each position across all compatibility function applications (dot products) with other positions.","We evaluated our model on the WMT 2014 machine translation task, using the WMT 2014 English-German dataset consisting of approximately 4.5M sentence pairs and the 2014 WMT English-French dataset consisting of approximately 36M sentence pairs.","Table 1: Experimental results for WMT 2014 English-to-German (EN-DE) and English-to-French (EN-FR) translation tasks, using newstest2014 test set.","Table 2: Experimental results for varying the clipping distance, k.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what is the previous work they are comparing to?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What baselines do they compare to?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What training set sizes do they use?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What languages do they experiment with?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Which dataset do they evaluate on?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Which existing models does this approach outperform?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What human evaluation method is proposed?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is the average length of the recordings?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What were their results?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is the architecture of the decoder?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is the architecture of the encoder?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How better are results of new model compared to competitive methods?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is the size of built dataset?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How big is slot filing dataset?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How can an existing bot detection system by customized for health-related research?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How do they define upward and downward reasoning?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
what phenomena do they mention is hard to capture?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
by how much did the BLEU score improve?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What dicrimating features are discovered?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What results are obtained on the alternate datasets?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
what features of the essays are extracted?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
what were the evaluation metrics?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
what future work is described?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What baseline did they compare Entity-GCN to?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How did they get relations between mentions?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How did they detect entity mentions?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What performance does the Entity-GCN get on WIKIHOP?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How much improvement does their method get over the fine tuning baseline?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How much training data from the non-English language is used by the system?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How is the model transferred to other languages?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What additional features are proposed for future work?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are their initial results on this task?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How are the synthetic examples generated?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What linguistic model does the conventional method use?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
"How are the positive instances annotated? e.g. by annotators, or by laughter from the audience?",Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are the languages they use in their experiment?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is the computational complexity of old method,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
By how much does their model outperform both the state-of-the-art systems?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is the state-of-the art?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How much is pre-training loss increased in Low/Medium/Hard level of pruning?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
"How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques?",Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How much improvement is gained from Adversarial Reward Augmented Maximum Likelihood (ARAML)?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are the restrictions of the restricted track?,Sample Answer,1907.00168-Introduction-0,1907.00168-FST-based Grammatical Error Correction-1,1907.00168-Experimental Setup-4,1907.00168-Experimental Setup-5,1907.00168-4-Table5-1.png,"The automatic correction of errors in text [In a such situaction INLINEFORM0 In such a situation] is receiving more and more attention from the natural language processing community. A series of competitions has been devoted to grammatical error correction (GEC): the CoNLL-2013 shared task BIBREF0 , the CoNLL-2014 shared task BIBREF1 , and finally the BEA 2019 shared task BIBREF2 . This paper presents the contributions from the Cambridge University Engineering Department to the latest GEC competition at the BEA 2019 workshop.","In a first step, the source sentence is converted to an FST INLINEFORM0 (Fig. FIGREF3 ). This initial FST is augmented by composition (denoted with the INLINEFORM1 -operator) with various other FSTs to cover different error types. Composition is a widely used standard operation on FSTs and supported efficiently by FST toolkits such as OpenFST BIBREF8 . We construct the hypothesis space as follows:","Back-translation BIBREF5 has become the most widely used technique to use monolingual data in neural machine translation. Back-translation extends the existing parallel training set by additional training samples with real English target sentences but synthetic source sentences. Different methods have been proposed to synthesize the source sentence such as using dummy tokens BIBREF5 , copying the target sentence BIBREF29 , or sampling from or decoding with a reverse sequence-to-sequence model BIBREF5 , BIBREF30 , BIBREF4 . The most popular approach is to generate the synthetic source sentences with a reverse model that is trained to transform target to source sentences using beam search. In GEC, this means that the reverse model learns to introduce errors into a correct English sentence. Back-translation has been applied successfully to GEC by BIBREF4 . We confirm the effectiveness of back-translation in GEC and discuss some of the differences between applying this technique to grammatical error correction and machine translation.","Our experiments with back-translation are summarized in Tab. TABREF28 . Adding 1M synthetic sentences to the training data already yields very substantial gains on both test sets. We achieve our best results with 5M synthetic sentences (+8.44 on BEA-2019 Dev). In machine translation, it is important to maintain a balance between authentic and synthetic data BIBREF5 , BIBREF31 , BIBREF32 . Over-sampling the real data is a common practice to rectify that ratio if large amounts of synthetic data are available. Interestingly, over-sampling real data in GEC hurts performance (row 3 vs. 5 in Tab. TABREF28 ), and it is possible to mix real and synthetic sentences at a ratio of 1:7.9 (last three rows in Tab. TABREF28 ). We will proceed with the 5M setup for the remainder of this paper.",Table 5: BEA-2019 parallel training data with and without removing pairs where source and target sentences are the same.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How much did the model outperform,Sample Answer,2004.02143-Introduction-0,2004.02143-Introduction-2,2004.02143-Introduction-5,2004.02143-Proposed Approach ::: Multi-Hop Question Generation Model ::: Question Decoder-4,2004.02143-Results and Analysis ::: Qualitative Analysis-1,"In natural language processing (NLP), question generation is considered to be an important yet challenging problem. Given a passage and answer as inputs to the model, the task is to generate a semantically coherent question for the given answer.","In multi-hop question answering, one has to reason over multiple relevant sentences from different paragraphs to answer a given question. We refer to these relevant sentences as supporting facts in the context. Hence, we frame Multi-hop question generation as the task of generating the question conditioned on the information gathered from reasoning over all the supporting facts across multiple paragraphs/documents. Since this task requires assembling and summarizing information from multiple relevant documents in contrast to a single sentence/paragraph, therefore, it is more challenging than the existing single-hop QG task. Further, the presence of irrelevant information makes it difficult to capture the supporting facts required for question generation. The explicit information about the supporting facts in the document is not often readily available, which makes the task more complex. In this work, we provide an alternative to get the supporting facts information from the document with the help of multi-task learning. Table TABREF1 gives sample examples from SQuAD and HotPotQA dataset. It is cleared from the example that the single-hop question is formed by focusing on a single sentence/document and answer, while in multi-hop question, multiple supporting facts from different documents and answer are accumulated to form the question.","Our main contributions in this work are: (i). We introduce the problem of multi-hop question generation and propose a multi-task training framework to condition the shared encoder with supporting facts information. (ii). We formulate a novel reward function, multihop-enhanced reward via question-aware supporting fact predictions to enforce the maximum utilization of supporting facts to generate a question; (iii). We introduce an automatic evaluation metric to measure the coverage of supporting facts in the generated question. (iv). Empirical results show that our proposed method outperforms the current state-of-the-art single-hop QG models over several automatic and human evaluation metrics on the HotPotQA dataset.","where, $\mathbf {W_a}$ and $\mathbf {W_b}$ are the weight matrices and $\sigma $ represents the Sigmoid function. The probability distribution over the words in the document is computed by summing over all the attention scores of the corresponding words:","Human Evaluation: For human evaluation, we directly compare the performance of the proposed approach with NQG model. We randomly sample 100 document-question-answer triplets from the test set and ask four professional English speakers to evaluate them. We consider three modalities: naturalness, which indicates the grammar and fluency; difficulty, which measures the document-question syntactic divergence and the reasoning needed to answer the question, and SF coverage similar to the metric discussed in Section SECREF4 except we replace the supporting facts prediction network with a human evaluator and we measure the relative supporting facts coverage compared to the ground-truth supporting facts. measure the relative coverage of supporting facts in the questions with respect to the ground-truth supporting facts. SF coverage provides a measure of the extent of supporting facts used for question generation. For the first two modalities, evaluators are asked to rate the performance of the question generator on a 1–5 scale (5 for the best). To estimate the SF coverage metric, the evaluators are asked to highlight the supporting facts from the documents based on the generated question.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What language is in the dataset?,Sample Answer,2004.02143-Introduction-0,2004.02143-Introduction-2,2004.02143-Introduction-3,2004.02143-Experimental Setup ::: Dataset:-2,2004.02143-Conclusion-0,"In natural language processing (NLP), question generation is considered to be an important yet challenging problem. Given a passage and answer as inputs to the model, the task is to generate a semantically coherent question for the given answer.","In multi-hop question answering, one has to reason over multiple relevant sentences from different paragraphs to answer a given question. We refer to these relevant sentences as supporting facts in the context. Hence, we frame Multi-hop question generation as the task of generating the question conditioned on the information gathered from reasoning over all the supporting facts across multiple paragraphs/documents. Since this task requires assembling and summarizing information from multiple relevant documents in contrast to a single sentence/paragraph, therefore, it is more challenging than the existing single-hop QG task. Further, the presence of irrelevant information makes it difficult to capture the supporting facts required for question generation. The explicit information about the supporting facts in the document is not often readily available, which makes the task more complex. In this work, we provide an alternative to get the supporting facts information from the document with the help of multi-task learning. Table TABREF1 gives sample examples from SQuAD and HotPotQA dataset. It is cleared from the example that the single-hop question is formed by focusing on a single sentence/document and answer, while in multi-hop question, multiple supporting facts from different documents and answer are accumulated to form the question.","Multi-hop QG has real-world applications in several domains, such as education, chatbots, etc. The questions generated from the multi-hop approach will inspire critical thinking in students by encouraging them to reason over the relationship between multiple sentences to answer correctly. Specifically, solving these questions requires higher-order cognitive-skills (e.g., applying, analyzing). Therefore, forming challenging questions is crucial for evaluating a student’s knowledge and stimulating self-learning. Similarly, in goal-oriented chatbots, multi-hop QG is an important skill for chatbots, e.g., in initiating conversations, asking and providing detailed information to the user by considering multiple sources of information. In contrast, in a single-hop QG, only single source of information is considered while generation.","Metric for MultiHoping in QG: To assess the multi-hop capability of the question generation model, we introduce additional metric SF coverage, which measures in terms of F1 score. This metric is similar to MultiHop-Enhanced Reward, where we use the question-aware supporting facts predictions network that takes the generated question and document list as input and predict the supporting facts. F1 score measures the average overlap between the predicted and ground-truth supporting facts as computed in BIBREF11.","In this paper, we have introduced the multi-hop question generation task, which extends the natural language question generation paradigm to multiple document QA. Thereafter, we present a novel reward formulation to improve the multi-hop question generation using reinforcement and multi-task learning frameworks. Our proposed method performs considerably better than the state-of-the-art question generation systems on HotPotQA dataset. We also introduce SF Coverage, an evaluation metric to compare the performance of question generation systems based on their capacity to accumulate information from various documents. Overall, we propose a new direction for question generation research with several practical applications. In the future, we will be focusing on to improve the performance of multi-hop question generation without any strong supporting facts supervision.",1.0,1.0,1.0,1.0,1.0,0.25,0.3333333333333333,0.2
How does this compare to traditional calibration methods like Platt Scaling?,Sample Answer,1905.13413-Introduction-0,1905.13413-Introduction-1,1905.13413-Problem Formulation-0,1905.13413-Problem Formulation-1,1905.13413-Evaluation Results-4,"Open information extraction (IE, sekine2006demand, Banko:2007:OIE) aims to extract open-domain assertions represented in the form of $n$ -tuples (e.g., was born in; Barack Obama; Hawaii) from natural language sentences (e.g., Barack Obama was born in Hawaii). Open IE started from rule-based BIBREF0 and syntax-driven systems BIBREF1 , BIBREF2 , and recently has used neural networks for supervised learning BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 .","A key step in open IE is confidence modeling, which ranks a list of candidate extractions based on their estimated quality. This is important for downstream tasks, which rely on trade-offs between the precision and recall of extracted assertions. For instance, an open IE-powered medical question answering (QA) system may require its assertions in higher precision (and consequently lower recall) than QA systems for other domains. For supervised open IE systems, the confidence score of an assertion is typically computed based on its extraction likelihood given by the model BIBREF3 , BIBREF5 . However, we observe that this often yields sub-optimal ranking results, with incorrect extractions of one sentence having higher likelihood than correct extractions of another sentence. We hypothesize this is due to the issue of a disconnect between training and test-time objectives. Specifically, the system is trained solely to raise likelihood of gold-standard extractions, and during training the model is not aware of its test-time behavior of ranking a set of system-generated assertions across sentences that potentially include incorrect extractions.","Given sentence $\mathbf {s}=(w_1, w_2, ..., w_n)$ , the goal of open IE is to extract assertions in the form of tuples $\mathbf {r}=(\mathbf {p}, \mathbf {a}_1, \mathbf {a}_2, ..., \mathbf {a}_m)$ , composed of a single predicate and $m$ arguments. Generally, these components in $\mathbf {r}$ need not to be contiguous, but to simplify the problem we assume they are contiguous spans of words from $\mathbf {s}$ and there is no overlap between them.","Methods to solve this problem have recently been formulated as sequence-to-sequence generation BIBREF4 , BIBREF5 , BIBREF6 or sequence labeling BIBREF3 , BIBREF7 . We adopt the second formulation because it is simple and can take advantage of the fact that assertions only consist of words from the sentence. Within this framework, an assertion $\mathbf {r}$ can be mapped to a unique BIO BIBREF3 label sequence $\mathbf {y}$ by assigning $O$ to the words not contained in $\mathbf {r}$ , $B_{p}$ / $I_{p}$ to the words in $\mathbf {p}$ , and $B_{a_i}$ / $I_{a_i}$ to the words in $\mathbf {a}_i$ respectively, depending on whether the word is at the beginning or inside of the span.","We classify the errors into three categories and summarize their proportions in tab:err. “Overgenerated predicate” is where predicates not included in ground truth are overgenerated, because all the verbs are used as candidate predicates. An effective mechanism should be designed to reject useless candidates. “Wrong argument” is where extracted arguments do not coincide with ground truth, which is mainly caused by merging multiple arguments in ground truth into one. “Missing argument” is where the model fails to recognize arguments. These two errors usually happen when the structure of the sentence is complicated and coreference is involved. More linguistic information should be introduced to solve these problems.",1.0,1.0,1.0,1.0,1.0,0.25,0.3333333333333333,0.2
Which machine learning methods are used in experiments?,Sample Answer,1601.02403-Our contributions-1,1601.02403-Argumentation Mining-2,1601.02403-Stance detection-1,1601.02403-Annotation study 2: Annotating micro-structure of arguments-28,1601.02403-Annotation study 2: Annotating micro-structure of arguments-32,"Since the data come from a variety of sources and no assumptions about its actual content with respect to argumentation can be drawn, we conduct two extensive annotation studies. In the first study, we tackle the problem of relatively high “noise” in the retrieved data. In particular, not all of the documents are related to the given topics in a way that makes them candidates for further deep analysis of argumentation (this study results into 990 annotated documents). In the second study, we discuss the selection of an appropriate argumentation model based on evidence in argumentation research and propose a model that is suitable for analyzing micro-level argumention in user-generated Web content. Using this model, we annotate 340 documents (approx. 90,000 tokens), reaching a substantial inter-annotator agreement. We provide a hand-analysis of all the phenomena typical to argumentation that are prevalent in our data. These findings may also serve as empirical evidence to issues that are on the spot of current argumentation research.","Reed.Rowe.2004 presented Araucaria, a tool for argumentation diagramming which supports both convergent and linked arguments, missing premises (enthymemes), and refutations. They also released the AracuariaDB corpus which has later been used for experiments in the argumentation mining field. However, the creation of the dataset in terms of annotation guidelines and reliability is not reported – these limitations as well as its rather small size have been identified BIBREF10 .","Somasundaran.Wiebe.2009 built a computational model for recognizing stances in dual-topic debates about named entities in the electronic products domain by combining preferences learned from the Web data and discourse markers from PDTB BIBREF48 . Hasan.Ng.2013 determined stance in on-line ideological debates on four topics using data from createdebate.com, employing supervised machine learning and features ranging from n-grams to semantic frames. Predicting stance of posts in Debatepedia as well as external articles using a probabilistic graphical model was presented in BIBREF49 . This approach also employed sentiment lexicons and Named Entity Recognition as a preprocessing step and achieved accuracy about 0.80 in binary prediction of stances in debate posts.","Although the measure has been used in related annotation works BIBREF61 , BIBREF7 , BIBREF72 , there is one important detail that has not been properly communicated. The INLINEFORM0 is computed over a continuum of the smallest units, such as tokens. This continuum corresponds to a single document in the original Krippendorff's work. However, there are two possible extensions to multiple documents (a corpus), namely (a) to compute INLINEFORM1 for each document first and then report an average value, or (b) to concatenate all documents into one large continuum and compute INLINEFORM2 over it. The first approach with averaging yielded extremely high the standard deviation of INLINEFORM3 (i.e., avg. = 0.253; std. dev. = 0.886; median = 0.476 for the claim). This says that some documents are easy to annotate while others are harder, but interpretation of such averaged value has no evidence either in BIBREF71 or other papers based upon it. Thus we use the other methodology and treat the whole corpus as a single long continuum (which yields in the example of claim 0.541 INLINEFORM4 ).","First, we examine the disagreement in annotations by posing the following research question: are there any measurable properties of the annotated documents that might systematically cause low inter-annotator agreement? We use Pearson's correlation coefficient between INLINEFORM0 on each document and the particular property under investigation. We investigated the following set of measures.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the invertibility condition?,Sample Answer,1808.09111-Unsupervised POS tagging-2,1808.09111-Conclusion-0,1808.09111-1-Figure1-1.png,1808.09111-6-Table1-1.png,1808.09111-7-Table2-1.png,"We compare our approach with basic HMM, Gaussian HMM, and several state-of-the-art systems, including sophisticated HMM variants and clustering techniques with hand-engineered features. The results are presented in Table TABREF32 . Through the introduced latent embeddings and additional neural projection, our approach improves over the Gaussian HMM by 5.4 points in M-1 and 5.6 points in VM. Neural HMM (NHMM) BIBREF10 is a baseline that also learns word representation jointly. Both their basic model and extended Conv version does not outperform the Gaussian HMM. Their best model incorporates another LSTM to model long distance dependency and breaks the Markov assumption, yet our approach still achieves substantial improvement over it without considering more context information. Moreover, our method outperforms the best published result that benefits from hand-engineered features BIBREF27 by 2.0 points on VM.","In this work, we define a novel generative approach to leverage continuous word representations for unsupervised learning of syntactic structure. Experiments on both POS induction and unsupervised dependency parsing tasks demonstrate the effectiveness of our proposed approach. Future work might explore more sophisticated invertible projections, or recurrent projections that jointly transform the entire input sequence. ",Figure 1: Visualization (t-SNE) of skip-gram embeddings (trained on one billion words with context window size equal to 1) and latent embeddings learned by our approach with a Markov-structured prior. Each node represents a word and is colored according to the most likely gold POS tag from the Penn Treebank (best seen in color).,"Table 1: Unsupervised POS tagging results on entire WSJ, compared with other baselines and state-of-the-art systems. Standard deviation is given in parentheses when available.","Table 2: Directed dependency accuracy on section 23 of WSJ, evaluating on sentences of length 6 10 and all lengths. Starred entries (∗) denote that the system benefits from additional punctuation-based constraints. Standard deviation is given in parentheses when available.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
On what dataset is Aristo system trained?,Sample Answer,1909.01958-Introduction ::: The Turing Test versus Standardized Tests-0,1909.01958-The Aristo System ::: Reasoning Methods-2,1909.01958-The Aristo System ::: Reasoning Methods-9,1909.01958-Experiments and Results ::: Main Results-4,1909.01958-Experiments and Results ::: Adversarial Answer Options-0,"In 1950, Alan Turing proposed the now well-known Turing Test as a possible test of machine intelligence: If a system can exhibit conversational behavior that is indistinguishable from that of a human during a conversation, that system could be considered intelligent (BID1). As the field of AI has grown, the test has become less meaningful as a challenge task for several reasons. First, its setup is not well defined (e.g., who is the person giving the test?). A computer scientist would likely know good distinguishing questions to ask, while a random member of the general public may not. What constraints are there on the interaction? What guidelines are provided to the judges? Second, recent Turing Test competitions have shown that, in certain formulations, the test itself is gameable; that is, people can be fooled by systems that simply retrieve sentences and make no claim of being intelligent (BID2;BID3). John Markoff of The New York Times wrote that the Turing Test is more a test of human gullibility than machine intelligence. Finally, the test, as originally conceived, is pass/fail rather than scored, thus providing no measure of progress toward a goal, something essential for any challenge problem.","On-the-fly tuples ($T^{\prime }$), extracted at question-answering time from t<he same corpus, to handle questions from new domains not covered by the training set.","In particular, the system learns through training to track the polarity of influences: For example, if we were to change “lower” to “higher” in the above example, the system will change its answer choice. Another example is shown in Figure FIGREF19. Again, if “melted” were changed to “cooled”, the system would change its choice to “(B) less energy”.","On the entire exam, the NY State Education Department considers a score of 65% as “Meeting the Standards”, and over 85% as “Meeting the Standards with Distinction”. If this rubric applies equally to the NDMC subset we have studied, this would mean Aristo has met the standard with distinction in 8th Grade Science.","One way of testing robustness in multiple choice is to change or add incorrect answer options, and see if the system's performance degrades (BID26). If a system has mastery of the material, we would expect its score to be relatively unaffected by such modifications. To explore this, we investigated adversarially adding extra incorrect options, i.e., searching for answer options that might confuse the system, using AristoRoBERTa, and adding them as extra choices to the existing questions.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How much performance improvements they achieve on SQuAD?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
By how much is performance improved with multimodality?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What was their accuracy score?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What dataset did they evaluate on?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What experimental evaluation is used?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How is the architecture fault-tolerant?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Which elements of the platform are modular?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
what models did they compare with?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How much is classification performance improved in experiments for low data regime and class-imbalance problems?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
how was annotation done?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
By how much does using phonetic feedback improve state-of-the-art systems?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What languages are used as input?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are the components of the classifier?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
By how much does their method outperform the multi-head attention model?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How large is the corpus they use?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Does DCA or GMM-based attention perform better in experiments?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
what is the monolingual baseline?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What empirical evaluation was used?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Which linguistic features are used?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How to extract affect attributes from the sentence?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What dataset is used to train the model?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How is the proficiency score calculated?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What proficiency indicators are used to the score the utterances?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What accuracy is achieved by the speech recognition system?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How is the speech recognition system evaluated?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How many of the utterances are transcribed?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How many utterances are in the corpus?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How long are the two unlabelled corpora?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Does larger granularity lead to better translation quality?,Sample Answer,1907.12984-Context-aware Translation Model-2,1907.12984-Partial Decoding-0,1907.12984-Speech Irregularities Normalization-6,1907.12984-Constrained Decoding and Content Censorship-0,1907.12984-Constrained Decoding and Content Censorship-1,"For example, by treating a full-sentence as an IU, the model is reduced to the standard translation model. When the IU is one segment, it is reduced to the segment-to-segment translation model BIBREF18 , BIBREF12 . Moreover, if we treat one token as an IU, it is reduced to our previous wait-k model BIBREF0 . The key point of our model is to train the IU detector to recognize the IU boundary at the corresponding granularity.","Traditional NMT models are usually trained on bilingual corpora containing only complete sentences. However in our context-aware translation model, information units usually are sub-sentences. Intuitively, the discrepancy between the training and the decoding will lead to a problematic translation, if we use the conventional NMT model to translate such information units. On the other hand, conventional NMT models rarely do anticipation. Whereas in simultaneous interpreting, human interpreters often have to anticipate the up-coming input and render a constituent at the same time or even before it is uttered by the speaker.","In the above definition, the value of INLINEFORM0 and INLINEFORM1 can be efficiently computed by a language model. In our final system, we firstly train a language model on the domain-specific monolingual corpus, and then identify the abnormal content before the context-aware translation model. For the detected abnormal content, we simply discard it rather than finding an alternative, which will lead to additional errors potentially. Actually, human interpreters often routinely omit source content due to the limited memory.","For an industrial product, it is extremely important to control the content that will be presented to the audience. Additionally, it is also important to make a consistent translation for the domain-specific entities and terminologies. This two demands lead to two associate problems: content censorship and constrained decoding, where the former aims to avoid producing some translation while the latter has the opposite target, generating pre-specified translation.","Recently, post2018fast proposed a Dynamic Beam Allocation (DBA) strategy, a beam search algorithm that forces the inclusion of pre-specified words and phrases in the output. In the DBA strategy, there are many manually annotated constraints, to force the beam search generating the pre-specified translation. To satisfy the requirement of content censorship, we extend this algorithm to prevent the model from generating the pre-specified forbidden content, a collection that contains words and phrases alluding to violence, religion, sex and politics. Specially, during the beam search, we punish the candidate beam that matches a constraint of pre-specified forbidden content, to prevent it from being selected as the final translation.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How big is dataset used for training/testing?,Sample Answer,1910.12618-Introduction-0,1910.12618-Experiments ::: Feature selection-4,1910.12618-Experiments ::: Interpretability of the models ::: Vector embedding representation-4,1910.12618--1,1910.12618-17-TableA.10-1.png,"Whether it is in the field of energy, finance or meteorology, accurately predicting the behavior of time series is nowadays of paramount importance for optimal decision making or profit. While the field of time series forecasting is extremely prolific from a research point-of-view, up to now it has narrowed its efforts on the exploitation of regular numerical features extracted from sensors, data bases or stock exchanges. Unstructured data such as text on the other hand remains underexploited for prediction tasks, despite its potentially valuable informative content. Empirical studies have already proven that textual sources such as news articles or blog entries can be correlated to stock exchange time series and have explanatory power for their variations BIBREF0, BIBREF1. This observation has motivated multiple extensive experiments to extract relevant features from textual documents in different ways and use them for prediction, notably in the field of finance. In Lavrenko et al. BIBREF2, language models (considering only the presence of a word) are used to estimate the probability of trends such as surges or falls of 127 different stock values using articles from Biz Yahoo!. Their results show that this text driven approach could be used to make profit on the market. One of the most conventional ways for text representation is the TF-IDF (Term Frequency - Inverse Document Frequency) approach. Authors have included such features derived from news pieces in multiple traditional machine learning algorithms such as support vector machines (SVM) BIBREF3 or logistic regression BIBREF4 to predict the variations of financial series again. An alternative way to encode the text is through latent Dirichlet allocation (LDA) BIBREF5. It assigns topic probabilities to a text, which can be used as inputs for subsequent tasks. This is for instance the case in Wang's aforementioned work (alongside TF-IDF). In BIBREF6, the authors used Reuters news encoded by LDA to predict if NASDAQ and Dow Jones closing prices increased or decreased compared to the opening ones. Their empirical results show that this approach was efficient to improve the prediction of stock volatility. More recently Kanungsukkasem et al. BIBREF7 introduced a variant of the LDA graphical model, named FinLDA, to craft probabilities that are specifically tailored for a financial time series prediction task (although their approach could be generalized to other ones). Their results showed that indeed performance was better when using probabilities from their alternative than those of the original LDA. Deep learning with its natural ability to work with text through word embeddings has also been used for time series prediction with text. Combined with traditional time series features, the authors of BIBREF8 derived sentiment features from a convolutional neural network (CNN) to reduce the prediction error of oil prices. Akita et al. BIBREF9 represented news articles through the use of paragraph vectors BIBREF10 in order to predict 10 closing stock values from the Nikkei 225. While in the case of financial time series the existence of specialized press makes it easy to decide which textual source to use, it is much more tedious in other fields. Recently in Rodrigues et al. BIBREF11, short description of events (such as concerts, sports matches, ...) are leveraged through a word embedding and neural networks in addition to more traditional features. Their experiments show that including the text can bring an improvement of up to 2% of root mean squared error compared to an approach without textual information. Although the presented studies conclude on the usefulness of text to improve predictions, they never thoroughly analyze which aspects of the text are of importance, keeping the models as black-boxes.","The results of this procedure for the French data is represented in figure FIGREF24. The best median $R^2$ is achieved for $V^* = 52$, although one could argue that not much gain is obtained after 36 words. The results are very similar for the UK data set, thus for the sake of simplicity the same value $V^* = 52$ is used. Note that the same subset of words is used for all the different forecasting models, which could be improved in further work using other selection criteria (e.g. mutual information, see BIBREF29). An example of normalized feature importance is given in figure. FIGREF32.","In order to achieve a global view of the embeddings, the t-SNE algorithm BIBREF30 is applied to project an embedding matrix into a 2 dimensional space, for both languages. The observations for the few aforementioned words are confirmed by this representation, as plotted in figure FIGREF44. Thematic clusters can be observed, roughly corresponding to winter, summer, week-days, week-end days for both languages. Globally summer and winter seem opposed, although one should keep in mind that the t-SNE representation does not preserve the cosine distance. The clusters of the French embedding appear much more compact than the UK one, comforting the observations made when explicitly calculating the cosine distances.","While not strictly normally distributed, the residuals for the French electricity demand display an acceptable behavior. This holds also true for the British consumption, and both temperature time series, but is of lesser quality for the wind one.",Table A.10: Forecast errors on the national wind for France.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What geometric properties do embeddings display?,Sample Answer,1910.12618--1,1910.12618-17-TableA.10-1.png,1910.12618-17-TableA.11-1.png,1910.12618-17-TableA.12-1.png,1910.12618-20-TableA.14-1.png,"While not strictly normally distributed, the residuals for the French electricity demand display an acceptable behavior. This holds also true for the British consumption, and both temperature time series, but is of lesser quality for the wind one.",Table A.10: Forecast errors on the national wind for France.,Table A.11: Forecast errors on the national temperature for Great-Britain.,Table A.12: Forecast errors on the national wind for Great-Britain.,"Table A.14: Closest words (in the cosine sense) to ”August”,”Sunday and ”thunderstorms” for the France",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How accurate is model trained on text exclusively?,Sample Answer,1910.12618-Experiments ::: Feature selection-2,1910.12618--0,1910.12618--1,1910.12618-13-Figure7-1.png,1910.12618-19-FigureA.13-1.png,The features are then successively added to the RF in decreasing order of feature importance.,Additional results for the prediction tasks on temperature and wind speed can be found in tables TABREF47 to TABREF50. An example of forecast for the French temperature is given in figure FIGREF51.,"While not strictly normally distributed, the residuals for the French electricity demand display an acceptable behavior. This holds also true for the British consumption, and both temperature time series, but is of lesser quality for the wind one.",Figure 7: Coefficients βw in the british load LASSO regression.,Figure A.13: Coefficients βw in the British wind LASSO regression.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the size of the dataset?,Sample Answer,1909.02265-Introduction-0,1909.02265-Introduction-3,1909.02265-Introduction-5,1909.02265-Introduction-6,1909.02265-Introduction-10,"In this work, we investigate the problem of task-oriented dialogue in mixed-domain settings. Our work is related to two lines of research in Spoken Dialogue System (SDS), namely task-oriented dialogue system and multi-domain dialogue system. We briefly review the recent literature related to these topics as follows.","One of the significant effort in developing end-to-end task-oriented systems is the recent Sequicity framework BIBREF8. This framework also relies on the sequence-to-sequence model and can be optimized with supervised or reinforcement learning. The Sequicity framework introduces the concept of belief span (bspan), which is a text span that tracks the dialogue states at each turn. In this framework, the task-oriented dialogue problem is decomposed into two stages: bspan generation and response generation. This framework has been shown to significantly outperform state-of-the-art pipeline-based methods.","The problem that we are interested in this work is task-oriented dialogue in mixed-domain settings. This is different from the multi-domain dialogue problem above in several aspects, as follows:","First, we investigate the phenomenon of alternating between different dialogue domains in subsequent dialogue turns, where each turn is defined as a pair of user question and machine answer. That is, the domains are mixed between turns. For example, in the first turn, the user requests some information of a restaurant; then in the second turn, he switches to the a different domain, for example, he asks about the weather at a specific location. In a next turn, he would either switch to a new domain or come back to ask about some other property of the suggested restaurant. This is a realistic scenario which usually happens in practical chatbot applications in our observations. We prefer calling this problem mixed-domain dialogue rather than multiple-domain dialogue.",A combination of specialized state tracking system and an end-to-end task-oriented dialogue system is beneficial in mix-domain dialogue systems. Our hybrid system is able to improve the belief tracking accuracy of about 28% of average absolute point on a standard multi-domain dialogue dataset.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what was the baseline?,Sample Answer,1907.03060-Our Japanese–Russian Setting-0,1907.03060-MT Methods Examined-11,1907.03060-Augmentation with Back-translation-0,1907.03060-Augmentation with Back-translation-7,1907.03060-Acknowledgments-0,"In this paper, we deal with Ja INLINEFORM0 Ru news translation. This language pair is very challenging because the languages involved have completely different writing system, phonology, morphology, grammar, and syntax. Among various domains, we experimented with translations in the news domain, considering the importance of sharing news between different language speakers. Moreover, news domain is one of the most challenging tasks, due to large presence of out-of-vocabulary (OOV) tokens and long sentences. To establish and evaluate existing methods, we also involved English as the third language. As direct parallel corpora are scarce, involving a language such as English for pivoting is quite common BIBREF10 .",We used the open-source implementation of the RNMT and the Transformer models in tensor2tensor. A uni-directional model for each of the six translation directions was trained on the corresponding parallel corpus. Bi-directional and M2M models were realized by adding an artificial token that specifies the target language to the beginning of each source sentence and shuffling the entire training data BIBREF8 .,"Given that the M2M Transformer NMT model (b3) achieved best results for most of the translation directions and competitive results for the rest, we further explored it through back-translation.","Selected monolingual sentences were then translated using the M2M Transformer NMT model (b3) to compose pseudo-parallel data. Then, the pseudo-parallel data were enlarged by over-sampling as summarized in Table TABREF32 . Finally, new NMT models were trained on the concatenation of the original parallel and pseudo-parallel data from scratch in the same manner as the previous NMT models with the same hyper-parameters.","This work was carried out when Aizhan Imankulova was taking up an internship at NICT, Japan. We would like to thank the reviewers for their insightful comments. A part of this work was conducted under the program “Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology” of the Ministry of Internal Affairs and Communications (MIC), Japan.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How much gain does the model achieve with pretraining MVCNN?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are the strong baselines you have?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How do they show genetic relationships between languages?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What type of system does the baseline classification use?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are dilated convolutions?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What baselines did they compare their model with?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What was the performance of their model?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What evaluation metrics are used?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How were the navigation instructions collected?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What language is the experiment done in?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How are content clusters used to improve the prediction of incident severity?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What cluster identification method is used in this paper?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Where did they get training data?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What extraction model did they use?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Which datasets did they experiment on?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Why is lemmatization not necessary in English?,Sample Answer,1909.03135-Word sense disambiguation test sets-6,1909.03135-Word sense disambiguation test sets-20,1909.03135-Word sense disambiguation test sets-32,1909.03135-Experiments-4,1909.03135-4-Table4-1.png,atmosphere,plan,`...the diplomatic bodies of the Bogdikhan and the Empress was furnished with extraordinary solemnity. Thousands of mandarins and other dignitaries were placed on three marble terraces leading to...'.,"Table TABREF32 shows the results, together with the random and most frequent sense (MFS) baselines for each dataset.",Table 4: Averaged macro-F1 scores for WSD,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How big was the corpora they trained ELMo on?,Sample Answer,1909.03135-Word sense disambiguation test sets-25,1909.03135-Word sense disambiguation test sets-26,1909.03135-Word sense disambiguation test sets-27,1909.03135-Word sense disambiguation test sets-30,1909.03135-Experiments-4,"In some situations Postscript can be faster than the escape sequence type of printer control file. It uses post fix notation, where arguments come first and operators follow. This is basically the same as Reverse Polish Notation as used on certain calculators, and follows directly from the stack based approach.","It this sentence, the word `argument' is used in the sense of a mathematical operator.",The RUSSE'18 dataset was created in 2018 for the shared task in Russian word sense induction. This dataset contains only nouns; the list of words with their English translations is given in Table TABREF30.,Here is an example from the RUSSE'18 for the ambiguous word russianмандарин `mandarin' in the sense `Chinese official title':,"Table TABREF32 shows the results, together with the random and most frequent sense (MFS) baselines for each dataset.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What sources did they get the data from?,Sample Answer,2003.10564-Introduction-0,2003.10564-Introduction ::: Ambiguity in non-diacritized text-0,2003.10564-Introduction ::: Improving generalization performance-0,2003.10564-Results-0,2003.10564-Conclusions and Future Work-0,"Yorùbá is a tonal language spoken by more than 40 Million people in the countries of Nigeria, Benin and Togo in West Africa. The phonology is comprised of eighteen consonants, seven oral vowel and five nasal vowel phonemes with three kinds of tones realized on all vowels and syllabic nasal consonants BIBREF0. Yorùbá orthography makes notable use of tonal diacritics, known as amí ohùn, to designate tonal patterns, and orthographic diacritics like underdots for various language sounds BIBREF1, BIBREF2.","The main challenge in non-diacritized text is that it is very ambiguous BIBREF3, BIBREF4, BIBREF1, BIBREF5. ADR attempts to decode the ambiguity present in undiacritized text. Adegbola et al. assert that for ADR the “prevailing error factor is the number of valid alternative arrangements of the diacritical marks that can be applied to the vowels and syllabic nasals within the words"" BIBREF1.","To make the first open-sourced ADR models available to a wider audience, we tested extensively on colloquial and conversational text. These soft-attention seq2seq models BIBREF3, trained on the first three sources in Table TABREF5, suffered from domain-mismatch generalization errors and appeared particularly weak when presented with contractions, loan words or variants of common phrases. Because they were trained on majority Biblical text, we attributed these errors to low-diversity of sources and an insufficient number of training examples. To remedy this problem, we aggregated text from a variety of online public-domain sources as well as actual books. After scanning physical books from personal libraries, we successfully employed commercial Optical Character Recognition (OCR) software to concurrently use English, Romanian and Vietnamese characters, forming an approximative superset of the Yorùbá character set. Text with inconsistent quality was put into a special queue for subsequent human supervision and manual correction. The post-OCR correction of Háà Ènìyàn, a work of fiction of some 20,038 words, took a single expert two weeks of part-time work by to review and correct. Overall, the new data sources comprised varied text from conversational, various literary and religious sources as well as news magazines, a book of proverbs and a Human Rights declaration.","We evaluated the ADR models by computing a single-reference BLEU score using the Moses multi-bleu.perl scoring script, the predicted perplexity of the model's own predictions and the Word Error Rate (WER). All models with additional data improved over the 3-corpus soft-attention baseline, with JW300 providing a {33%, 11%} boost in BLEU and absolute WER respectively. Error analyses revealed that the Transformer was robust to receiving digits, rare or code-switched words as input and degraded ADR performance gracefully. In many cases, this meant the model predicted the undiacritized word form or a related word from the context, but continued to correctly predict subsequent words in the sequence. The FastText embedding provided a small boost in performance for the Transformer, but was mixed across metrics for the soft-attention models.","Promising next steps include further automation of our human-in-the-middle data-cleaning tools, further research on contextualized word embeddings for Yorùbá and serving or deploying the improved ADR models in user-facing applications and devices.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the contributions of this paper?,Sample Answer,1810.02229-Task Description-2,1810.02229-Dataset-1,1810.02229-Results and Discussion-1,1810.02229-Results and Discussion-4,1810.02229-Conclusion and Future Work-0,"Each event is further assigned to one of 7 possible classes, namely: OCCURRENCE, ASPECTUAL, PERCEPTION, REPORTING, I(NTESIONAL)_STATE, I(NTENSIONAL)_ACTION, and STATE. These classes are derived from the English TimeML Annotation Guidelines BIBREF12 . The TimeML event classes distinguishes with respect to other classifications, such as ACE BIBREF1 or FrameNet BIBREF13 , because they expresses relationships the target event participates in (such as factual, evidential, reported, intensional) rather than semantic categories denoting the meaning of the event. This means that the EVENT classes are assigned by taking into account both the semantic and the syntactic context of occurrence of the target event. Readers are referred to the EVENTI Annotation Guidelines for more details.","Tables 1 and 1 report, respectively, the distribution of the events per token part-of speech (POS) and per event class. Not surprisingly, verbs are the largest annotated category, followed by nouns, adjectives, and prepositional phrases. Such a distribution reflects both a kind of “natural” distribution of the realization of events in an Indo-european language, and, at the same time, specific annotation choices. For instance, adjectives have been annotated only when in a predicative position and when introduced by a copula or a copular construction. As for the classes, OCCURRENCE and STATE represent the large majority of all events, followed by the intensional ones (I_STATE and I_ACTION), expressing some factual relationship between the target events and their arguments, and finally the others (REPORTING, ASPECTUAL, and PERCEPTION).","The results of the Bi-LSTM-CRF network are varied in both evaluation configurations. The differences are mainly due to the embeddings used to initialize the network. The best embedding configuration is Fastext-It that differentiate from all the others for the approach used for generating the embeddings. Embedding's dimensionality impacts on the performances supporting the findings in BIBREF14 , but it seems that the quantity (and variety) of data used to generate the embeddings can have a mitigating effect, as shown by the results of the DH-FBK-100 configuration (especially in the classification subtask, and in the Recall scores for the event extent subtask). Coverage of the embeddings (and consequenlty, tokenization of the dataset and the embeddings) is a further aspect to keep into account, but it seems to have a minor impact with respect to dimensionality. It turns out that BIBREF15 's embeddings are those suffering the most from out of vocabulary (OVV) tokens (2.14% and 1.06% in training, 2.77% and 1.84% in test for the word2vec model and GloVe, respectively) with respect to the others. However, they still outperform DH-FBK_100 and ILC-ItWack, whose OVV are much lower (0.73% in training and 1.12% in test for DH-FBK_100; 0.74% in training and 0.83% in test for ILC-ItWack).","Concerning the classification, we focused on the mismatches between correctly identified events (extent layer) and class assignment. The Fastext-It model wrongly assigns the class to only 557 event tokens compared to the 729 cases for FBK-HLT. The distribution of the class errors, in terms of absolute numbers, is the same between the two systems, with the top three wrong classes being, in both cases, OCCURRENCE, I_ACTION and STATE. OCCURRENCE, not surprisingly, is the class that tends to be assigned more often by both systems, being also the most frequent. However, if FBK-HLT largely overgeneralizes OCCURRENCE (59.53% of all class errors), this corresponds to only one third of the errors (37.70%) in the Bi-LSTM-CRF network. Other notable differences concern I_ACTION (27.82% of errors for the Bi-LSTM-CRF vs. 17.28% for FBK-HLT), STATE (8.79% for the Bi-LSTM-CRF vs. 15.22% for FBK-HLT) and REPORTING (7.89% for the Bi-LSTM-CRF vs. 2.33% for FBK-HLT) classes.","This paper has investigated the application of different word embeddings for the initialization of a state-of-the-art Bi-LSTM-CRF network to solve the event detection and classification task in Italian, according to the EVENTI exercise. We obtained new state-of-the-art results using the Fastext-It embeddings, and improved the F1-class score of 6.5 points in strict evaluation mode. As for the event detection subtask, we observe a limited improvement (+1.3 points in strict F1), mainly due to gains in Recall. Such results are extremely positive as the task has been modeled in a single step approach, i.e. detection and classification at once, for the first time in Italian. Further support that embeddings have a major impact in the performance of neural architectures is provided, as the variations in performance of the Bi-LSMT-CRF models show. This is due to a combination of factors such as dimensionality, (raw) data, and the method used for generating the embeddings.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the baselines this paper uses?,Sample Answer,1810.02229-Introduction-0,1810.02229-Dataset-1,1810.02229-Results and Discussion-1,1810.02229-Results and Discussion-4,1810.02229-Conclusion and Future Work-0,"Current societies are exposed to a continuous flow of information that results in a large production of data (e.g. news articles, micro-blogs, social media posts, among others), at different moments in time. In addition to this, the consumption of information has dramatically changed: more and more people directly access information through social media platforms (e.g. Facebook and Twitter), and are less and less exposed to a diversity of perspectives and opinions. The combination of these factors may easily result in information overload and impenetrable “filter bubbles”. Events, i.e. things that happen or hold as true in the world, are the basic components of such data stream. Being able to correctly identify and classify them plays a major role to develop robust solutions to deal with the current stream of data (e.g. the storyline framework BIBREF0 ), as well to improve the performance of many Natural Language Processing (NLP) applications such as automatic summarization and question answering (Q.A.).","Tables 1 and 1 report, respectively, the distribution of the events per token part-of speech (POS) and per event class. Not surprisingly, verbs are the largest annotated category, followed by nouns, adjectives, and prepositional phrases. Such a distribution reflects both a kind of “natural” distribution of the realization of events in an Indo-european language, and, at the same time, specific annotation choices. For instance, adjectives have been annotated only when in a predicative position and when introduced by a copula or a copular construction. As for the classes, OCCURRENCE and STATE represent the large majority of all events, followed by the intensional ones (I_STATE and I_ACTION), expressing some factual relationship between the target events and their arguments, and finally the others (REPORTING, ASPECTUAL, and PERCEPTION).","The results of the Bi-LSTM-CRF network are varied in both evaluation configurations. The differences are mainly due to the embeddings used to initialize the network. The best embedding configuration is Fastext-It that differentiate from all the others for the approach used for generating the embeddings. Embedding's dimensionality impacts on the performances supporting the findings in BIBREF14 , but it seems that the quantity (and variety) of data used to generate the embeddings can have a mitigating effect, as shown by the results of the DH-FBK-100 configuration (especially in the classification subtask, and in the Recall scores for the event extent subtask). Coverage of the embeddings (and consequenlty, tokenization of the dataset and the embeddings) is a further aspect to keep into account, but it seems to have a minor impact with respect to dimensionality. It turns out that BIBREF15 's embeddings are those suffering the most from out of vocabulary (OVV) tokens (2.14% and 1.06% in training, 2.77% and 1.84% in test for the word2vec model and GloVe, respectively) with respect to the others. However, they still outperform DH-FBK_100 and ILC-ItWack, whose OVV are much lower (0.73% in training and 1.12% in test for DH-FBK_100; 0.74% in training and 0.83% in test for ILC-ItWack).","Concerning the classification, we focused on the mismatches between correctly identified events (extent layer) and class assignment. The Fastext-It model wrongly assigns the class to only 557 event tokens compared to the 729 cases for FBK-HLT. The distribution of the class errors, in terms of absolute numbers, is the same between the two systems, with the top three wrong classes being, in both cases, OCCURRENCE, I_ACTION and STATE. OCCURRENCE, not surprisingly, is the class that tends to be assigned more often by both systems, being also the most frequent. However, if FBK-HLT largely overgeneralizes OCCURRENCE (59.53% of all class errors), this corresponds to only one third of the errors (37.70%) in the Bi-LSTM-CRF network. Other notable differences concern I_ACTION (27.82% of errors for the Bi-LSTM-CRF vs. 17.28% for FBK-HLT), STATE (8.79% for the Bi-LSTM-CRF vs. 15.22% for FBK-HLT) and REPORTING (7.89% for the Bi-LSTM-CRF vs. 2.33% for FBK-HLT) classes.","This paper has investigated the application of different word embeddings for the initialization of a state-of-the-art Bi-LSTM-CRF network to solve the event detection and classification task in Italian, according to the EVENTI exercise. We obtained new state-of-the-art results using the Fastext-It embeddings, and improved the F1-class score of 6.5 points in strict evaluation mode. As for the event detection subtask, we observe a limited improvement (+1.3 points in strict F1), mainly due to gains in Recall. Such results are extremely positive as the task has been modeled in a single step approach, i.e. detection and classification at once, for the first time in Italian. Further support that embeddings have a major impact in the performance of neural architectures is provided, as the variations in performance of the Bi-LSMT-CRF models show. This is due to a combination of factors such as dimensionality, (raw) data, and the method used for generating the embeddings.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is increase in percentage of humor contained in headlines generated with TitleStylist method (w.r.t. baselines)?,Sample Answer,2004.01980-Methods ::: Problem Formulation-0,2004.01980-Experiments ::: Evaluation Metrics ::: Setup of Automatic Evaluation-0,2004.01980-Results and Discussion ::: Human Evaluation Results-0,2004.01980-Results and Discussion ::: Automatic Evaluation Results-6,2004.01980-3-Figure2-1.png,"The model is trained on a source dataset $S$ and target dataset $T$. The source dataset $S=\lbrace (\mathbf {a^{(i)}},\mathbf {h^{(i)}})\rbrace _{i=1}^N$ consists of pairs of a news article $\mathbf {a}$ and its plain headline $\mathbf {h}$. We assume that the source corpus has a distribution $P(A, H)$, where $A=\lbrace \mathbf {a^{(i)}}\rbrace _{i=1}^N$, and $H=\lbrace \mathbf {h^{(i)}}\rbrace _{i=1}^N$. The target corpus $T=\lbrace \mathbf {t^{(i)}}\rbrace _{i=1}^{M}$ comprises of sentences $\mathbf {t}$ written in a specific style (e.g., humor). We assume that it conforms to the distribution $P(T)$.","Apart from the comprehensive human evaluation, we use automatic evaluation to measure the generation quality through two conventional aspects: summarization quality and language fluency. Note that the purpose of this two-way automatic evaluation is to confirm that the performance of our model is in an acceptable range. Good automatic evaluation performances are necessary proofs to compliment human evaluations on the model effectiveness.","The human evaluation is to have a comprehensive measurement of the performances. We conduct experiments on four criteria, relevance, attraction, fluency, and style strength. We summarize the human evaluation results on the first three criteria in Table TABREF51, and the last criteria in Table TABREF57. Note that through automatic evaluation, the baselines NST, Fine-tuned, and Gigaword-MASS perform poorer than other methods (in Section SECREF58), thereby we removed them in human evaluation to save unnecessary work for human raters.","We also reported the perplexity (PPL) of the generated headlines to evaluate the language fluency, as shown in Table TABREF59. All outputs from baselines NHG and Multitask and our proposed TitleStylist show similar PPL compared with the test set (used in the fine-tuning stage) PPL 42.5, indicating that they are all fluent expressions for news headlines.",Figure 2: The Transformer-based architecture of our model.,1.0,1.0,1.0,1.0,1.0,0.28571428571428575,0.5,0.2
How are the two different models trained?,Sample Answer,1905.01962-Introduction-2,1905.01962-Introduction-4,1905.01962-Results-0,1905.01962-Discussion-1,1905.01962-Namesake-0,"We show that BERT performs well on hyperpartisan sentiment classification. We use unsupervised learning on the set of 600,000 source-labeled articles provided as part of the task, then train using supervised learning for the 645 hand-labeled articles. We believe that learning on source-labeled articles would bias our model to learn the partisanship of a source, instead of the article. Additionally, the accuracy of the model on validation data labeled by article differs heavily when the articles are labeled by publisher. Thus, we decided to use a small subset of the hand-labeled articles as our validation set for all of our experiments. As the articles are too large for the model to be trained on the full text each time, we consider the number of word-pieces that the model uses from each article a hyperparameter.","In this paper, we demonstrate the effectiveness of BERT models for the hyperpartisan news classification task, with validation accuracy as high as 85% and test accuracy as high as 77% . We also make significant investigations into the importance of different factors relating to the articles and training in BERT's success. The remainder of this paper is organized as follows. Section SECREF2 describes previous work on the BERT model and semi-supervised learning. Section SECREF3 outlines our model, data, and experiments. Our results are presented in Section SECREF4 , with their ramifications discussed in Section SECREF5 . We close with an introduction to our system's namesake, fictional journalist Clint Buchanan, in Section SECREF6 .",Our results are primarily based on a validation set we constructed using the last 20% of the hand-labeled articles. It is important to note that our validation set was fairly unbalanced. About 72% of articles were not hyperpartisan and this mainly arose because we were not provided with a balanced set of hand-labeled articles. The small validation split ended up increasing the imbalance in exchange for training on a more balanced set. The test accuracies we report were done on SemEval Task 4's balanced test dataset.,"Our models classified different parts of a given article identically, demonstrating that the overall hyperpartisan aspects were similar across an article. In addition, the model had significantly lower accuracy when word pieces were shuffled around, but that accuracy was almost entirely restored when shuffling around chunks of four or more word pieces, suggesting that most of the important features can already be extracted at this level.","Our system is named after Clint Buchanan, a fictional journalist on the soap opera One Life to Live. Following the unbelievable stories of Clint and his associates may be one of the few tasks more difficult than identifying hyperpartisan news.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How long is the dataset?,Sample Answer,1905.01962-Model-0,1905.01962-Experiments-1,1905.01962-Results-0,1905.01962-Effective Model Context-1,1905.01962-Namesake-0,"We adjust the standard BERT model for the hyperpartisan news task, evaluating its performance both on a validation set we construct and on the test set provided by Task 4 at SemEval. The training of the model follows the methodology of the original BERT paper.","We also explore whether and how the BERT models we use classify different parts of each individual article. Since the model can only consider a limited number of word pieces and not a full article, we test how the model judges different sections of the same article. Here, we are interested in the extent to which the same class will be assigned to each segment of an article. Finally, we test whether the model's behavior varies if we randomly shuffle word-pieces from the articles during training. Our goal in this experiment is to understand whether the model focuses on individual words and phrases or if it achieves more global understanding. We alter the the size of the chunks to be shuffled ( INLINEFORM0 ) in each iteration of this experiment, from shuffling individual word-pieces ( INLINEFORM1 ) to shuffling larger multiword chunks.",Our results are primarily based on a validation set we constructed using the last 20% of the hand-labeled articles. It is important to note that our validation set was fairly unbalanced. About 72% of articles were not hyperpartisan and this mainly arose because we were not provided with a balanced set of hand-labeled articles. The small validation split ended up increasing the imbalance in exchange for training on a more balanced set. The test accuracies we report were done on SemEval Task 4's balanced test dataset.,"Accuracy drops a lot with only a bag of word pieces, but still reaches 67.4%. Also, most of the accuracy of the model (within 2%) is achieved with only 4-grams of word pieces, so the model is not getting much of a boost from global content.","Our system is named after Clint Buchanan, a fictional journalist on the soap opera One Life to Live. Following the unbelievable stories of Clint and his associates may be one of the few tasks more difficult than identifying hyperpartisan news.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what evaluation metrics were used?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
what state of the art models did they compare with?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How do they determine demographics on an image?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is the most underrepresented person group in ILSVRC?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What dataset did they use?,Sample Answer,2002.01984-Experiments: Factoid Question Answering Task ::: Setup-1,2002.01984-Experiments: Factoid Question Answering Task ::: Setup-6,2002.01984-Experiments: Factoid Question Answering Task ::: Training and error analysis-3,2002.01984-Our Systems and Their Performance on Factoid Questions ::: LAT Feature considered and its impact (slightly negative)-7,"2002.01984-APPENDIX ::: Assumptions, rules and logic flow for deriving Lexical Answer Types from questions ::: Case-2:-1","The original data format is converted to the BERT/BioBERT format, where BioBERT expects ‘start_index’ of the actual answer. The ‘start_index corresponds to the index of the answer text present in the paragraph/ Context. For finding ‘start_index’ we used built-in python function find(). The function returns the lowest index of the actual answer present in the context(paragraph). If the answer is not found ‘-1’ is returned as the index. The efficient way of finding start_index is, if the paragraph (Context) has multiple instances of answer text, then ‘start_index’ of the answer should be that instance of answer text whose context actually matches with what’s been asked in the question.","""Flumazenil use in benzodiazepine overdose in the UK: a retrospective survey of NPIS data. OBJECTIVE: Benzodiazepine (BZD) overdose (OD) continues to cause significant morbidity and mortality in the UK. Flumazenil is an effective antidote but there is a risk of seizures, particularly in those who have co-ingested tricyclic antidepressants. A study was undertaken to examine the frequency of use, safety and efficacy of flumazenil in the management of BZD OD in the UK. METHODS: A 2-year retrospective cohort study was performed of all enquiries to the UK National Poisons Information Service involving BZD OD. RESULTS: Flumazenil was administered to 80 patients in 4504 BZD-related enquiries, 68 of whom did not have ventilatory failure or had recognised contraindications to flumazenil. Factors associated with flumazenil use were increased age, severe poisoning and ventilatory failure. Co-ingestion of tricyclic antidepressants and chronic obstructive pulmonary disease did not influence flumazenil administration. Seizure frequency in patients not treated with flumazenil was 0.3%"".",Question: Which mutated gene causes Chediak Higashi Syndrome?,LATs are terms in the question that indicate what type of entity is being asked for.,Example Question: Which enzyme is targeted by Evolocumab?,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What was their highest MRR score?,Sample Answer,2002.01984-Experiments: Factoid Question Answering Task ::: Setup-6,2002.01984-Our Systems and Their Performance on Factoid Questions-4,2002.01984-Our Systems and Their Performance on Factoid Questions ::: Impact of Using Context from URLs (negative)-0,2002.01984-Summary of our results ::: List Questions-0,"2002.01984-APPENDIX ::: Assumptions, rules and logic flow for deriving Lexical Answer Types from questions ::: Case-2:-1","""Flumazenil use in benzodiazepine overdose in the UK: a retrospective survey of NPIS data. OBJECTIVE: Benzodiazepine (BZD) overdose (OD) continues to cause significant morbidity and mortality in the UK. Flumazenil is an effective antidote but there is a risk of seizures, particularly in those who have co-ingested tricyclic antidepressants. A study was undertaken to examine the frequency of use, safety and efficacy of flumazenil in the management of BZD OD in the UK. METHODS: A 2-year retrospective cohort study was performed of all enquiries to the UK National Poisons Information Service involving BZD OD. RESULTS: Flumazenil was administered to 80 patients in 4504 BZD-related enquiries, 68 of whom did not have ventilatory failure or had recognised contraindications to flumazenil. Factors associated with flumazenil use were increased age, severe poisoning and ventilatory failure. Co-ingestion of tricyclic antidepressants and chronic obstructive pulmonary disease did not influence flumazenil administration. Seizure frequency in patients not treated with flumazenil was 0.3%"".",using the BioAsq snippets for context vs. using the documents from the provided URLs for context,"In some experiments, for context in testing, we used documents for which URL pointers are provided in BioAsq. However, our system UNCC_QA3 underperformed our other system tested only on the provided snippets.","For List-type questions, although post processing helped in the later batches, we never managed to obtain competitive precision, although our recall was good.",Example Question: Which enzyme is targeted by Evolocumab?,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How many sentence transformations on average are available per unique sentence in dataset?,Sample Answer,1912.01673-Annotation ::: First Round: Collecting Ideas-1,1912.01673-Annotation ::: Second Round: Collecting Data ::: Seed Data-5,1912.01673-Annotation ::: Second Round: Collecting Data ::: Seed Data-7,1912.01673-Conclusion and Future Work-2,1912.01673-3-Table1-1.png,"Unfortunately, these examples turned out to be highly influential on the annotators' decisions and they correspond to almost two thirds of all of modifications gathered in the first round. Other very common transformations include change of a word order or transformation into a interrogative/imperative sentence.",Zvláštní ekvádorský případ Correa vs. Crudo,"too vague, overly dependent on the context:",The corpus is freely available at the following link:,"Table 1: Examples of transformations given to annotators for the source sentence Several hunters slept on a clearing. The third column shows how many of all the transformation suggestions collected in the first round closely mimic the particular example. The number is approximate as annotators typically call one transformation by several names, e.g. less formally, formality diminished, decrease of formality, not formal expressions, non-formal, less formal, formality decreased, ...",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What annotations are available in the dataset?,Sample Answer,1912.01673-Background-13,1912.01673-Annotation ::: First Round: Collecting Ideas-2,1912.01673-Annotation ::: Second Round: Collecting Data ::: Sentence Transformations-2,1912.01673-Annotation ::: Second Round: Collecting Data ::: Seed Data-5,1912.01673-Dataset Description ::: First Observations-1,"We would like to move to more interesting non-trivial sentence comparison, beyond those in zhu-etal-2018-exploring or BaBo2019, such as change of style of a sentence, the introduction of a small modification that drastically changes the meaning of a sentence or reshuffling of words in a sentence that alters its meaning.","Other interesting modification were also proposed such as change into a fairy-tale style, excessive use of diminutives/vulgarisms or dadaism—a swap of roles in the sentence so that the resulting sentence is grammatically correct but nonsensical in our world. Of these suggestions, we selected only the dadaistic swap of roles for the current exploration (see nonsense in Table TABREF7).","Several modification types were specifically selected to constitute a thorough test of embeddings. In different meaning, the annotators should create a sentence with some other meaning using the same words as the original sentence. Other transformations which should be difficult for embeddings include minimal change, in which the sentence meaning should be significantly changed by using only very small modification, or nonsense, in which words of the source sentence should be shuffled so that it is grammatically correct, but without any sense.",Zvláštní ekvádorský případ Correa vs. Crudo,"The lack of semantic relations in the LASER space is also reflected in vector similarities, summarized in similarities. The minimal change operation substantially changed the meaning of the sentence, and yet the embedding of the transformation lies very closely to the original sentence (average similarity of 0.930). Tense changes and some form of negation or banning also keep the vectors very similar.",1.0,1.0,1.0,1.0,1.0,0.28571428571428575,0.5,0.2
"How are possible sentence transformations represented in dataset, as new sentences?",Sample Answer,1912.01673-Annotation ::: First Round: Collecting Ideas-0,1912.01673-Annotation ::: Second Round: Collecting Data ::: Seed Data-1,1912.01673-Annotation ::: Second Round: Collecting Data ::: Seed Data-5,1912.01673-Annotation ::: Second Round: Collecting Data ::: Seed Data-8,1912.01673-Dataset Description-0,"We manually selected 15 newspaper headlines. Eleven annotators were asked to modify each headline up to 20 times and describe the modification with a short name. They were given an example sentence and several of its possible alternations, see tab:firstroundexamples.","too unreal, out of this world, such as:",Zvláštní ekvádorský případ Correa vs. Crudo,Běž tam a mluv na ni.,"In the second round, we collected 293 annotations from 12 annotators. After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. Statistics of individual annotators are available in tab:statistics.",1.0,1.0,1.0,1.0,1.0,0.33333333333333337,1.0,0.2
What are all 15 types of modifications ilustrated in the dataset?,Sample Answer,1912.01673-Introduction-3,1912.01673-Introduction-5,1912.01673-Annotation ::: Second Round: Collecting Data ::: Seed Data-5,1912.01673-Dataset Description-0,1912.01673-3-Table1-1.png,"Sentence embeddings are becoming equally ubiquitous in NLP, with novel representations appearing almost every other week. With an overwhelming number of methods to compute sentence vector representations, the study of their general properties becomes difficult. Furthermore, it is not so clear in which way the embeddings should be evaluated.","In this work, we present COSTRA, a new dataset of COmplex Sentence TRAnsformations. In its first version, the dataset is limited to sample sentences in Czech. The goal is to support studies of semantic and syntactic relations between sentences in the continuous space. Our dataset is the prerequisite for one of possible ways of exploring sentence meaning relatability: we envision that the continuous space of sentences induced by an ideal embedding method would exhibit topological similarity to the graph of sentence variations. For instance, one could argue that a subset of sentences could be organized along a linear scale reflecting the formalness of the language used. Another set of sentences could form a partially ordered set of gradually less and less concrete statements. And yet another set, intersecting both of the previous ones in multiple sentences could be partially or linearly ordered according to the strength of the speakers confidence in the claim.",Zvláštní ekvádorský případ Correa vs. Crudo,"In the second round, we collected 293 annotations from 12 annotators. After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. Statistics of individual annotators are available in tab:statistics.","Table 1: Examples of transformations given to annotators for the source sentence Several hunters slept on a clearing. The third column shows how many of all the transformation suggestions collected in the first round closely mimic the particular example. The number is approximate as annotators typically call one transformation by several names, e.g. less formally, formality diminished, decrease of formality, not formal expressions, non-formal, less formal, formality decreased, ...",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How slow is the unparallelizable ART model in the first place?  ,Sample Answer,1909.06708-Introduction-4,1909.06708-Approach-0,1909.06708-Approach ::: Hints from the ART teacher Model ::: Hints from word alignments-1,1909.06708-Experiments ::: Experimental Settings-1,1909.06708-Experiments ::: Experimental Results-3,"To tackle this, we proposed a novel hint-based method for NART model training. We first investigate the causes of the poor performance of the NART model. Comparing with the ART model, we find that: (1) the positions where the NART model outputs incoherent tokens will have very high hidden states similarity; (2) the attention distributions of the NART model are more ambiguous than those of ART model. Therefore, we design two kinds of hints from the hidden states and attention distributions of the ART model to help the training of the NART model. The experimental results show that our model achieves significant improvement over the NART baseline models and is even comparable to a strong ART baseline in BIBREF4.","In this section, we first describe the observations on the ART and NART models, and then discuss what kinds of information can be used as hints to help the training of the NART model. We follow the network structure in BIBREF8, use a copy of the source sentence as decoder input, remove the attention masks in decoder self-attention layers and add a positional attention layer as suggested in BIBREF5. We provide a visualization of ART and NART models we used in Figure FIGREF11 and a detailed description of the model structure in Appendix.","In particular, we minimize KL-divergence between the per-head encoder-to-decoder attention distributions of the teacher and the student to encourage the student to have similar word alignments to the teacher model, i.e.","We pretrain Transformer BIBREF8 as the teacher model on each dataset, which achieves 33.26/27.30/31.29 in terms of BLEU BIBREF11 in IWSLT14 De-En, WMT14 En-De and De-En test sets. The student model shares the same number of layers in encoder/decoder, size of hidden states/embeddings and number of heads as the teacher models (Figure FIGREF11). Following BIBREF5, BIBREF12, we replace the target sentences by the decoded output of the teacher models.","According to our empirical analysis, the percentage of repetitive words drops from 8.3% to 6.5% by our proposed methods on the IWSLT14 De-En test set, which is a 20%+ reduction. This shows that our proposed method effectively improve the quality of the translation outputs. We also provide several case studies in Appendix.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What metric is used to measure translation accuracy?,Sample Answer,1909.06708-Approach-0,1909.06708-Approach ::: Hints from the ART teacher Model-0,1909.06708-Experiments ::: Experimental Results-2,1909.06708-Conclusion-0,1909.06708-2-Figure1-1.png,"In this section, we first describe the observations on the ART and NART models, and then discuss what kinds of information can be used as hints to help the training of the NART model. We follow the network structure in BIBREF8, use a copy of the source sentence as decoder input, remove the attention masks in decoder self-attention layers and add a positional attention layer as suggested in BIBREF5. We provide a visualization of ART and NART models we used in Figure FIGREF11 and a detailed description of the model structure in Appendix.","Our study motivates us to leverage the intermediate hidden information from an ART model to improve the NART model. We focus on how to define hints from a well-trained ART teacher model and use it to guide the training process of a NART student model. We study layer-to-layer hints and assume both the teacher and student models have an $M$-layer encoder and an $N$-layer decoder, despite the difference in stacked components.","We also visualize the hidden state cosine similarities and attention distributions for the NART model with hint-based training, as shown in Figure FIGREF4(c) and FIGREF6(c). With hints from hidden states, the hidden states similarities of the NART model decrease in general, and especially for the positions where the original NART model outputs incoherent phrases. The attention distribution of the NART model after hint-based training is more similar to the ART teacher model and less ambiguous comparing to the NART model without hints.","In this paper, we proposed to use hints from a well-trained ART model to enhance the training of NART models. Our results on WMT14 En-De and De-En significantly outperform previous NART baselines, with one order of magnitude faster in inference than ART models. In the future, we will focus on designing new architectures and training methods for NART models to achieve comparable accuracy as ART models.","Figure 1: Case study: the above three figures visualize the hidden state cosine similarities of different models. The axes correspond to the generated target tokens. Each pixel shows the cosine similarities cosij between the last layer hidden states of the i-th and j-th generated tokens, where the diagonal pixel will always be 1.0.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Which two datasets does the resource come from?,Sample Answer,1809.02494-Introduction-1,1809.02494-Qualitative analysis of the data sets-2,1809.02494-A further analysis: apparent issues-1,1809.02494-A further analysis: apparent issues-3,1809.02494-2-Figure1-1.png,"Performing language grounding allows ensuring that generated texts include words whose meaning is aligned with what writers understand or what readers would expect BIBREF0 , given the variation that is known to exist among writers and readers BIBREF5 . Moreover, when contradictory data appears in corpora or any other resource that is used to create the data-to-words mapping, creating models that remove inconsistencies can also be a challenging part of language grounding which can influence the development of a successful system BIBREF3 .","If we focus on single descriptors, one interesting outcome is that some of the answers for “Northern Galicia” and “Southern Galicia” overlap for both subject groups. Thus, although `north' and `south' are natural antonyms, if we take into account the opinion of each group as a whole, there exists a small area where points can be considered as belonging to both descriptors at the same time (see Fig. FIGREF9 ). In the case of “west” and “east”, the drawings made by the experts were almost divergent and showed no overlapping between those two descriptors.","In the case of the students, we have identified minor drawing errors appearing in most of the descriptors, which in general shouldn't have a negative impact in the long term thanks to the high number of participants in the original survey. For some descriptors, however, there exist polygons drawn by subjects that clearly deviate from what could be considered a proper answer. The clearest example of this problem involves the `west' and `east' descriptors, which were confused by some of the students who drew them inversely (see Fig. FIGREF11 , around 10-15% of the answers).","As for the expert group, a similar case is found for “Northeastern Galicia” (see Fig. FIGREF12 ), where some of the given answers (3/8) clearly correspond to “Northwestern Galicia”. However, unlike the issue related to “west” and “east” found for the student group, this problem is not found reciprocally for the “northwestern” answers.",Figure 1: Snapshot of the version of the survey answered by the meteorologists (translated from Spanish).,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What classification tasks do they experiment on?,Sample Answer,1911.03854-Fakeddit-1,1911.03854-Fakeddit-8,1911.03854-Experiments ::: Fake News Detection-1,1911.03854-Acknowledgments-0,1911.03854-4-Table2-1.png,"We sourced our dataset from Reddit, a social news and discussion website where users can post submissions on various subreddits. Each subreddit has its own theme like `nottheonion', where people post seemingly false stories that are surprisingly true. Active Reddit users are able to upvote, downvote, and comment on the submission.","False Connection: Submission images in this category do not accurately support their text descriptions. We have four subreddits with this label, containing posts of images with captions that do not relate to the true meaning of the image. These include misleadingthumbnails, confusing_perspective, pareidolia, and fakehistoryporn.","We used the InferSent model because it performs very well as a universal sentence embeddings generator. For this model, we loaded a vocabulary of 1 million of the most common words in English and used fastText as opposed to ELMO embeddings because fastText can perform relatively well for rare words and words that do not appear in the vocabulary BIBREF20, BIBREF21. We obtained encoded sentence features of length 4096 for each submission title using InferSent.","We would like to acknowledge Facebook for the Online Safety Benchmark Award. The authors are solely responsible for the contents of the paper, and the opinions expressed in this publication do not reflect those of the funding agencies.","Table 2: Results on fake news detection for 2, 3, and 5-way classification with combination method of maximum.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the most discriminating patterns which are analyzed?,Sample Answer,1709.05295-Introduction-0,1709.05295-Data-0,1709.05295-Analysis-3,1709.05295-Analysis-4,1709.05295-2-Figure1-1.png,"Human lives are being lived online in transformative ways: people can now ask questions, solve problems, share opinions, or discuss current events with anyone they want, at any time, in any location, on any topic. The purposes of these exchanges are varied, but a significant fraction of them are argumentative, ranging from hot-button political controversies (e.g., national health care) to religious interpretation (e.g., Biblical exegesis). And while the study of the structure of arguments has a long lineage in psychology BIBREF0 and rhetoric BIBREF1 , large shared corpora of natural informal argumentative dialogues have only recently become available.","The IAC corpus is a freely available annotated collection of 109,553 forum posts (11,216 discussion threads). In such forums, conversations are started by posting a topic or a question in a particular category, such as society, politics, or religion BIBREF6 . Forum participants can then post their opinions, choosing whether to respond directly to a previous post or to the top level topic (start a new thread). These discussions are essentially dialogic; however the affordances of the forum such as asynchrony, and the ability to start a new thread rather than continue an existing one, leads to dialogic structures that are different than other multiparty informal conversations BIBREF25 . An additional source of dialogic structure in these discussions, above and beyond the thread structure, is the use of the quote mechanism, which is an interface feature that allows participants to optionally break down a previous post into the components of its argument and respond to each component in turn.","Next, we further examine the NP Prep patterns since they are so prevalent. Figure FIGREF19 shows the percentages of the most frequently occurring prepositions found in the NP Prep patterns learned for each class. Patterns containing the preposition ""of"" make up the vast majority of prepositional phrases for both the fact and feel classes, but is more common in the fact class. In contrast, we observe that patterns with the preposition “for” are substantially more common in the feel class than the fact class.","Table TABREF20 shows examples of learned NP Prep patterns with the preposition ""of"" in the fact class and ""for"" in the feel class. The ""of"" preposition in the factual arguments often attaches to objective terminology. The ""for"" preposition in the feeling-based arguments is commonly used to express advocacy (e.g., demand for) or refer to affected population groups (e.g., treatment for). Interestingly, these phrases are subtle indicators of feeling-based arguments rather than explicit expressions of emotion or sentiment.",Figure 1: Examples of FACTUAL and FEELING based debate forum Quotes and Responses. Only the responses were labeled for FACT vs. FEEL.,1.0,1.0,1.0,1.0,1.0,0.28571428571428575,0.5,0.2
What bootstrapping methodology was used to find new patterns?,Sample Answer,1709.05295-Data-0,1709.05295-Data-1,1709.05295-Bootstrapped Pattern Learning-0,1709.05295-Analysis-4,1709.05295-2-Figure1-1.png,"The IAC corpus is a freely available annotated collection of 109,553 forum posts (11,216 discussion threads). In such forums, conversations are started by posting a topic or a question in a particular category, such as society, politics, or religion BIBREF6 . Forum participants can then post their opinions, choosing whether to respond directly to a previous post or to the top level topic (start a new thread). These discussions are essentially dialogic; however the affordances of the forum such as asynchrony, and the ability to start a new thread rather than continue an existing one, leads to dialogic structures that are different than other multiparty informal conversations BIBREF25 . An additional source of dialogic structure in these discussions, above and beyond the thread structure, is the use of the quote mechanism, which is an interface feature that allows participants to optionally break down a previous post into the components of its argument and respond to each component in turn.","The IAC includes 10,003 Quote-Response (Q-R) pairs with annotations for factual vs. feeling argument style, across a range of topics. Figure FIGREF4 shows the wording of the survey question used to collect the annotations. Fact vs. Feeling was measured as a scalar ranging from -5 to +5, because previous work suggested that taking the means of scalar annotations reduces noise in Mechanical Turk annotations BIBREF26 . Each of the pairs was annotated by 5-7 annotators. For our experiments, we use only the response texts and assign a binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class. We did not use the responses with scores between -1 and 1 because they had a very weak Fact/Feeling assessment, which could be attributed to responses either containing aspects of both factual and feeling expression, or neither. The resulting set contains 3,466 fact and 2,382 feeling posts. We randomly partitioned the fact/feel responses into three subsets: a training set with 70% of the data (2,426 fact and 1,667 feeling posts), a development (tuning) set with 20% of the data (693 fact and 476 feeling posts), and a test set with 10% of the data (347 fact and 239 feeling posts). For the bootstrapping method, we also used 11,560 responses from the unannotated data.",The goal of our research is to gain insights into the types of linguistic expressions and properties that are distinctive and common in factual and feeling based argumentation. We also explore whether it is possible to develop a high-precision fact vs. feeling classifier that can be applied to unannotated data to find new linguistic expressions that did not occur in our original labeled corpus.,"Table TABREF20 shows examples of learned NP Prep patterns with the preposition ""of"" in the fact class and ""for"" in the feel class. The ""of"" preposition in the factual arguments often attaches to objective terminology. The ""for"" preposition in the feeling-based arguments is commonly used to express advocacy (e.g., demand for) or refer to affected population groups (e.g., treatment for). Interestingly, these phrases are subtle indicators of feeling-based arguments rather than explicit expressions of emotion or sentiment.",Figure 1: Examples of FACTUAL and FEELING based debate forum Quotes and Responses. Only the responses were labeled for FACT vs. FEEL.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What patterns were extracted which were correlated with emotional arguments?,Sample Answer,1709.05295-Bootstrapped Pattern Learning-3,1709.05295-Bootstrapped Pattern Learning-4,1709.05295-Evaluation-1,1709.05295-Evaluation-4,1709.05295-2-Figure1-1.png,"AutoSlog-TS uses a set of syntactic templates to define different types of linguistic expressions. The left-hand side of Figure FIGREF8 shows the set of syntactic templates defined in the AutoSlog-TS software package. PassVP refers to passive voice verb phrases (VPs), ActVP refers to active voice VPs, InfVP refers to infinitive VPs, and AuxVP refers to VPs where the main verb is a form of “to be” or “to have”. Subjects (subj), direct objects (dobj), noun phrases (np), and possessives (genitives) can be extracted by the patterns. AutoSlog-TS applies the Sundance shallow parser BIBREF28 to each sentence and finds every possible match for each pattern template. For each match, the template is instantiated with the corresponding words in the sentence to produce a specific lexico-syntactic expression. The right-hand side of Figure FIGREF8 shows an example of a specific lexico-syntactic pattern that corresponds to each general pattern template.","In addition to the original 17 pattern templates in AutoSlog-TS (shown in Figure FIGREF8 ), we defined 7 new pattern templates for the following bigrams and trigrams: Adj Noun, Adj Conj Adj, Adv Adv, Adv Adv Adv, Adj Adj, Adv Adj, Adv Adv Adj. We added these n-gram patterns to provide coverage for adjective and adverb expressions because the original templates were primarily designed to capture noun phrase and verb phrase expressions.","The upper section of Table TABREF11 shows the Precision and Recall results for the patterns learned during bootstrapping. The Iter 0 row shows the performance of the patterns learned only from the original, annotated training data. The remaining rows show the results for the patterns learned from the unannotated texts during bootstrapping, added cumulatively. We show the results after each iteration of bootstrapping.","Table TABREF14 shows the number of patterns learned from the annotated data (Iter 0) and the number of new patterns added after each bootstrapping iteration. The first iteration dramatically increases the set of patterns, and more patterns are steadily added throughout the rest of bootstrapping process.",Figure 1: Examples of FACTUAL and FEELING based debate forum Quotes and Responses. Only the responses were labeled for FACT vs. FEEL.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What patterns were extracted which were correlated with factual arguments?,Sample Answer,1709.05295-Bootstrapped Pattern Learning-3,1709.05295-Bootstrapped Pattern Learning-4,1709.05295-Evaluation-1,1709.05295-Evaluation-4,1709.05295-2-Figure1-1.png,"AutoSlog-TS uses a set of syntactic templates to define different types of linguistic expressions. The left-hand side of Figure FIGREF8 shows the set of syntactic templates defined in the AutoSlog-TS software package. PassVP refers to passive voice verb phrases (VPs), ActVP refers to active voice VPs, InfVP refers to infinitive VPs, and AuxVP refers to VPs where the main verb is a form of “to be” or “to have”. Subjects (subj), direct objects (dobj), noun phrases (np), and possessives (genitives) can be extracted by the patterns. AutoSlog-TS applies the Sundance shallow parser BIBREF28 to each sentence and finds every possible match for each pattern template. For each match, the template is instantiated with the corresponding words in the sentence to produce a specific lexico-syntactic expression. The right-hand side of Figure FIGREF8 shows an example of a specific lexico-syntactic pattern that corresponds to each general pattern template.","In addition to the original 17 pattern templates in AutoSlog-TS (shown in Figure FIGREF8 ), we defined 7 new pattern templates for the following bigrams and trigrams: Adj Noun, Adj Conj Adj, Adv Adv, Adv Adv Adv, Adj Adj, Adv Adj, Adv Adv Adj. We added these n-gram patterns to provide coverage for adjective and adverb expressions because the original templates were primarily designed to capture noun phrase and verb phrase expressions.","The upper section of Table TABREF11 shows the Precision and Recall results for the patterns learned during bootstrapping. The Iter 0 row shows the performance of the patterns learned only from the original, annotated training data. The remaining rows show the results for the patterns learned from the unannotated texts during bootstrapping, added cumulatively. We show the results after each iteration of bootstrapping.","Table TABREF14 shows the number of patterns learned from the annotated data (Iter 0) and the number of new patterns added after each bootstrapping iteration. The first iteration dramatically increases the set of patterns, and more patterns are steadily added throughout the rest of bootstrapping process.",Figure 1: Examples of FACTUAL and FEELING based debate forum Quotes and Responses. Only the responses were labeled for FACT vs. FEEL.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Which shallow approaches did they experiment with?,Sample Answer,1707.06806-Introduction-2,1707.06806-Related Work-1,1707.06806-Method-0,1707.06806-Bidirectional Long Short-Term Memory Network-1,1707.06806-Conclusions-0,"In this paper we propose a method for online content popularity prediction based on a bidirectional recurrent neural network called BiLSTM. This work is inspired by recent successful applications of deep neural networks in many natural language processing problems BIBREF5 , BIBREF6 . Our method attempts to model complex relationships between the title of an article and its popularity using novel deep network architecture that, in contrast to the previous approaches, gives highly interpretable results. Last but not least, the proposed BiLSTM method provides a significant performance boost in terms of prediction accuracy over the standard shallow approach, while outperforming the current state-of-the-art on two distinct datasets with over 40,000 samples.","Forecasting popularity of news articles was especially well studied in the context of Twitter - a social media platform designed specifically for sharing textual data BIBREF11 , BIBREF12 . Not only did the previous works focus on the prediction part, but also on modeling message propagation within the network BIBREF13 . However, most of the works were focused on analyzing the social interactions between the users and the characteristics of so-called social graph of users' connections, rather than on the textual features. Contrary to those approaches, in this paper we base our predictions using only textual features of the article title. We also validate our proposed method on one dataset collected using a different social media platform, namely Facebook, and another one created from various news articles BIBREF4 .",In this section we present the bidirectional LSTM model for popularity prediction. We start by formulating the problem and follow up with the description of word embeddings used in our approach. We then present the Long Short-Term Memory network that serves as a backbone for our bidirectional LSTM architecture. We conclude this section with our interpretation of hidden bidirectional states and describe how they can be employed for title introspection.,"Let INLINEFORM0 be INLINEFORM1 -dimensional word vector corresponding to the INLINEFORM2 -the word in the headline, then a variable length sequence: INLINEFORM3 represents a headline. A recurrent neural network (RNN) processes this sequence by recursively applying a transformation function to the current element of sequence INLINEFORM4 and its previous hidden internal state INLINEFORM5 (optionally outputting INLINEFORM6 ). At each time step INLINEFORM7 , the hidden state is updated by: DISPLAYFORM0 ","In this paper we present a novel approach to the problem of online article popularity prediction. To our knowledge, this is the first attempt of predicting the performance of content on social media using only textual information from its title. We show that our method consistently outperforms benchmark models. Additionally, the proposed method could not only be used to compare competing titles with regard to their estimated probability, but also to gain insights about what constitutes a good title. Future work includes modeling popularity prediction problem with multiple data modalities, such as images or videos. Furthermore, all of the evaluated models function at the word level, which could be problematic due to idiosyncratic nature of social media and Internet content. It is, therefore, worth investigating, whether combining models that operate at the character level to learn and generate vector representation of titles with visual features could improve the overall performance.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Which soft-selection approaches are evaluated?,Sample Answer,1909.11297-Experiments ::: Experimental Results on Multi-Aspect Sentences-1,1909.11297-Conclusion-0,1909.11297-5-Table1-1.png,1909.11297-7-Table3-1.png,1909.11297-7-Table4-1.png,"A multi-aspect sentence can be categorized by two dimensions: the Number of aspects and the Polarity dimension which indicates whether the sentiment polarities of all aspects are the same or not. In the dimension of Number, we categorize the multi-aspect sentences as 2-3 and More. 2-3 refers to the sentences with two or three aspects while More refers to the sentences with more than three aspects. The statistics in the original dataset shows that there are much more sentences with 2-3 aspects than those with More aspects. In the dimension Polarity, the multi-aspect sentences can be categorized into Same and Diff. Same indicates that all aspects in the sentence have the same sentiment polarity. Diff indicates that the aspects have different polarities.","In this paper, we propose a hard-selection approach for aspect-based sentiment analysis, which determines the start and end positions of the opinion snippet for a given input aspect. The deep associations between the sentence and aspect, and the long-term dependencies within the sentence are taken into consideration by leveraging the pre-trained BERT model. With the hard selection of the opinion snippet, our approach can alleviate the attention distraction problem of traditional attention-based soft-selection methods. Experimental results demonstrate the effectiveness of our method. Especially, our hard-selection approach outperforms soft-selection approaches significantly when handling multi-aspect sentences with different sentiment polarities.","Table 1: Dataset statistics. T and C denote the aspectterm and aspect-category tasks, respectively. P, N, and Nu represent the numbers of instances with positive, negative and neutral polarities, and All is the total number of instances.",Table 3: Distribution of the multi-aspect test set. Around 67% of the multi-aspect sentences belong to the Same category.,Table 4: Distribution of the multi-aspect training set. 2-asp and 3-asp indicate that the sentence contains two or three aspects respectively. Each multi-aspect sentence is categorized as Same or Diff.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How is the quality of the translation evaluated?,Sample Answer,1908.05925-Methodology ::: Unsupervised Machine Translation ::: Unsupervised PBSMT-3,1908.05925-Methodology ::: Unknown Word Replacement ::: Step 2-0,1908.05925-Methodology ::: Unknown Word Replacement ::: Step 4-0,1908.05925-Experiments ::: Training ::: Language Model-0,1908.05925-4-Figure2-1.png,"In this task, we follow the method proposed by BIBREF10 to initialize the phrase table, train the KenLM language models BIBREF17 and train a PBSMT model, but we make two changes. First, we only initialize a uni-gram phrase table because of the large vocabulary size of German and Czech and the limitation of computational resources. Second, instead of training the model in the truecase mode, we maintain the same pre-processing step (see more details in §SECREF20) as the NMT models.","Each context word is searched for in the corresponding PBSMT translation. From our observation, the meanings of the words in Czech are highly likely to be the same if only the last few characters are different. Therefore, we allow the last two characters to be different between the context words and the words they match.","Step 2 and Step 3 are repeated until all the context words have been searched. After removing all the punctuation and the context words in the candidate list, the replacement word is the one that most frequently appears in the candidate list. If no candidate word is found, we just remove the <UNK> without adding a word.","According to the findings in BIBREF23, the morphological richness of a language is closely related to the performance of the model, which indicates that the language models will be extremely hard to train for Czech, as it is one of the most complex languages. We train the QRNN model with 12 million sentences randomly sampled from the original WMT Czech monolingual dataset, which is also pre-processed in the way mentioned in §SECREF20. To maintain the quality of the language model, we enlarge the vocabulary size to three million by including all the words that appear more than 15 times. Finally, the PPL of the language model on the test set achieves 93.54.","Figure 2: The illustration of the unknown word replacement (UWR) procedure for word-level NMT. The words of the PBSMT model translation in the pink boxes match the context words of the unknown word <UNK> in the word-level NMT model translation in the blue boxes. Finally, we choose a possible target word, in the yellow box, from the PBSMT model translation to replace the unknown word in the green box.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How large is the dataset they generate?,Sample Answer,1809.08298-Introduction-0,1809.08298-Introduction-1,1809.08298-Introduction-3,1809.08298-Sequence to Sequence Model with Attention Mechanism-1,1809.08298-Acknowledgments-0,A run-on sentence is defined as having at least two main or independent clauses that lack either a conjunction to connect them or a punctuation mark to separate them. Run-ons are problematic because they not only make the sentence unfriendly to the reader but potentially also to the local discourse. Consider the example in Table TABREF1 .,"In the field of grammatical error correction (GEC), most work has typically focused on determiner, preposition, verb and other errors which non-native writers make more frequently. Run-ons have received little to no attention even though they are common errors for both native and non-native speakers. Among college students in the United States, run-on sentences are the 18th most frequent error and the 8th most frequent error made by students who are not native English speakers BIBREF0 .","In this paper, we analyze the task of automatically correcting run-on sentences. We develop two methods: a conditional random field model (roCRF) and a Seq2Seq attention model (roS2S) and show that they outperform models from the sister tasks of punctuation restoration and whole-sentence grammatical error correction. We also experiment with artificially generating training examples in clean, otherwise grammatical text, and show that models trained on this data do nearly as well predicting artificial and naturally occurring run-on sentences.","Our model, roS2S, is a Seq2Seq attention model based on the neural machine translation model BIBREF23 . The encoder is a bidirectional LSTM, where a recurrent layer processes the input sequence in both forward and backward direction. The decoder is a uni-directional LSTM. An attention mechanism is used to obtain the context vector.",We thank the three anonymous reviewers for their helpful feedback.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What dataset is used?,Sample Answer,1804.07789-Fused Bifocal Attention Mechanism-0,1804.07789-Gated Orthogonalization for Modeling Stay-On and Never Look Back behaviour-1,1804.07789-Gated Orthogonalization for Modeling Stay-On and Never Look Back behaviour-2,1804.07789-Gated Orthogonalization for Modeling Stay-On and Never Look Back behaviour-4,1804.07789-2-Figure1-1.png,"Intuitively, when a human writes a description from a table she keeps track of information at two levels. At the macro level, it is important to decide which is the appropriate field to attend to next and at a micro level (i.e., within a field) it is important to know which values to attend to next. To capture this behavior, we use a bifocal attention mechanism as described below.","where INLINEFORM0 are parameters to be learned. The job of the forget gate is to ensure that INLINEFORM1 is similar to INLINEFORM2 when required (i.e., by learning INLINEFORM3 when we want to continue focusing on the same field) and different when it is time to move on (by learning that INLINEFORM4 ).","Next, the never look back property implies that once we have moved away from a field we are unlikely to pay attention to it again. For example, once we have rendered all the occupations in the generated description there is no need to return back to the occupation field. In other words, once we have moved on ( INLINEFORM0 ), we want the successive field context vectors INLINEFORM1 to be very different from the previous field vectors INLINEFORM2 . One way of ensuring this is to orthogonalize successive field vectors using DISPLAYFORM0 ","In summary, Equation provides a mechanism for remembering the current field vector when appropriate (thus capturing stay-on behavior) using a remember gate. On the other hand, Equation EQREF10 explicitly ensures that the field vector is very different (soft-orthogonal) from the previous field vectors once it is time to move on (thus capturing never look back behavior). The value of INLINEFORM0 computed in Equation EQREF10 is then used in Equation . The INLINEFORM1 (macro) thus obtained is then concatenated with INLINEFORM2 (micro) and fed to the decoder (see Fig. FIGREF3 )","Figure 1: Sample Infobox with description : V. Balakrishnan (born 1943 as Venkataraman Balakrishnan) is an Indian theoretical physicist who has worked in a number of fields of areas, including particle physics, many-body theory, the mechanical behavior of solids, dynamical systems, stochastic processes, and quantum dynamics.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How much better is performance of proposed method than state-of-the-art methods in experiments?,Sample Answer,1910.03891-Proposed Model ::: Attribute Embedding Layer-0,1910.03891-Proposed Model ::: Attribute Embedding Layer-1,1910.03891-Proposed Model ::: Attribute Embedding Layer-3,1910.03891-Proposed Model ::: Attribute Embedding Layer-4,1910.03891-Proposed Model ::: Embedding Propagation Layer-11,"The value in attribute triples usually is sentence or a word. To encode the representation of value from its sentence or word, we need to encode the variable-length sentences to a fixed-length vector. In this study, we adopt two different encoders to model the attribute value.","Bag-of-Words Encoder. The representation of attribute value can be generated by a summation of all words embeddings of values. We denote the attribute value $a$ as a word sequence $a = w_{1},...,w_{n}$, where $w_{i}$ is the word at position $i$. The embedding of $\textbf {a}$ can be defined as follows.","Bag-of-Words Encoder is a simple and intuitive method, which can capture the relative importance of words. But this method suffers in that two strings that contains the same words with different order will have the same representation.","LSTM Encoder. In order to overcome the limitation of Bag-of-Word encoder, we consider using LSTM networks to encoder a sequence of words in attribute value into a single vector. The final hidden state of the LSTM networks is selected as a representation of the attribute value.",Averaging Aggregator sums all embeddings of multi-head graph attention and the output embedding in the final is calculated applying averaging:,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is typical GAN architecture for each text-to-image synhesis group?,Sample Answer,1910.09399-Text-to-Image Synthesis Taxonomy and Categorization ::: Resolution Enhancement GANs-0,1910.09399-Text-to-Image Synthesis Taxonomy and Categorization ::: Resolution Enhancement GANs ::: HDGAN-0,1910.09399-Text-to-Image Synthesis Taxonomy and Categorization ::: Diversity Enhancement GANs-0,1910.09399-Text-to-Image Synthesis Taxonomy and Categorization ::: Motion Enhancement GANs ::: ObamaNet and T2S-0,"1910.09399-GAN Based Text-to-Image Synthesis Applications, Benchmark, and Evaluation and Comparisons ::: Text-to-image Synthesis Benchmark Datasets-2","Due to the fact that training GANs will be much difficult when generating high-resolution images, a two stage GAN (i.e. stackGAN) is proposed in which rough images(i.e. low-resolution images) are generated in stage-I and refined in stage-II. To further improve the quality of generated images, the second version of StackGAN (i.e. Stack++) is proposed to use multi-stage GANs to generate multi-scale images. A color-consistency regularization term is also added into the loss to keep the consistency of images in different scales.","Hierarchically-nested adversarial network (HDGAN) is a method proposed by BIBREF36, and its main objective is to tackle the difficult problem of dealing with photographic images from semantic text descriptions. These semantic text descriptions are applied on images from diverse datasets. This method introduces adversarial objectives nested inside hierarchically oriented networks BIBREF36. Hierarchical networks helps regularize mid-level manifestations. In addition to regularize mid-level manifestations, it assists the training of the generator in order to capture highly complex still media elements. These elements are captured in statistical order to train the generator based on settings extracted directly from the image. The latter is an ideal scenario. However, this paper aims to incorporate a single-stream architecture. This single-stream architecture functions as the generator that will form an optimum adaptability towards the jointed discriminators. Once jointed discriminators are setup in an optimum manner, the single-stream architecture will then advance generated images to achieve a much higher resolution BIBREF36.","In this subsection, we introduce text-to-image synthesis methods which try to maximize the diversity of the output images, based on the text descriptions.","One early/interesting work of motion enhancement GANs is to generate spoofed speech and lip-sync videos (or talking face) of Barack Obama (i.e. ObamaNet) based on text input BIBREF62. This framework is consisted of three parts, i.e. text to speech using “Char2Wav”, mouth shape representation synced to the audio using a time-delayed LSTM and “video generation” conditioned on the mouth shape using “U-Net” architecture. Although the results seem promising, ObamaNet only models the mouth region and the videos are not generated from noise which can be regarded as video prediction other than video generation.","While most deep learning algorithms use MNIST BIBREF74 dataset as the benchmark, there are three main datasets that are commonly used for evaluation of proposed GAN models for text-to-image synthesis: CUB BIBREF75, Oxford BIBREF76, COCO BIBREF77, and CIFAR-10 BIBREF78. CUB BIBREF75 contains 200 birds with matching text descriptions and Oxford BIBREF76 contains 102 categories of flowers with 40-258 images each and matching text descriptions. These datasets contain individual objects, with the text description corresponding to that object, making them relatively simple. COCO BIBREF77 is much more complex, containing 328k images with 91 different object types. CIFAI-10 BIBREF78 dataset consists of 60000 32$times$32 colour images in 10 classes, with 6000 images per class. In contrast to CUB and Oxford, whose images each contain an individual object, COCO’s images may contain multiple objects, each with a label, so there are many labels per image. The total number of labels over the 328k images is 2.5 million BIBREF77.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what state of the art methods are compared to?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
what is the original model they refer to?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
how are sentences selected prior to making the summary?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are the other two Vietnamese datasets?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What methods are used to build two other Viatnamese datsets?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What deep neural network models are used in evaluation?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How authors evaluate datasets using models trained on different datasets?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
what is the size of the real-world civil case dataset?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How do they verify generalization ability?,Sample Answer,1811.08603-Preliminaries and Framework-1,1811.08603-Global Features-1,1811.08603-Global Features-6,1811.08603-Conclusion-1,1811.08603-4-Figure2-1.png,"Formally, we define the entity linking problem as follows: Given a set of mentions INLINEFORM0 in a document INLINEFORM1 , and an entity graph INLINEFORM2 , the goal is to find an assignment INLINEFORM3 .","Formally, we define neighbor mentions as INLINEFORM0 adjacent mentions before and after current mention INLINEFORM1 : INLINEFORM2 , where INLINEFORM3 is the pre-defined window size. Thus, the topical coherence at document level shall be achieved in a chain-like way. As shown in Figure FIGREF10 ( INLINEFORM4 ), mentions Hussain and Essex, a cricket player and the cricket club, provide adequate disambiguation clues to induce the underlying topic “cricket"" for the current mention England, which impacts positively on identifying the mention surrey as another cricket club via the common neighbor mention Essex.","Formally, we define the subgraph as: INLINEFORM0 , where INLINEFORM1 . For example (Figure FIGREF1 ), for entity England cricket team, the subgraph contains the relation from it to all candidates of neighbor mentions: England cricket team, Nasser Hussain (rugby union), Nasser Hussain, Essex, Essex County Cricket Club and Essex, New York. To support batch-wise acceleration, we represent INLINEFORM2 in the form of adjacency table based vectors: INLINEFORM3 , where INLINEFORM4 is the number of candidates per mention.","In the future, we will extend our method into cross-lingual settings to help link entities in low-resourced languages by exploiting rich knowledge from high-resourced languages, and deal with NIL entities to facilitate specific applications.","Figure 2: Framework of NCEL. The inputs of a set of mentions in a document are listed in the left side. The words in red indicate the current mention mi, where mi−1,mi+1 are neighbor mentions, and Φ(mi) = {ei1, ei2, ei3} denotes the candidate entity set for mi.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Do they only use adjacent entity mentions or use more than that in some cases (next to adjacent)?,Sample Answer,1811.08603-Introduction-5,1811.08603-Model Architecture-6,1811.08603-Results on GERBIL-1,1811.08603-Results on TAC2010 and WW-1,1811.08603-Results on TAC2010 and WW-3,"The second drawback of the global approach has been alleviated through approximate optimization techniques, such as PageRank/random walks BIBREF21 , graph pruning BIBREF22 , ranking SVMs BIBREF23 , or loopy belief propagation (LBP) BIBREF18 , BIBREF24 . However, these methods are not differentiable and thus difficult to be integrated into neural network models (the solution for the first limitation).","Sub-Graph Convolution Network Similar to GCN, this module learns to abstract features from the hidden state of the mention itself as well as its neighbors. Suppose INLINEFORM0 is the hidden states of the neighbor INLINEFORM1 , we stack them to expand the current hidden states of INLINEFORM2 as INLINEFORM3 , such that each row corresponds to that in the subgraph adjacent matrix INLINEFORM4 . We define sub-graph convolution as: INLINEFORM5 ","As shown in Table TABREF26 , NCEL achieves the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1. The baseline methods also achieve competitive results on some datasets but fail to adapt to the others. For example, AIDA and xLisa perform quite well on ACE2004 but poorly on other datasets, or WAT, PBoH, and WNED have a favorable performance on CoNLL but lower values on ACE2004 and AQUAINT. Our proposed method performs consistently well on all datasets that demonstrates the good generalization ability.","The results are shown in Table FIGREF28 and Table FIGREF28 . We can see the average linking precision (Micro) of WW is lower than that of TAC2010, and NCEL outperforms all baseline methods in both easy and hard cases. In the “easy"" case, local models have similar performance with global models since only little global information is available (2 mentions per document). Besides, NN-based models, NTEE and NCEL-local, perform significantly better than others including most global models, demonstrating that the effectiveness of neural models deals with the first limitation in the introduction.","As shown in Figure FIGREF28 , the prior probability performs quite well in TAC2010 but poorly in WW. Compared with NCEL-local, the global module in NCEL brings more improvements in the “hard"" case than that for “easy"" dataset, because local features are discriminative enough in most cases of TAC2010, and global information becomes quite helpful when local features cannot handle. That is, our propose collective model is robust and shows a good generalization ability to difficult EL. The improvements by each main module are relatively small in TAC2010, while the modules of attention and embedding features show non-negligible impacts in WW (even worse than local model), mainly because WW contains much noise, and these two modules are effective in improving the robustness to noise and the ability of generalization by selecting informative words and providing more accurate semantics, respectively.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the baselines?,Sample Answer,1911.08962-Introduction-0,1911.08962-Introduction-2,1911.08962-Introduction-3,1911.08962-Overview of Dataset ::: Task Definition-0,1911.08962-Overview of Dataset ::: Dataset Construction and Details-9,"Similar Case Matching (SCM) plays a major role in legal system, especially in common law legal system. The most similar cases in the past determine the judgment results of cases in common law systems. As a result, legal professionals often spend much time finding and judging similar cases to prove fairness in judgment. As automatically finding similar cases can benefit to the legal system, we select SCM as one of the tasks of CAIL2019.","More specifically, CAIL2019-SCM contains 8,964 triplets of legal documents. Every legal documents is collected from China Judgments Online. In order to ensure the similarity of the cases in one triplet, all selected documents are related to Private Lending. Every document in the triplet contains the fact description. CAIL2019-SCM requires researchers to decide which two cases are more similar in a triplet. By detecting similar cases in triplets, we can apply this algorithm for ranking all documents to find the most similar document in the database. There are 247 teams who have participated CAIL2019-SCM, and the best team has reached a score of $71.88$, which is about 20 points higher than the baseline. The results show that the existing methods have made great progress on this task, but there is still much room for improvement.","In other words, CAIL2019-SCM can benefit the research of legal case matching. Furthermore, there are several main challenges of CAIL2019-SCM: (1) The difference between documents may be small, and then it is hard to decide which two documents are more similar. Moreover, the similarity is defined by legal workers. We must utilize legal knowledge into this task rather than calculate similarity on the lexical level. (2) The length of the documents is quite long. Most documents contain more than 512 characters, and then it is hard for existing methods to capture document level information.","We first define the task of CAIL2019-SCM here. The input of CAIL2019-SCM is a triplet $(A,B,C)$, where $A,B,C$ are fact descriptions of three cases. Here we define a function $sim$ which is used for measuring the similarity between two cases. Then the task of CAIL2019-SCM is to predict whether $sim(A,B)>sim(A,C)$ or $sim(A,C)>sim(A,B)$.","Loan agreement, including loan contract, or borrowing, “WeChat, SMS, phone or other chat records”, receipt, irrigation, repayment commitment, guarantee, unknown or fuzzy and others.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Is this library implemented into Torch or is framework agnostic?,Sample Answer,2002.00876-Introduction-2,2002.00876-Introduction-3,2002.00876-Introduction-8,2002.00876-Technical Approach ::: Conditional Random Fields-1,2002.00876-Technical Approach ::: Conditional Random Fields-5,"In recent years, research into deep structured prediction has studied how these approaches can be integrated with neural networks and pretrained models. One line of work has utilized structured prediction as the final final layer for deep models BIBREF13, BIBREF14. Another has incorporated structured prediction within deep learning models, exploring novel models for latent-structure learning, unsupervised learning, or model control BIBREF15, BIBREF16, BIBREF17. We aspire to make both of these use-cases as easy to use as standard neural networks.","The practical challenge of employing structured prediction is that many required algorithms are difficult to implement efficiently and correctly. Most projects reimplement custom versions of standard algorithms or focus particularly on a single well-defined model class. This research style makes it difficult to combine and try out new approaches, a problem that has compounded with the complexity of research in deep structured prediction.","In this system description, we first motivate the approach taken by the library, then present a technical description of the methods used, and finally present several example use cases.","Define the log-partition as $A(\ell ) = \mathrm {LSE}(\ell )$, i.e. log of the denominator, where $\mathrm {LSE}$ is the log-sum-exp operator. Computing probabilities or sampling from this distribution, requires enumerating $\cal Z$ to compute the log-partition $A$. A useful identity is that derivatives of $A$ yield category probabilities,","Derivatives of the log-partition again provide distributional properties. For instance, the marginal probabilities of parts are given by,",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How better are results compared to baseline models?,Sample Answer,2004.03034-Introduction-2,2004.03034-Dataset-0,2004.03034-Dataset-1,2004.03034-Dataset-2,2004.03034-Dataset-4,"Very recent work, on the other hand, shows that attributes of both the audience and the communicator constitute important cues for determining argument strength BIBREF12, BIBREF13. They further show that audience and communicator attributes can influence the relative importance of linguistic features for predicting the persuasiveness of an argument. These results confirm previous findings in the social sciences that show a person's perception of an argument can be influenced by his background and personality traits.","Claims and impact votes. We collected 47,219 claims from kialo.com for 741 controversial topics and their corresponding impact votes. Impact votes are provided by the users of the platform to evaluate how impactful a particular claim is. Users can pick one of 5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact. While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument. An interesting observation is that, in this dataset, the same claim can have different impact labels depending on the context in which it is presented.","Figure FIGREF1 shows a partial argument tree for the argument thesis “Physical torture of prisoners is an acceptable interrogation tool.”. Each node in the argument tree corresponds to a claim, and these argument trees are constructed and edited collaboratively by the users of the platform.","Except the thesis, every claim in the argument tree either opposes or supports its parent claim. Each path from the root to leaf nodes corresponds to an argument path which represents a particular line of reasoning on the given controversial topic.","Distribution of impact votes. The distribution of claims with the given range of number of impact votes are shown in Table TABREF5. There are 19,512 claims in total with 3 or more votes. Out of the claims with 3 or more votes, majority of them have 5 or more votes. We limit our study to the claims with at least 5 votes to have a more reliable assignment for the accumulated impact label for each claim.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
how many domains did they experiment with?,Sample Answer,1907.00854-Introduction-1,1907.00854-Question Identification-1,1907.00854-Reading Comprehension-0,1907.00854-Architecture and Configuration-0,1907.00854-Architecture and Configuration-1,"Developers could support question answering using publicly available chatbot platforms, such as Watson Assistant or DialogFlow. To do this, a user would need to program an intent for each anticipated question with various examples of the question and one or more curated responses. This approach has the advantage of generating high quality answers, but it is limited to those questions anticipated by developers. Moreover, the management burden of such a system might be prohibitive as the number of questions that needs to be supported is likely to increase over time.","The question identifier uses a rule-based approach to question identification. As suggested in BIBREF3 , we utilize the presence of question marks and 5W1H words to determine if the input is a question. Based on our testing, this provides quite high performance (90%+ accuracy) and is not a blocker to overall performance.","The final module of the Katecheo system is the reading comprehension (or just “comprehension"") module. This module takes as input the original input question plus the matched knowledge base article body text and uses a reading comprehension model to select an appropriate answer from within the article.","All four of the Katecheo modules are containerized with Docker BIBREF10 and are deployed as pods on top of Kubernetes BIBREF11 (see Figure FIGREF12 ). In this way, Katecheo is completely portable to any standard Kubernetes cluster including hosted versions in AWS, GCP, Digital Ocean, Azure, etc. and on-premises version that use vanilla Kubernetes, OpenShift, CaaS, etc.","To provide developers with a familiar interface to the question answering system, we provide a REST API interface. Developers can call Katecheo via a single endpoint with ingress to the system provided by Ambassador, a Kubernetes-native API Gateway.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What was their F1 score on the Bengali NER corpus?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
what was their system's f1 performance?,Sample Answer,1809.10644-Introduction-1,1809.10644-Introduction-2,1809.10644-Data-1,1809.10644-Error Analysis-11,1809.10644-Preprocessing-1,"Despite the lack of consensus around what constitutes abusive speech, some definition of hate speech must be used to build automated systems to address it. We rely on BIBREF4 's definition of hate speech, specifically: “language that is used to express hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group.”","In this paper, we present a neural classification system that uses minimal preprocessing to take advantage of a modified Simple Word Embeddings-based Model BIBREF5 to predict the occurrence of hate speech. Our classifier features:","Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets. Tweets were labeled as “Harrassing” or “Non-Harrassing”; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader “Harrassing” category BIBREF9 .","RT @GrantLeeStone: @MT8_9 I don't even know what that is, or where it's from. Was that supposed to be funny? It wasn't.","RT @AGuyNamed_Nick Now, I'm not sexist in any way shape or form but I think women are better at gift wrapping. It's the XX chromosome thing",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
how was the speech collected?,Sample Answer,1912.03627-Experiments and Results ::: Speaker Verification Experiments-5,1912.03627-Experiments and Results ::: Speaker Verification Experiments-6,1912.03627-Experiments and Results ::: Speaker Verification Experiments-9,1912.03627-Experiments and Results ::: Speaker Verification Experiments-10,1912.03627-Experiments and Results ::: Speaker Verification Experiments-11,"One interesting advantage of the DeepMine database compared to both RSR2015 and RedDots is having several target speakers with more than one mobile device. This is allows us to analyse the effects of channel compensation methods. The second row in Table TABREF23 corresponds to the most difficult trials where the target trials come from mobile devices with different models while imposter trials come from the same device models. It is clear that severe degradation was caused by this kind of channel effects (i.e. decreasing within-speaker similarities while increasing between-speaker similarities), especially for females.",The results in the third row show the condition when target speakers at the test time use exactly the same device that was used for enrollment. Comparing this row with the results in the first row proves how much improvement can be achieved when exactly the same device is used by the target speaker.,"The degraded female results in the sixth row as compared to the third row show the effect of using a different device model from the same brand for target trials. For males, the filters brings almost the same subsets of trials, which explains the very similar results in this case.","Looking at the first two and the last row of Table TABREF23, one can notice the significantly worse performance obtained for the female trials as compared to males. Note that these three rows include target trials where the devices used for enrollment do not necessarily match the devices used for recording test utterances. On the other hand, in rows 3 to 6, which exclude such mismatched trials, the performance for males and females is comparable. This suggest that the degraded results for females are caused by some problematic trials with device mismatch. The exact reason for this degradation is so far unclear and needs a further investigation.","In the last row of the table, the condition of the second row is relaxed: the target device should have different model possibly from the same brand and imposter device only needs to be from the same brand. In this case, as was expected, the performance degradation is smaller than in the second row.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the competing models?,Sample Answer,1910.11235-Introduction-0,1910.11235-Related Works-1,1910.11235-Model Description-10,1910.11235-Experiment ::: Datasets-1,1910.11235-4-Table3-1.png,"Likelihood-based language models with deep neural networks have been widely adopted to tackle language tasks BIBREF0, BIBREF1, BIBREF2, BIBREF3. By far, one of the most popular training strategies is teacher forcing, which derives from the general maximum likelihood estimation (MLE) principle BIBREF4. Under the teacher forcing schema, a model is trained to make predictions conditioned on ground-truth inputs. Although this strategy enables effective training of large neural networks, it is susceptible to aggravate exposure bias: a model may perform poorly at the inference stage, once its self-generated prefix diverges from the previously learned ground-truth data BIBREF5.","In recent RL-inspired works, BIBREF10 built on the REINFORCE algorithm to directly optimize the test-time evaluation metric score. BIBREF11 employed a similar approach by training a critic network to predict the metric score that the actor's generated sequence of tokens would obtain. In both cases, the reliance on a metric to accurately reflect the quality of generated samples becomes a major limitation. Such metrics are often unavailable and difficult to design by nature.","Multi-Entropy Sampling: Language GANs can be seen an online RL methods, where the actor is updated from data generated by its own policy with strong correlation. Inspired by BIBREF20, we empirically find that altering the entropy of the actor's sample distribution during training is beneficial to the AC network's robust performance. In specific, we alternate the temperature $\tau $ to generate samples under different behavior policies. During the critic's training, the ground-truth sequences are assigned a perfect target value of 1. The samples obtained with $\tau < 1$ are supposed to contain lower entropy and to diverge less from the real data, that they receive a higher target value close to 1. Those obtained with $\tau > 1$ contain higher entropy and more errors that their target values are lower and closer to 0. This mechanism decorrelates updates during sequential sampling by sampling multiple diverse entropy distributions from actor synchronously.","EMNLP2017 WMT News is provided in BIBREF21, a benchmarking platform for text generation models. We split the entire dataset into a training set of 195,010 sentences, a validation set of 83,576 sentences, and a test set of 10,000 sentences. The vocabulary size is 5,254 and the average sentence length is 27.","Table 3: Results on the Google-small dataset. The 95 % confidence intervals from multiple trials are reported. † This dataset was not tested in (Guo et al., 2017) and we are unable to train LeakGAN on this dataset using the official code due to its training complexity (taking 10+ hours per epoch).",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How is pseudo-perplexity defined?,Sample Answer,2003.11562-Introduction-2,2003.11562-Introduction-4,2003.11562-Background & Methods-6,2003.11562-Experiments & Results ::: BERT-3,2003.11562-3-TableIII-1.png,"Another architecture of interest would be the Transformer-XL, which introduces the notion of recurrence in a self-attention model.","In this project, we explore the implementation of Transformer-based models (BERT and Transformer-XL) in language modeling for Finnish. We will use the same training data as in BIBREF8 so that we can do fair comparisons with the performance of the LSTM models. Also, as the BERT model is a bi-directional transformer, we will have to approximate the conditional probabilities given a sequence of words. We also experiment with using sub-word units with Transformer-XL to cope with the large vocabulary problems associated with the Finnish Language. With smaller units, the modeled sequences are longer, and we hope that the recursive XL architecture can allow us to still model long term effects. To the best of our knowledge this is the first work with the Finnish language to use the following:","The Next Sentence Prediction (NSP) is a binary classification task which predicts whether two segments follow each other in the original text. This pre-training task was proposed to further improve the performance on downstreaming tasks, like Natural Language Inference(NLI) but in reality removing the NSP loss matches or slightly improves the downstream task performance BIBREF12. In this paper, we have omitted the NSP task from the BERT pre-training procedure and changed the input from a SEGMENT-PAIR input to a SINGLE SEGMENT input. As seen in (Fig FIGREF8)","The results of the pseudo-perplexity described in the previous section to evaluate the above models on the test data-set is in table (Table TABREF12).The test dataset is of a different context when compared to the training data, and interestingly both the models are quite confident when it comes to the test dataset. The pseudo-perplexity values of left-marked are lower when compared to left-right-marked signifying that it is more confident.",TABLE III BERT TRAINING PERFORMANCE,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is a wizard of oz setup?,Sample Answer,1912.08904-Introduction-0,1912.08904-Retrieval and Question Answering in Macaw-0,1912.08904-User Interfaces-2,1912.08904-Contribution-0,1912.08904-Contribution-1,"The rapid growth in speech and small screen interfaces, particularly on mobile devices, has significantly influenced the way users interact with intelligent systems to satisfy their information needs. The growing interest in personal digital assistants, such as Amazon Alexa, Apple Siri, Google Assistant, and Microsoft Cortana, demonstrates the willingness of users to employ conversational interactions BIBREF0. As a result, conversational information seeking (CIS) has been recognized as a major emerging research area in the Third Strategic Workshop on Information Retrieval (SWIRL 2018) BIBREF1.",The overview of retrieval and question answering actions in Macaw is shown in FIGREF17. These actions consist of the following components:,"File IO: This interface is designed for experimental purposes, such as evaluating the performance of a conversational search technique on a dataset with multiple queries. This is not an interactive interface.","Macaw is distributed under the MIT License. We welcome contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com. This project has adopted the Microsoft Open Source Code of Conduct.","When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How is the clinical text structuring task defined?,Sample Answer,1908.06606-The Proposed Model for QA-CTS Task ::: Integration Method-1,1908.06606-The Proposed Model for QA-CTS Task ::: Integration Method-2,1908.06606-The Proposed Model for QA-CTS Task ::: Final Prediction-1,1908.06606-The Proposed Model for QA-CTS Task ::: Two-Stage Training Mechanism-0,1908.06606-Experimental Studies ::: Comparisons Between Two Integration Methods-2,"While for the transformation method, we use multi-head attention BIBREF30 to encode the two vectors. It can be defined as follows where $h$ is the number of heads and $W_o$ is used to projects back the dimension of concatenated matrix.",$Attention$ denotes the traditional attention and it can be defined as follows.,Then we permute the two dimensions for softmax calculation. The calculation process of loss function can be defined as followed.,"Two-stage training mechanism is previously applied on bilinear model in fine-grained visual recognition BIBREF31, BIBREF32, BIBREF33. Two CNNs are deployed in the model. One is trained at first for coarse-graind features while freezing the parameter of the other. Then unfreeze the other one and train the entire model in a low learning rate for fetching fine-grained features.","Although Table TABREF27 shows the best integration method is concatenation, multi-head attention still has great potential. Due to the lack of computational resources, our experiment fixed the head number and hidden vector size. However, tuning these hyper parameters may have impact on the result. Tuning integration method and try to utilize larger datasets may give help to improving the performance.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
"Is all text in this dataset a question, or are there unrelated sentences in between questions?",Sample Answer,1908.06606-Introduction-0,1908.06606-Introduction-2,1908.06606-Related Work ::: Pre-trained Language Model-0,1908.06606-The Proposed Model for QA-CTS Task ::: Two-Stage Training Mechanism-0,1908.06606-Experimental Studies ::: Dataset and Evaluation Metrics-1,"Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches.","Traditionally, CTS tasks can be addressed by rule and dictionary based methods BIBREF0, BIBREF1, BIBREF2, task-specific end-to-end methods BIBREF3, BIBREF4, BIBREF5, BIBREF6 and pipeline methods BIBREF7, BIBREF8, BIBREF9. Rule and dictionary based methods suffer from costly human-designed extraction rules, while task-specific end-to-end methods have non-uniform output formats and require task-specific training dataset. Pipeline methods break down the entire process into several pieces which improves the performance and generality. However, when the pipeline depth grows, error propagation will have a greater impact on the performance.","Recently, some works focused on pre-trained language representation models to capture language information from text and then utilizing the information to improve the performance of specific natural language processing tasks BIBREF24, BIBREF25, BIBREF26, BIBREF27 which makes language model a shared model to all natural language processing tasks. Radford et al. BIBREF24 proposed a framework for fine-tuning pre-trained language model. Peters et al. BIBREF25 proposed ELMo which concatenates forward and backward language models in a shallow manner. Devlin et al. BIBREF26 used bidirectional Transformers to model deep interactions between the two directions. Yang et al. BIBREF27 replaced the fixed forward or backward factorization order with all possible permutations of the factorization order and avoided using the [MASK] tag which causes pretrain-finetune discrepancy that BERT is subject to.","Two-stage training mechanism is previously applied on bilinear model in fine-grained visual recognition BIBREF31, BIBREF32, BIBREF33. Two CNNs are deployed in the model. One is trained at first for coarse-graind features while freezing the parameter of the other. Then unfreeze the other one and train the entire model in a low learning rate for fetching fine-grained features.","In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage of predictions that match any one of the ground truth answers exactly. The F$_1$-score metric is a looser metric measures the average overlap between the prediction and ground truth answer.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How many questions are in the dataset?,Sample Answer,1908.06606-Introduction-0,1908.06606-Introduction-2,1908.06606-Related Work ::: Clinical Text Structuring-0,1908.06606-Question Answering based Clinical Text Structuring-2,1908.06606-The Proposed Model for QA-CTS Task ::: Two-Stage Training Mechanism-0,"Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches.","Traditionally, CTS tasks can be addressed by rule and dictionary based methods BIBREF0, BIBREF1, BIBREF2, task-specific end-to-end methods BIBREF3, BIBREF4, BIBREF5, BIBREF6 and pipeline methods BIBREF7, BIBREF8, BIBREF9. Rule and dictionary based methods suffer from costly human-designed extraction rules, while task-specific end-to-end methods have non-uniform output formats and require task-specific training dataset. Pipeline methods break down the entire process into several pieces which improves the performance and generality. However, when the pipeline depth grows, error propagation will have a greater impact on the performance.","Clinical text structuring is a final problem which is highly related to practical applications. Most of existing studies are case-by-case. Few of them are developed for the general purpose structuring task. These studies can be roughly divided into three categories: rule and dictionary based methods, task-specific end-to-end methods and pipeline methods.","Since BERT BIBREF26 has already demonstrated the usefulness of shared model, we suppose extracting commonality of this problem and unifying the output format will make the model more powerful than dedicated model and meanwhile, for a specific clinical task, use the data for other tasks to supplement the training data.","Two-stage training mechanism is previously applied on bilinear model in fine-grained visual recognition BIBREF31, BIBREF32, BIBREF33. Two CNNs are deployed in the model. One is trained at first for coarse-graind features while freezing the parameter of the other. Then unfreeze the other one and train the entire model in a low learning rate for fetching fine-grained features.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What was the result of the highest performing system?,Sample Answer,1709.10217-Introduction-1,1709.10217-Task 1: User Intent Classification-0,1709.10217-Task 2: Online Testing of Task-oriented Dialogue-3,1709.10217-Task 2: Online Testing of Task-oriented Dialogue-10,1709.10217-1-Figure1-1.png,"Although the blooming of human-computer dialogue technology in both academia and industry, how to evaluate a dialogue system, especially an open domain chit-chat system, is still an open question. Figure FIGREF6 presents a brief comparison of the open domain chit-chat system and the task-oriented dialogue system.","In using of human-computer dialogue based applications, human may have various intent, for example, chit-chatting, asking questions, booking air tickets, inquiring weather, etc. Therefore, after receiving an input message (text or ASR result) from a user, the first step is to classify the user intent into a specific domain for further processing. Table TABREF7 shows an example of user intent with category information.","In task 2, there are three categories. They are “air tickets”, “train tickets” and “hotel”. Correspondingly, there are three type of tasks. All the tasks are in the scope of the three categories. However, a complete user intent may include more than one task. For example, a user may first inquiring the air tickets. However, due to the high price, the user decide to buy a train tickets. Furthermore, the user may also need to book a hotel room at the destination.","For the number of dialogue turns, we have a penalty rule that for a dialogue task, if the system cannot return the result (or accomplish the task) in 30 turns, the dialogue task is end by force. Meanwhile, if a system cannot accomplish a task in less than 30 dialogue turns, the number of dialogue turns is set to 30.",Figure 1: A brief comparison of the open domain chit-chat system and the task-oriented dialogue system.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the size of the dataset?,Sample Answer,1809.03695-Introduction-1,1809.03695-Introduction-2,1809.03695-Introduction-3,1809.03695-The vSTS dataset-2,1809.03695-Experiments-1,"In another strand of related work, tasks that combine representations of multiple modalities have gained increasing attention, including image-caption retrieval, video and text alignment, caption generation, and visual question answering. A common approach is to learn image and text embeddings that share the same space so that sentence vectors are close to the representation of the images they describe BIBREF3 , BIBREF4 . BIBREF5 provides an approach that learns to align images with descriptions. Joint spaces are typically learned combining various types of deep learning networks such us recurrent networks or convolutional networks, with some attention mechanism BIBREF6 , BIBREF7 , BIBREF8 .","The complementarity of visual and text representations for improved language understanding have been shown also on word representations, where embeddings have been combined with visual or perceptual input to produce grounded representations of words BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 . These improved representation models have outperformed traditional text-only distributional models on a series of word similarity tasks, showing that visual information coming from images is complementary to textual information.","In this paper we present Visual Semantic Textual Similarity (vSTS), a dataset which allows to study whether better sentence representations can be built when having access to corresponding images, e.g. a caption and its image, in contrast with having access to the text alone. This dataset is based on a subset of the STS benchmark BIBREF1 , more specifically, the so called STS-images subset, which contains pairs of captions. Note that the annotations are based on the textual information alone. vSTS extends the existing subset with images, and aims at being a standard dataset to test the contribution of visual information when evaluating sentence representations.","Subset 2014 This subset is derived from the Image Descriptions dataset which is a subset of the PASCAL VOC-2008 dataset BIBREF16 . PASCAL VOC-2008 dataset consists of 1,000 images and has been used by a number of image description systems. In total, we obtained 374 pairs (out of 750 in the original file).","STS Models We checked four models of different complexity and modalities. The baseline is a word overlap model (overlap), in which input texts are tokenized with white space, vectorized according to a word index, and similarity is computed as the cosine of the vectors. We also calculated the centroid of Glove word embeddings BIBREF17 (caverage) and then computed the cosine as a second text-based model. The third text-based model is the state of the art Decomposable Attention Model BIBREF18 (dam), trained on the STS benchmark dataset as explained above. Finally, we use the top layer of a pretrained resnet50 model BIBREF19 to represent the images associated to text, and use the cosine for computing the similarity of a pair of images (resnet50).",1.0,1.0,1.0,1.0,1.0,0.25,0.3333333333333333,0.2
What discourse relations does it work best/worst for?,Sample Answer,1804.05918-Parameter Settings and Model Training-1,1804.05918-Parameter Settings and Model Training-2,1804.05918-Evaluation Settings-1,1804.05918-Experimental Results-4,1804.05918-Ensemble Model-0,"We chose the standard cross-entropy loss function for training our neural network model and adopted Adam BIBREF38 optimizer with the initial learning rate of 5e-4 and a mini-batch size of 128. If one instance is annotated with two labels (4% of all instances), we use both of them in loss calculation and regard the prediction as correct if model predicts one of the annotated labels. All the proposed models were implemented with Pytorch and converged to the best performance within 20-40 epochs.","To alleviate the influence of randomness in neural network model training and obtain stable experimental results, we ran each of the proposed models and our own baseline models ten times and report the average performance of each model instead of the best performance as reported in many previous works.","Since none of the recent previous work reported class-wise implicit relation classification performance in the multi-way classification setting, for better comparisons, we re-implemented the neural tensor network architecture (so-called SWIM in BIBREF18 ) which is essentially a Bi-LSTM model with tensors and report its detailed evaluation result in the multi-way classification setting. As another baseline, we report the performance of a Bi-LSTM model without tensors as well. Both baseline models take two relevant discourse units as the only input.","Binary Classification: From table 4 , we can see that compared against the best previous systems, our paragraph-level model with untied parameters in the prediction layer achieves F1-score improvements of 6 points on Comparison and 7 points on Temporal, which demonstrates that paragraph-wide contexts are important in detecting minority discourse relations. Note that the CRF layer of the model is not suitable for binary classification.","As we explained in section 4.2, we ran our models for 10 times to obtain stable average performance. Then we also created ensemble models by applying majority voting to combine results of ten runs. From table 5 , each ensemble model obtains performance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively. Furthermore, the ensemble model achieves the best performance for predicting both implicit and explicit discourse relations simultaneously.",1.0,1.0,1.0,1.0,1.0,0.1818181818181818,0.16666666666666666,0.2
what are the evaluation metrics?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
which datasets were used in evaluation?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
what are the baselines?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are the three measures of bias which are reduced in experiments?,Sample Answer,1910.14497-A Probabilistic Framework for Bias Mitigation ::: Nearest Neighbor Bias Mitigation-2,1910.14497-WEAT Word Sets-3,1910.14497-WEAT Word Sets-8,1910.14497-WEAT Word Sets-13,1910.14497-WEAT Word Sets-20,,"Y: ""ant"", ""caterpillar"", ""flea"", ""locust"", ""spider"", ""bedbug"", ""centipede"", ""fly"", ""maggot"", ""tarantula"", ""bee"", ""cockroach"", ""gnat"", ""mosquito"", ""termite"", ""beetle"", ""cricket"", ""hornet"", ""moth"", ""wasp"", ""blackfly"", ""dragonfly"", ""horsefly"", ""roach"", ""weevil""","Y: ""arrow"", ""club"", ""gun"", ""missile"", ""spear"", ""ax"", ""dagger"", ""harpoon"", ""pistol"", ""sword"", ""blade"", ""dynamite"", ""hatchet"", ""rifle"", ""tank"", ""bomb"", ""firearm"", ""knife"", ""shotgun"", ""teargas"", ""cannon"", ""grenade"", ""mace"", ""slingshot"", ""whip""","Y: ""sister"", ""mother"", ""aunt"", ""grandmother"", ""daughter"", ""she"", ""hers"", ""her"", ""woman"", ""herself"", ""women"", ""wife"", ""aunt"", ""niece"", ""girlfriend"", ""queen"", ""actress""","B: ""sister"", ""mother"", ""aunt"", ""grandmother"", ""daughter"", ""she"", ""hers"", ""her"", ""woman"", ""herself"", ""women"", ""wife"", ""aunt"", ""niece"", ""girlfriend"", ""queen"", ""actress""",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the latest paper covered by this survey?,Sample Answer,1905.08949-Introduction-0,1905.08949-Learning Paradigm-0,1905.08949-Encoding Answers-3,1905.08949-Technical Considerations-1,1905.08949-Conclusion – What's the Outlook?-1,"Question Generation (QG) concerns the task of “automatically generating questions from various inputs such as raw text, database, or semantic representation"" BIBREF0 . People have the ability to ask rich, creative, and revealing questions BIBREF1 ; e.g., asking Why did Gollum betray his master Frodo Baggins? after reading the fantasy novel The Lord of the Rings. How can machines be endowed with the ability to ask relevant and to-the-point questions, given various inputs? This is a challenging, complementary task to Question Answering (QA). Both QA and QG require an in-depth understanding of the input source and the ability to reason over relevant contexts. But beyond understanding, QG additionally integrates the challenges of Natural Language Generation (NLG), i.e., generating grammatically and semantically correct questions.","QG research traditionally considers two fundamental aspects in question asking: “What to ask” and “How to ask”. A typical QG task considers the identification of the important aspects to ask about (“what to ask”), and learning to realize such identified aspects as natural language (“how to ask”). Deciding what to ask is a form of machine understanding: a machine needs to capture important information dependent on the target application, akin to automatic summarization. Learning how to ask, however, focuses on aspects of the language quality such as grammatical correctness, semantically preciseness and language flexibility.","We forecast treating the passage and the target answer separately as a future trend, as it results in a more flexible model, which generalizes to the abstractive case when the answer is not a text span in the input passage. However, this inevitably increases the model complexity and difficulty in training.","1. Copying Mechanism. Most NQG models BIBREF48 , BIBREF60 , BIBREF61 , BIBREF50 , BIBREF62 employ the copying mechanism of BIBREF23 , which directly copies relevant words from the source sentence to the question during decoding. This idea is widely accepted as it is common to refer back to phrases and entities appearing in the text when formulating factoid questions, and difficult for a RNN decoder to generate such rare words on its own.","What's next for NGQ? We end with future potential directions by applying past insights to current NQG models; the “unknown unknown"", promising directions yet explored.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What learning paradigms do they cover in this survey?,Sample Answer,1905.08949-Introduction-0,1905.08949-Evaluation Metrics-1,1905.08949-Encoding Answers-0,1905.08949-Encoding Answers-2,1905.08949-Conclusion – What's the Outlook?-1,"Question Generation (QG) concerns the task of “automatically generating questions from various inputs such as raw text, database, or semantic representation"" BIBREF0 . People have the ability to ask rich, creative, and revealing questions BIBREF1 ; e.g., asking Why did Gollum betray his master Frodo Baggins? after reading the fantasy novel The Lord of the Rings. How can machines be endowed with the ability to ask relevant and to-the-point questions, given various inputs? This is a challenging, complementary task to Question Answering (QA). Both QA and QG require an in-depth understanding of the input source and the ability to reason over relevant contexts. But beyond understanding, QG additionally integrates the challenges of Natural Language Generation (NLG), i.e., generating grammatically and semantically correct questions.","As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE BIBREF43 , are also widely used. However, some studies BIBREF44 , BIBREF45 have shown that these metrics do not correlate well with fluency, adequacy, coherence, as they essentially compute the $n$ -gram similarity between the source sentence and the generated question. To overcome this, BIBREF46 proposed a new metric to evaluate the “answerability” of a question by calculating the scores for several question-specific factors, including question type, content words, function words, and named entities. However, as it is newly proposed, it has not been applied to evaluate any NQG system yet.","The most commonly considered factor by current NQG systems is the target answer, which is typically taken as an additional input to guide the model in deciding which information to focus on when generating; otherwise, the NQG model tend to generate questions without specific target (e.g., “What is mentioned?""). Models have solved this by either treating the answer's position as an extra input feature BIBREF48 , BIBREF51 , or by encoding the answer with a separate RNN BIBREF49 , BIBREF52 .","To generate answer-related questions, extra answer indicators explicitly emphasize the importance of answer; however, it also increases the tendency that generated questions include words from the answer, resulting in useless questions, as observed by BIBREF52 . For example, given the input “John Francis O’Hara was elected president of Notre Dame in 1934."", an improperly generated question would be “Who was elected John Francis?"", which exposes some words in the answer. To address this, they propose to replace the answer into a special token for passage encoding, and a separate RNN is used to encode the answer. The outputs from two encoders are concatenated as inputs to the decoder. BIBREF54 adopted a similar idea that separately encodes passage and answer, but they instead use the multi-perspective matching between two encodings as an extra input to the decoder.","What's next for NGQ? We end with future potential directions by applying past insights to current NQG models; the “unknown unknown"", promising directions yet explored.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are all the input modalities considered in prior work in question generation?,Sample Answer,1905.08949-Evaluation Metrics-0,1905.08949-Methodology-3,1905.08949-Encoding Answers-3,1905.08949-Technical Considerations-0,1905.08949-Conclusion – What's the Outlook?-1,"Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask. Meaningful, syntactically correct, semantically sound and natural are all useful criteria, yet they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks.","Although these NQG models all share the Seq2Seq framework, they differ in the consideration of — (1) QG-specific factors (e.g., answer encoding, question word generation, and paragraph-level contexts), and (2) common NLG techniques (e.g., copying mechanism, linguistic features, and reinforcement learning) — discussed next.","We forecast treating the passage and the target answer separately as a future trend, as it results in a more flexible model, which generalizes to the abstractive case when the answer is not a text span in the input passage. However, this inevitably increases the model complexity and difficulty in training.","Common techniques of NLG have also been considered in NQG model, summarized as 3 tactics:","What's next for NGQ? We end with future potential directions by applying past insights to current NQG models; the “unknown unknown"", promising directions yet explored.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How are EAC evaluated?,Sample Answer,1906.09774-Introduction-0,1906.09774-Introduction-1,1906.09774-Introduction-3,1906.09774-History of Emotionally-Aware Chatbot-0,1906.09774-5-Table2-1.png,"Conversational agents or dialogue systems development are gaining more attention from both industry and academia BIBREF0 , BIBREF1 in the latest years. Some works tried to model them into domain-specific tasks such as customer service BIBREF2 , BIBREF3 , and shopping assistance BIBREF4 . Other works design a multi-purpose agents such as SIRI, Amazon Alexa, and Google Assistance. This domain is a well-researched area in Human-Computer Interaction research community but still become a hot topic now. The main development focus right now is to have an intelligent and humanizing machine to have a better engagement when communicating with human BIBREF5 . Having a better engagement will lead to higher user satisfaction, which becomes the main objective from the industry perspective.","In this study, we will only focus on textual conversational agent or chatbot, a conversational artificial intelligence which can conduct a textual communication with a human by exploiting several natural language processing techniques. There are several approaches used to build a chatbot, start by using a simple rule-based approach BIBREF6 , BIBREF7 until more sophisticated one by using neural-based technique BIBREF8 , BIBREF9 . Nowadays, chatbots are mostly used as customer service such as booking systems BIBREF10 , BIBREF11 , shopping assistance BIBREF3 or just as conversational partner such as Endurance and Insomnobot . Therefore, there is a significant urgency to humanize chatbot for having a better user-engagement. Some works were already proposed several approaches to improve chatbot's user-engagement, such as building a context-aware chatbot BIBREF12 and injecting personality into the machine BIBREF13 . Other works also try to incorporate affective computing to build emotionally-aware chatbots BIBREF2 , BIBREF14 , BIBREF15 .","In this paper, we will try to summarize some previous studies which focus on injecting emotion information into chatbots, on discovering recent issues and barriers in building engaging emotionally-aware chatbots. Therefore, we propose some research questions to have a better problem definition:","The early development of chatbot was inspired by Turing test in 1950 BIBREF20 . Eliza was the first publicly known chatbot, built by using simple hand-crafted script BIBREF21 . Parry BIBREF22 was another chatbot which successfully passed the Turing test. Similar to Eliza, Parry still uses a rule-based approach but with a better understanding, including the mental model that can stimulate emotion. Therefore, Parry is the first chatbot which involving emotion in its development. Also, worth to be mentioned is ALICE (Artificial Linguistic Internet Computer Entity), a customizable chatbot by using Artificial Intelligence Markup Language (AIML). Therefore, ALICE still also use a rule-based approach by executing a pattern-matcher recursively to obtain the response. Then in May 2014, Microsoft introduced XiaoIce BIBREF23 , an empathetic social chatbot which is able to recognize users' emotional needs. XiaoIce can provide an engaging interpersonal communication by giving encouragement or other affective messages, so that can hold human attention during communication.",Table 2: Summarization of dataset available for emotionally-aware chatbot.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How did they evaluate the quality of annotations?,Sample Answer,1910.09387-Creation of Clotho dataset ::: Audio data collection and processing-2,1910.09387-Baseline method and evaluation-0,1910.09387-Baseline method and evaluation-1,1910.09387-Baseline method and evaluation-2,1910.09387-Conclusions-0,"We target at audio samples $\mathbf {x}$ having a uniform distribution between 15 and 30 s. Thus, we further process $\mathbb {X}_{\text{med}}$, keeping the files with a maximum duration of 30 s and cutting a segment from the rest. We randomly select a set of values for the duration of the segments that will maximize the entropy of the duration of the files, discretizing the durations with a resolution of 0.05 s. In order to not pick segment without activity, we sample the files by taking a window with a selected duration that maximizes the energy of the sample. Finally, we apply a 512-point Hamming window to the beginning and the end of the samples, smoothing the effect of sampling. The above process results to $\mathbb {X}_{\text{sam}}=\lbrace \mathbf {x}_{\text{sam}}^{z}\rbrace _{z=1}^{N_{\text{med}}}$, where the distribution of durations is approximately uniform between 15 and 30 s.","In order to provide an example of how to employ Clotho and some initial (baseline) results, we use a previously utilized method for audio captioning BIBREF3 which is based on an encoder-decoder scheme with attention. The method accepts as an input a length-$T$ sequence of 64 log mel-band energies $\mathbf {X}\in \mathbb {R}^{T\times 64}$, which is used as an input to a DNN which outputs a probability distribution of words. The generated caption is constructed from the output of the DNN, as in BIBREF3. We optimize the parameters of the method using the development split of Clotho and we evaluate it using the evaluation and the testing splits, separately.","We first extract 64 log mel-band energies, using a Hamming window of 46 ms, with 50% overlap. We tokenize the captions of the development split, using a one-hot encoding of the words. Since all the words in in the development split appear in the other two splits as well, there are no unknown tokens/words. We also employ the start- and end-of-sequence tokens ($\left<\text{SOS}\right>$ and $\left<\text{EOS}\right>$ respectively), in order to signify the start and end of a caption.","The encoder is a series of bi-directional gated recurrent units (bi-GRUs) BIBREF10, similarly to BIBREF3. The output dimensionality for the GRU layers (forward and backward GRUs have same dimensionality) is $\lbrace 256, 256, 256\rbrace $. The output of the encoder is processed by an attention mechanism and its output is given as an input to the decoder. The attention mechanism is a feed-forward neural network (FNN) and the decoder a GRU. Then, the output of the decoder is given as an input to another FNN with a softmax non-linearity, which acts as a classifier and outputs the probability distribution of words for the $i$-th time-step. To optimize the parameters of the employed method, we use five times each audio sample, using its five different captions as targeted outputs each time. We optimize jointly the parameters of the encoder, attention mechanism, decoder, and the classifier, using 150 epochs, the cross entropy loss, and Adam optimizer BIBREF11 with proposed hyper-parameters. Also, in each batch we pad the captions of the batch to the longest in the same batch, using the end-of-sequence token, and the input audio features to the longest ones, by prepending zeros.","In this work we present a novel dataset for audio captioning, named Clotho, that contains 4981 audio samples and five captions for each file (totaling to 24 905 captions). During the creating of Clotho care has been taken in order to promote diversity of captions, eliminate words that appear only once and named entities, and provide data splits that do not hamper the training or evaluation process. Also, there is an example of the usage of Clotho, using a method proposed at the original work of audio captioning. The baseline results indicate that the baseline method started learning the content of the input audio, but more tuning is needed in order to express the content properly. Future work includes the employment of Clotho and development of novel methods for audio captioning.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How large is the Dialog State Tracking Dataset?,Sample Answer,1605.07683-3-Table1-1.png,1605.07683-12-Table3-1.png,1605.07683-13-Table5-1.png,1605.07683-13-Table6-1.png,1605.07683-14-Table8-1.png,"Table 1: Data used in this paper. Tasks 1-5 were generated using our simulator and share the same KB. Task 6 was converted from the 2nd Dialog State Tracking Challenge (Henderson et al., 2014a). Concierge is made of chats extracted from a real online concierge service. (∗) Tasks 1-5 have two test sets, one using the vocabulary of the training set and the other using out-of-vocabulary words.",Table 3: Task 1 (Issue API call) The model learns to direct its attention towards the 4 memories containing the information key to issue the API call. More hops help to strengthen this signal. <silence> is a special token used to indicate that the user did not speak at this turn – the model has to carry out the conversation with no additional input.,"Table 5: Task 3 (Displaying options) The model knows it has to display options but the attention is wrong: it should attend on the ratings to select the best option (with highest rating). It cannot learn that properly and match type features do not help. It is correct here by luck, the task is not solved overall (see Tab. 2). We do not show all memories in the table, only those with meaningful attention.","Table 6: Task 4 (Providing extra-information) The model knows it must display a phone or an address, but, as explained in Section A the embeddings mix up the information and make it hard to distinguish between different phone numbers or addresses, making answering correctly very hard. As shown in the results of Tab. 2, this problem can be solved by adding match type features, that allow to emphasize entities actually appearing in the history. The attention is globally wrong here.","Table 8: Hyperparameters of Supervised Embeddings. When Use History is True, the whole conversation history is concatenated with the latest user utterance to create the input. If False, only the latest utterance is used as input.",1.0,1.0,1.0,1.0,1.0,0.33333333333333337,1.0,0.2
How do they obtain structured data?,Sample Answer,1901.09501-Experimental Setup-6,1901.09501-Experimental Setup-7,1901.09501-Automatic Evaluation-0,1901.09501-10-Table5-1.png,1901.09501-11-Table6-1.png,"Adversarial Style Transfer (AdvST) BIBREF12 . As another latest style transfer approach capable of handling more than one attributes, the model also mixes back-translation with auto-encoding as the above method, and additionally uses adversarial training to disentangle content and style representations.","Ours w/o Coverage. For ablation study, we compare with a model variant that omits the content coverage constraint. That is, the model is trained by maximizing only Eq.( EQREF13 ).","As no ground truth annotations are available, we first set up automatic metrics for quantitatively measuring the key aspects of model performance.","Table 5: Example Outputs by Different Models. Text of erroneous content is highlighted in red, where [...] indicates desired content is missing. Text portions in the reference sentences and the generated sentences by our model that fulfill the stylistic characteristics are highlighted in blue. Please see the text for more details.","Table 6: Example Erroneous Outputs. Text of erroneous content is highlighted in red. Missing content is denoted with [...]. We also show the desired correct outputs. In the first example, the model was confused by the data types; while in the second example, the model fails to understand there is only one team in the content record x and the number 88 is the free-throw percentage.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Which competing objectives for their unsupevised method do they use?,Sample Answer,1901.09501-Dataset-1,1901.09501-Dataset-2,1901.09501-Model-2,1901.09501-Automatic Evaluation-3,1901.09501-10-Table5-1.png,"To obtain our data, we first split each game report into individual sentences, and, for each sentence, find its corresponding data in the box-score table as the content record. A record can contain a varying number of tuples, with each tuple containing three fields, namely a data type, a value, and an associated player or team, e.g., (team_points, 106, Lakers). As the original corpus is already largely clean, we found some simple rules are sufficient to obtain high-quality results in this step. Please see the supplementary materials for more details. Each of the resulting record-sentence pairs is treated as a pair of INLINEFORM0 , namely (content record, auxiliary sentence). The next step is to find a suitable reference sentence INLINEFORM1 for each content record INLINEFORM2 . As defined above, the reference sentence should cover similar but not the same content as in record INLINEFORM3 . We achieve this by retrieving from the data another record-sentence pair using INLINEFORM4 , where the retrieved record is designated to have a slightly different structure than that of INLINEFORM5 by having less or more tuples and different data types. More details of the retrieval method are deferred to supplements. The retrieved record-sentence pair thus plays the role of INLINEFORM6 and is paired with INLINEFORM7 to form an instance.","Table TABREF6 summarizes the statistics of the final dataset. The vocabulary size is 8.4K. We can see that the training set contains over 31K instances. Each content record contains around 5 tuples, each of which takes one of the 34 data types.","Let INLINEFORM0 denote the model that takes in a record INLINEFORM1 and a reference sentence INLINEFORM2 , and generates an output sentence INLINEFORM3 . Here INLINEFORM4 is the model parameter.","Content fidelity. Following the table-to-document task BIBREF0 where our dataset is derived from, we use an information extraction (IE) approach to measure content fidelity. That is, given a generated sentence INLINEFORM0 and the conditioning content record INLINEFORM1 , we extract data tuples from INLINEFORM2 with an IE tool, and compute the precision and recall against INLINEFORM3 . We use the IE model provided in BIBREF0 and re-train with INLINEFORM4 pairs in our dataset. The IE model achieves around 87% precision and 76% recall on the test set, which is comparable to the one used in BIBREF0 .","Table 5: Example Outputs by Different Models. Text of erroneous content is highlighted in red, where [...] indicates desired content is missing. Text portions in the reference sentences and the generated sentences by our model that fulfill the stylistic characteristics are highlighted in blue. Please see the text for more details.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What methodology is used to compensate for limited labelled data?,Sample Answer,1904.07342-Data-2,1904.07342-Outcome Analysis-0,1904.07342-Results & Discussion-0,1904.07342-Results & Discussion-1,1904.07342-2-Table1-1.png,"The second data batch consists of event-related tweets for five natural disasters occurring in the U.S. in 2018. These are: the East Coast Bomb Cyclone (Jan. 2 - 6); the Mendocino, California wildfires (Jul. 27 - Sept. 18); Hurricane Florence (Aug. 31 - Sept. 19); Hurricane Michael (Oct. 7 - 16); and the California Camp Fires (Nov. 8 - 25). For each disaster, we scraped tweets starting from two weeks prior to the beginning of the event, and continuing through two weeks after the end of the event. Summary statistics on the downloaded event-specific tweets are provided in Table TABREF1 . Note that the number of tweets occurring prior to the two 2018 sets of California fires are relatively small. This is because the magnitudes of these wildfires were relatively unpredictable, whereas blizzards and hurricanes are often forecast weeks in advance alongside public warnings. The first (influential tweet data) and second (event-related tweet data) batches are de-duplicated to be mutually exclusive. In Section SECREF2 , we perform geographic analysis on the event-related tweets from which we can scrape self-reported user city from Twitter user profile header cards; overall this includes 840 pre-event and 5,984 post-event tweets.","Our second goal is to compare the mean values of users' binary sentiments both pre- and post- each natural disaster event. Applying our highest-performing RNN to event-related tweets yields the following breakdown of positive tweets: Bomb Cyclone (34.7%), Mendocino Wildfire (80.4%), Hurricane Florence (57.2%), Hurricane Michael (57.6%), and Camp Fire (70.1%). As sanity checks, we examine the predicted sentiments on a subset with geographic user information and compare results to the prior literature.","In Figure FIGREF8 , we see that overall sentiment averages rarely show movement post-event: that is, only Hurricane Florence shows a significant difference in average tweet sentiment pre- and post-event at the 1% level, corresponding to a 0.12 point decrease in positive climate change sentiment. However, controlling for the same group of users tells a different story: both Hurricane Florence and Hurricane Michael have significant tweet sentiment average differences pre- and post-event at the 1% level. Within-cohort, Hurricane Florence sees an increase in positive climate change sentiment by 0.21 points, which is contrary to the overall average change (the latter being likely biased since an influx of climate change deniers are likely to tweet about hurricanes only after the event). Hurricane Michael sees an increase in average tweet sentiment of 0.11 points, which reverses the direction of tweets from mostly negative pre-event to mostly positive post-event. Likely due to similar bias reasons, the Mendocino wildfires in California see a 0.06 point decrease in overall sentiment post-event, but a 0.09 point increase in within-cohort sentiment. Methodologically, we assert that overall averages are not robust results to use in sentiment analyses.","We now comment on the two events yielding similar results between overall and within-cohort comparisons. Most tweets regarding the Bomb Cyclone have negative sentiment, though sentiment increases by 0.02 and 0.04 points post-event for overall and within-cohort averages, respectively. Meanwhile, the California Camp Fires yield a 0.11 and 0.27 point sentiment decline in overall and within-cohort averages, respectively. This large difference in sentiment change can be attributed to two factors: first, the number of tweets made regarding wildfires prior to the (usually unexpected) event is quite low, so within-cohort users tend to have more polarized climate change beliefs. Second, the root cause of the Camp Fires was quickly linked to PG&E, bolstering claims that climate change had nothing to do with the rapid spread of fire; hence within-cohort users were less vocally positive regarding climate change post-event.",Table 1: Tweets collected for each U.S. 2018 natural disaster,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How many videos did they use?,Sample Answer,1906.04236-Data Collection and Annotation-1,1906.04236-Visual Action Annotation-4,1906.04236-Discussion-1,1906.04236-Evaluation and Results-2,1906.04236-5-Table3-1.png,"By collecting routine videos, instead of searching explicitly for actions, we do implicit data gathering, a form of data collection introduced by BIBREF0 . Because everyday actions are common and not unusual, searching for them directly does not return many results. In contrast, by collecting routine videos, we find many everyday activities present in these videos.","After spam removal, we compute the agreement score between the turkers using Fleiss kappa BIBREF43 . Over the entire data set, the Fleiss agreement score is 0.35, indicating fair agreement. On the ground truth data, the Fleiss kappa score is 0.46, indicating moderate agreement. This fair to moderate agreement indicates that the task is difficult, and there are cases where the visibility of the actions is hard to label. To illustrate, Figure FIGREF9 shows examples where the annotators had low agreement.","The action labels extracted from the transcript are highly dependent on the performance of the constituency parser. This can introduce noise or ill-defined action labels. Some acions contain extra words (e.g., “brush my teeth of course”), or lack words (e.g., “let me just”). Some of this noise is handled during the annotation process; for example, most actions that lack words are labeled as “not visible” or “not an action” because they are hard to interpret.","In general, we find that the text information plays an important role. ELMo embeddings lead to better results than LSTM embeddings, with a relative error rate reduction of 6.8%. This is not surprising given that ELMo uses two bidirectional LSTMs and has improved the state-of-the-art in many NLP tasks BIBREF38 . Consequently, we use ELMo in our multimodal model.",Table 3: Data statistics.,1.0,1.0,1.0,1.0,1.0,0.25,0.3333333333333333,0.2
How long are the videos?,Sample Answer,1906.04236-Related Work-4,1906.04236-Data Gathering-3,1906.04236-Visual Action Annotation-4,1906.04236-Discussion-1,1906.04236-Evaluation and Results-3,"Another important difference between our methodology and previously proposed methods is that we extract action labels from the transcripts. By gathering data before annotating the actions, our action labels are post-defined (as in BIBREF0 ). This is unlike the majority of the existing human action datasets that use pre-defined labels BIBREF5 , BIBREF2 , BIBREF16 , BIBREF1 , BIBREF4 , BIBREF29 , BIBREF6 , BIBREF3 . Post-defined labels allow us to use a larger set of labels, expanding on the simplified label set used in earlier datasets. These action labels are more inline with everyday scenarios, where people often use different names for the same action. For example, when interacting with a robot, a user could refer to an action in a variety of ways; our dataset includes the actions “stick it into the freezer,” “freeze it,” “pop into the freezer,” and “put into the freezer,” variations, which would not be included in current human action recognition datasets.","Extract Candidate Actions from Transcript. Starting with the transcript, we generate a noisy list of potential actions. This is done using the Stanford parser BIBREF42 to split the transcript into sentences and identify verb phrases, augmented by a set of hand-crafted rules to eliminate some parsing errors. The resulting actions are noisy, containing phrases such as “found it helpful if you” and “created before up the top you.”","After spam removal, we compute the agreement score between the turkers using Fleiss kappa BIBREF43 . Over the entire data set, the Fleiss agreement score is 0.35, indicating fair agreement. On the ground truth data, the Fleiss kappa score is 0.46, indicating moderate agreement. This fair to moderate agreement indicates that the task is difficult, and there are cases where the visibility of the actions is hard to label. To illustrate, Figure FIGREF9 shows examples where the annotators had low agreement.","The action labels extracted from the transcript are highly dependent on the performance of the constituency parser. This can introduce noise or ill-defined action labels. Some acions contain extra words (e.g., “brush my teeth of course”), or lack words (e.g., “let me just”). Some of this noise is handled during the annotation process; for example, most actions that lack words are labeled as “not visible” or “not an action” because they are hard to interpret.","Moreover, the addition of extra information improves the results for both modalities. Specifically, the addition of context is found to bring improvements. The use of POS is also found to be generally helpful.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the 12 AV approaches which are examined?,Sample Answer,1906.10551-Related Work-0,1906.10551-Reliability (Determinism)-1,1906.10551-Experiments-11,1906.10551-Experiments-12,1906.10551-Experiments-17,"Over the years, researchers in the field of authorship analysis identified a number of challenges and limitations regarding existing studies and approaches. Azarbonyad et al. BIBREF8 , for example, focused on the questions if the writing styles of authors of short texts change over time and how this affects AA. To answer these questions, the authors proposed an AA approach based on time-aware language models that incorporate the temporal changes of the writing style of authors. In one of our experiments, we focus on a similar question, namely, whether it is possible to recognize the writing style of authors, despite of large time spans between their documents. However, there are several differences between our experiment and the study of Azarbonyad et al. First, the authors consider an AA task, where one anonymous document INLINEFORM0 has to be attributed to one of INLINEFORM1 possible candidate authors, while we focus on an AV task, where INLINEFORM2 is compared against one document INLINEFORM3 of a known author. Second, the authors focus on texts with informal language (emails and tweets) in their study, while in our experiment we consider documents written in a formal language (scientific works). Third, Azarbonyad et al. analyzed texts with a time span of four years, while in our experiment the average time span is 15.6 years. Fourth, in contrast to the approach of the authors, none of the 12 examined AV approaches in our experiment considers a special handling of temporal stylistic changes.","In his standard reference book, Bollen BIBREF17 gives a clear description for this term: “Reliability is the consistency of measurement” and provides a simple example to illustrate its meaning: At time INLINEFORM0 we ask a large number of persons the same question Q and record their responses. Afterwards, we remove their memory of the dialogue. At time INLINEFORM1 we ask them again the same question Q and record their responses again. “The reliability is the consistency of the responses across individuals for the two time periods. To the extent that all individuals are consistent, the measure is reliable” BIBREF17 . This example deals with the consistency of the measured objects as a factor for the reliability of measurements. In the case of authorship verification, the analyzed objects are static data, and hence these cannot be a source of inconsistency. However, the measurement system itself can behave inconsistently and hence unreliable. This aspect can be described as intra-rater reliability.","In the following we show to which extent these assumptions hold. As a data basis for this experiment, we used the INLINEFORM0 corpus introduced in Section UID30 . The results regarding the 12 AV methods are given in Table TABREF44 , where it can be seen that our assumptions hold. All examined AV methods (with no exception) are fooled by the topical bias in the corpus. Here, the highest achieved results in terms of c@1 and AUC are very close to random guessing. A closer look at the confusion matrix outcomes reveals that some methods, for example ImpGI and OCCAV, perform almost entirely inverse to each other, where the former predicts nothing but Y and the latter nothing but N (except 1 Y). Moreover, we can assume that the lower c@1 is, the stronger is the focus of the respective AV method on the topic of the documents. Overall, the results of this experiment suggest that none of the examined AV methods is robust against topical influence.","In our third experiment, we investigate the question how text lengths affect the results of the examined AV methods. The motivation behind this experiment is based on the observation of Stamatatos et al. BIBREF12 that text length is an important issue, which has not been thoroughly studied within authorship verification research. To address this issue, we make use of the INLINEFORM0 corpus introduced in Section UID28 . The corpus is suitable for this purpose, as it comprises a large number of verification problems, where more than 90% of all documents have sufficient text lengths ( INLINEFORM1 2,000 characters). This allows a stepwise truncation and by this to analyze the effect between the text lengths and the recognition results. However, before considering this, we first focus on the results (shown in Table TABREF46 ) after applying all 12 AV methods on the original test corpus.","While inspecting the 250 characters long documents in more detail, we identified that they share similar vocabularies consisting of chat abbreviations such as “lol” (laughing out loud) or “k” (ok), smileys and specific obscene words. Therefore, we assume that the verification results of the examined methods are mainly caused by the similar vocabularies between the texts.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Do they evaluate only on English datasets?,Sample Answer,1804.05253-Introduction-2,1804.05253-Introduction-3,1804.05253-Typographic irony markers:-2,1804.05253-Typographic irony markers:-5,1804.05253-4-Table6-1.png,"Both utterances are labeled as ironic by their authors (using hashtags in INLINEFORM0 and the /s marker in INLINEFORM1 ). In the INLINEFORM2 example, the author uses several irony markers such as Rhetorical question (e.g., “are you telling” ...) and metaphor (e.g., “golden age”). In the INLINEFORM3 example, we notice the use of capitalization (“AWESOME”) and emoticons (“:P” (tongue out)) that the author uses to alert the readers that it is an ironic tweet.","We present three contributions in this paper. First, we provide a detailed investigation of a set of theoretically-grounded irony markers (e.g., tropes, morpho-syntactic, and typographic markers) in social media. We conduct the classification and frequency analysis based on their occurrence. Second, we analyze and compare the use of irony markers on two social media platforms ( INLINEFORM0 and INLINEFORM1 ). Third, we provide an analysis of markers on topically different social media content (e.g., technology vs. political subreddits).","Other punctuation marks - Punctuation marks such as “?”, “.”, “;” and their various uses (e.g., single/multiple/mix of two different punctuations) are used as features.","Emoji - Emojis are like emoticons, but they are actual pictures and recently have become very popular in social media. Figure FIGREF22 shows a tweet with two emojis (e.g., “unassumed” and “confounded” faces respectively) used as markers. We use an emoji library of 1,400 emojis to identify the particular emoji used in irony utterances and use them as binary indicators.","Table 6: Frequency of irony markers in different genres (subreddits). The mean and the SD (in bracket) are reported.x ∗∗ and x ∗ depict significance on p ≤ 0.005 and p ≤ 0.05, respectively.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What type of frequency analysis was used?,Sample Answer,1804.05253-Tropes:-1,1804.05253-Tropes:-2,1804.05253-Typographic irony markers:-2,1804.05253-Typographic irony markers:-4,1804.05253-Typographic irony markers:-5,"Metaphors - Metaphors often facilitate ironic representation and are used as markers. We have drawn metaphors from different sources (e.g., 884 and 8,600 adjective/noun metaphors from BIBREF11 and BIBREF12 , respectively, and used them as binary features. We also evaluate the metaphor detector BIBREF13 over INLINEFORM0 and INLINEFORM1 datasets. We considered metaphor candidates that have precision INLINEFORM2 0.75 (see BIBREF13 (2017)).",Hyperbole - Hyperboles or intensifiers are commonly used in irony because speakers frequently overstate the magnitude of a situation or event. We use terms that are denoted as “strong subjective” (positive/negative) from the MPQA corpus BIBREF14 as hyperboles. Apart from using hyperboles directly as the binary feature we also use their sentiment as features.,"Other punctuation marks - Punctuation marks such as “?”, “.”, “;” and their various uses (e.g., single/multiple/mix of two different punctuations) are used as features.","Emoticon - Emoticons are frequently used to emphasize the ironic intent of the user. In the example “I love the weather ;) #irony”, the emoticon “;)” (wink) alerts the reader to a possible ironic interpretation of weather (i.e., bad weather). We collected a comprehensive list of emoticons (over one-hundred) from Wikipedia and also used standard regular expressions to identify emoticons in our datasets. Beside using the emoticons directly as binary features, we use their sentiment as features as well (e.g., “wink” is regarded as positive sentiment in MPQA).","Emoji - Emojis are like emoticons, but they are actual pictures and recently have become very popular in social media. Figure FIGREF22 shows a tweet with two emojis (e.g., “unassumed” and “confounded” faces respectively) used as markers. We use an emoji library of 1,400 emojis to identify the particular emoji used in irony utterances and use them as binary indicators.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Who annotated the Twitter and Reddit data for irony?,Sample Answer,1804.05253-Morpho-syntactic (MS) irony markers:-0,1804.05253-Morpho-syntactic (MS) irony markers:-2,1804.05253-Typographic irony markers:-2,1804.05253-Classification Experiments and Results-1,1804.05253-Frequency analysis of markers-0,This type of markers appear at the morphologic and syntactic levels of an utterance.,"Tag questions - We built a list of tag questions (e.g.,, “didn't you?”, “aren't we?”) from a grammar site and use them as binary indicators.","Other punctuation marks - Punctuation marks such as “?”, “.”, “;” and their various uses (e.g., single/multiple/mix of two different punctuations) are used as features.","Table TABREF23 shows that for ironic utterances in INLINEFORM0 , removing tropes have the maximum negative effect on Recall, with a reduction on INLINEFORM1 score by 15%. This is primarily due to the removal of hyperboles that frequently appear in ironic utterances in INLINEFORM2 . Removing typographic markers (e.g., emojis, emoticons, etc.) have the maximum negative effect on the Precision for the irony INLINEFORM3 category, since particular emojis and emoticons appear regularly in ironic utterances (Table TABREF25 ). For INLINEFORM4 , Table TABREF24 shows that removal of typographic markers such as emoticons does not affect the F1 scores, whereas the removal of morpho-syntactic markers, e.g., tag questions, interjections have a negative effect on the F1. Table TABREF25 and Table TABREF26 represent the INLINEFORM5 most discriminative features for both categories based on the feature weights learned during the SVM training for INLINEFORM6 and INLINEFORM7 , respectively. Table TABREF25 shows that for INLINEFORM8 , typographic features such as emojis and emoticons have the highest feature weights for both categories. Interestingly, we observe that for ironic tweets users often express negative sentiment directly via emojis (e.g., angry face, rage) whereas for non-ironic utterances, emojis with positive sentiments (e.g., hearts, wedding) are more familiar. For INLINEFORM9 (Table TABREF26 ), we observe that instead of emojis, other markers such as exclamation marks, negative tag questions, and metaphors are discriminatory markers for the irony category. In contrary, for the non-irony category, positive tag questions and negative sentiment hyperboles are influential features.","We also investigate the occurrence of markers in the two platforms via frequency analysis (Table TABREF29 ). We report the mean of occurrence per utterance and the standard deviation (SD) of each marker. Table TABREF29 shows that markers such as hyperbole, punctuations, and interjections are popular in both platforms. Emojis and emoticons, although the two most popular markers in INLINEFORM0 are almost unused in INLINEFORM1 . Exclamations and INLINEFORM2 s are more common in the INLINEFORM3 corpus. Next, we combine each marker with the type they belong to (i.e., either trope, morpho-syntactic and typographic) and compare the means between each pair of types via independent t-tests. We found that the difference of means is significant ( INLINEFORM4 ) for all pair of types across the two platforms.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Which hyperparameters were varied in the experiments on the four tasks?,Sample Answer,1705.01265-Fine-grained Sentiment Analysis-2,1705.01265-Fine-Grained Sentiment Quantification-0,1705.01265-Fine-Grained Sentiment Quantification-1,1705.01265-Conclusion-0,1705.01265-5-Table4-1.png,"The evaluation measure selected in BIBREF13 for the task in the macro-averaged Mean Absolute Error (MAE INLINEFORM0 ). It is a measure of error, hence lower values are better. The measure's goal is to take into account the order of the classes when penalizing the decision of a classifier. For instance, misclassifying a very negative example as very positive is a bigger mistake than classifying it as negative or neutral. Penalizing a classifier according to how far the predictions are from the true class is captured by MAE INLINEFORM1 BIBREF14 . Also, the advantage of using the macro- version instead of the standard version of the measure is the robustness against the class imbalance in the data.","Quantification is the problem of estimating the prevalence of a class in a dataset. While classification concerns assigning a category to a single instance, like labeling a tweet with the sentiment it conveys, the goal of quantification is, given a set of instances, to estimate the relative frequency of single class. Therefore, sentiment quantification tries to answer questions like “Given a set of tweets about the new iPhone, what is the fraction of VeryPositive ones?”. In the rest, we show the effect of the features derived from the word embeddings clusters in the fine-grained classification problem, which was also part of the SemEval-2016 “Sentiment Analysis in Twitter” task BIBREF13 .","Learning Algorithm To perform the quantification task, we rely on a classify and count approach, which was shown effective in a related binary quantification problem BIBREF15 . The idea is that given a set of instances on a particular subject, one first classifies the instances and then aggregates the counts. To this end, we use the same feature representation steps and data with the ones used for fine grained classification (Section 3.2). Note that the data of the task are associated with subjects (described in full detail at BIBREF13 ), and, hence, quantification is performed for the tweets of a subject. For each of the five categories, the output of the approach is a 5-dimensional vector with the estimated prevalence of the categories.","We have shown empirically the effectiveness of incorporating cluster membership features in the feature extraction pipeline of Named-Entity recognition, sentiment classification and quantification tasks. Our results strongly suggest that incorporating cluster membership features benefit the performance in the tasks. The fact that the performance improvements are consistent in the four tasks we investigated, further highlights their usefulness, both for practitioners and researchers.",Table 4: Sample from two clusters that were found useful for the sentiment classification. Words with positive or negative meaning are grouped together.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How were the cluster extracted? ,Sample Answer,1705.01265-Named-Entity Recognition in Twitter-2,1705.01265-Fine-grained Sentiment Analysis-0,1705.01265-Fine-grained Sentiment Analysis-2,1705.01265-Fine-Grained Sentiment Quantification-0,1705.01265-Fine-Grained Sentiment Quantification-2, INLINEFORM0 tonite ... 90 's music .. oldskool night wiith INLINEFORM1 ,"The task of fine grained sentiment classification consists in predicting the sentiment of an input text according to a five point scale (sentiment INLINEFORM0 {VeryNegative, Negative, Neutral, Positive, VeryPositive}). We use the setting of task 4 of SemEval2016 “Sentiment Analysis in Twitter” and the dataset released by the organizers for subtask 4 BIBREF13 .","The evaluation measure selected in BIBREF13 for the task in the macro-averaged Mean Absolute Error (MAE INLINEFORM0 ). It is a measure of error, hence lower values are better. The measure's goal is to take into account the order of the classes when penalizing the decision of a classifier. For instance, misclassifying a very negative example as very positive is a bigger mistake than classifying it as negative or neutral. Penalizing a classifier according to how far the predictions are from the true class is captured by MAE INLINEFORM1 BIBREF14 . Also, the advantage of using the macro- version instead of the standard version of the measure is the robustness against the class imbalance in the data.","Quantification is the problem of estimating the prevalence of a class in a dataset. While classification concerns assigning a category to a single instance, like labeling a tweet with the sentiment it conveys, the goal of quantification is, given a set of instances, to estimate the relative frequency of single class. Therefore, sentiment quantification tries to answer questions like “Given a set of tweets about the new iPhone, what is the fraction of VeryPositive ones?”. In the rest, we show the effect of the features derived from the word embeddings clusters in the fine-grained classification problem, which was also part of the SemEval-2016 “Sentiment Analysis in Twitter” task BIBREF13 .","The evaluation measure for the problem is the Earth Movers Distance (EMD) BIBREF18 . EMD is a measure of error, hence lower values are better. It assumes ordered categories, which in our problem is naturally defined. Further assuming that the distance of consecutive categories (e.g., Positive and VeryPositive) is 1, the measure is calculated by: INLINEFORM0 ",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What other evaluation metrics are reported?,Sample Answer,1910.12203-Introduction-0,1910.12203-Introduction-2,1910.12203-Proposed Model ::: Input Representation-0,1910.12203-Proposed Model ::: Graph based Neural Networks ::: Graph Attention Network (GAT)-0,1910.12203-Conclusion-0,"In today's day and age of social media, there are ample opportunities for fake news production, dissemination and consumption. BIBREF0 break down fake news into three categories, hoax, propaganda and satire. A hoax article typically tries to convince the reader about a cooked-up story while propaganda ones usually mislead the reader into believing a false political or social agenda. BIBREF1 defines a satirical article as the one which deliberately exposes real-world individuals, organisations and events to ridicule.","In this work, we propose a graph neural network-based model to classify news articles while capturing the interaction of sentences across the document. We present a series of experiments on News Corpus with Varying Reliability dataset BIBREF0 and Satirical Legitimate News dataset BIBREF2. Our results demonstrate that the proposed model achieves state-of-the-art performance on these datasets and provides interesting insights. Experiments performed in out-of-domain settings establish the generalizability of our proposed method.","Each document in the corpus is represented as a graph. The nodes of the graph represent the sentences of a document while the edges represent the semantic similarity between a pair of sentences. Representing a document as a fully connected graph allows the model to directly capture the interaction of each sentence with every other sentence in the document. Formally,","BIBREF16 introduced graph attention networks to address various shortcomings of GCNs. Most importantly, they enable nodes to attend over their neighborhoods’ features without depending on the graph structure upfront. The key idea is to compute the hidden representations of each node in the graph, by attending over its neighbors, following a self-attention BIBREF15 strategy. By default, there is one attention head in the GAT model. For our GAT + 2 Attn Heads model, we use two attention heads and concatenate the node embeddings obtained from different heads before passing it to the pooling layer. For a fully connected graph, the GAT model allows every node to attend on every other node and learn the edge weights. Thus, initializing the edge weights using the SS model is useless as they are being learned. Mathematical details are provided in the Supplementary Material.","This paper introduces a novel way of encoding articles for fake news classification. The intuition behind representing documents as a graph is motivated by the fact that sentences interact differently with each other across different kinds of article. Recurrent networks are unable to maintain long term dependencies in large documents, whereas a fully connected graph captures the interaction between sentences at unit distance. The quantitative result shows the effectiveness of our proposed model and the qualitative results validate our hypothesis about difference in sentence interaction across different articles. Further, we show that our proposed model generalizes to unseen datasets.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What out of domain scenarios did they evaluate on?,Sample Answer,1910.12203-Introduction-0,1910.12203-Proposed Model ::: Input Representation-0,1910.12203-Proposed Model ::: Graph based Neural Networks-0,1910.12203-Proposed Model ::: Graph based Neural Networks ::: Graph Attention Network (GAT)-0,1910.12203-Conclusion-0,"In today's day and age of social media, there are ample opportunities for fake news production, dissemination and consumption. BIBREF0 break down fake news into three categories, hoax, propaganda and satire. A hoax article typically tries to convince the reader about a cooked-up story while propaganda ones usually mislead the reader into believing a false political or social agenda. BIBREF1 defines a satirical article as the one which deliberately exposes real-world individuals, organisations and events to ridicule.","Each document in the corpus is represented as a graph. The nodes of the graph represent the sentences of a document while the edges represent the semantic similarity between a pair of sentences. Representing a document as a fully connected graph allows the model to directly capture the interaction of each sentence with every other sentence in the document. Formally,","We reformulate the fake news classification problem as a graph classification task, where a graph represents a document. Given a graph $G= (E,S)$ where $E$ is the adjacency matrix and $S$ is the sentence feature matrix. We randomly initialize the word embeddings and use the last hidden state of a LSTM layer as the sentence embedding, shown in Figure FIGREF5. We experiment with two kinds of graph neural networks,","BIBREF16 introduced graph attention networks to address various shortcomings of GCNs. Most importantly, they enable nodes to attend over their neighborhoods’ features without depending on the graph structure upfront. The key idea is to compute the hidden representations of each node in the graph, by attending over its neighbors, following a self-attention BIBREF15 strategy. By default, there is one attention head in the GAT model. For our GAT + 2 Attn Heads model, we use two attention heads and concatenate the node embeddings obtained from different heads before passing it to the pooling layer. For a fully connected graph, the GAT model allows every node to attend on every other node and learn the edge weights. Thus, initializing the edge weights using the SS model is useless as they are being learned. Mathematical details are provided in the Supplementary Material.","This paper introduces a novel way of encoding articles for fake news classification. The intuition behind representing documents as a graph is motivated by the fact that sentences interact differently with each other across different kinds of article. Recurrent networks are unable to maintain long term dependencies in large documents, whereas a fully connected graph captures the interaction between sentences at unit distance. The quantitative result shows the effectiveness of our proposed model and the qualitative results validate our hypothesis about difference in sentence interaction across different articles. Further, we show that our proposed model generalizes to unseen datasets.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What was their state of the art accuracy score?,Sample Answer,1910.12203-Introduction-2,1910.12203-Dataset and Baseline-2,1910.12203-Proposed Model ::: Input Representation-0,1910.12203-Proposed Model ::: Graph based Neural Networks-0,1910.12203-Conclusion-0,"In this work, we propose a graph neural network-based model to classify news articles while capturing the interaction of sentences across the document. We present a series of experiments on News Corpus with Varying Reliability dataset BIBREF0 and Satirical Legitimate News dataset BIBREF2. Our results demonstrate that the proposed model achieves state-of-the-art performance on these datasets and provides interesting insights. Experiments performed in out-of-domain settings establish the generalizability of our proposed method.","LSTM: In this model, we encode the document using a LSTM (Long Short-Term Memory) layer BIBREF12. We use the hidden state at the last time step as the document vector which is passed to a fully connected projection layer to get the logits over output classes.","Each document in the corpus is represented as a graph. The nodes of the graph represent the sentences of a document while the edges represent the semantic similarity between a pair of sentences. Representing a document as a fully connected graph allows the model to directly capture the interaction of each sentence with every other sentence in the document. Formally,","We reformulate the fake news classification problem as a graph classification task, where a graph represents a document. Given a graph $G= (E,S)$ where $E$ is the adjacency matrix and $S$ is the sentence feature matrix. We randomly initialize the word embeddings and use the last hidden state of a LSTM layer as the sentence embedding, shown in Figure FIGREF5. We experiment with two kinds of graph neural networks,","This paper introduces a novel way of encoding articles for fake news classification. The intuition behind representing documents as a graph is motivated by the fact that sentences interact differently with each other across different kinds of article. Recurrent networks are unable to maintain long term dependencies in large documents, whereas a fully connected graph captures the interaction between sentences at unit distance. The quantitative result shows the effectiveness of our proposed model and the qualitative results validate our hypothesis about difference in sentence interaction across different articles. Further, we show that our proposed model generalizes to unseen datasets.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Which variation provides the best results on this dataset?,Sample Answer,1909.13104-Introduction-0,1909.13104-Introduction-2,1909.13104-Related Work-0,1909.13104-Proposed methodology ::: Text processing-0,1909.13104-Conclusion - Future work-0,"In the era of social media and networking platforms, Twitter has been doomed for abuse and harassment toward users specifically women. In fact, online harassment becomes very common in Twitter and there have been a lot of critics that Twitter has become the platform for many racists, misogynists and hate groups which can express themselves openly. Online harassment is usually in the form of verbal or graphical formats and is considered harassment, because it is neither invited nor has the consent of the receipt. Monitoring the contents including sexism and sexual harassment in traditional media is easier than monitoring on the online social media platforms like Twitter. The main reason is because of the large amount of user generated content in these media. So, the research about the automated detection of content containing sexual harassment is an important issue and could be the basis for removing that content or flagging it for human evaluation. The basic goal of this automatic classification is that it will significantly improve the process of detecting these types of hate speech on social media by reducing the time and effort required by human beings.","In this paper we present our work, which is a part of the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. The topic of the competition is the classification of different types of harassment and it is divided in two tasks. The first one is the classification of the tweets in harassment and non-harassment categories, while the second one is the classification in specific harassment categories like indirect harassment, physical and sexual harassment as well. We are using the dataset of the competition, which includes text from tweets having the aforementioned categories. Our approach is based on the Recurrent Neural Networks and particularly we are using a deep, classiﬁcation specific attention mechanism. Moreover, we present a comparison between different variations of this attention-based approach like multi-attention and single attention models. The next Section includes a short description of the related work, while the third Section includes a description of the dataset. After that, we describe our methodology. Finally, we describe the experiments and we present the results and our conclusion.","Waseem et al. BIBREF1 were the first who collected hateful tweets and categorized them into being sexist, racist or neither. However, they did not provide specific definitions for each category. Jha and Mamidi BIBREF0 focused on just sexist tweets and proposed two categories of hostile and benevolent sexism. However, these categories were general as they ignored other types of sexism happening in social media. Sharifirad S. and Matwin S. BIBREF2 proposed complimentary categories of sexist language inspired from social science work. They categorized the sexist tweets into the categories of indirect harassment, information threat, sexual harassment and physical harassment. In the next year the same authors proposed BIBREF3 a more comprehensive categorization of online harassment in social media e.g. twitter into the following categories, indirect harassment, information threat, sexual harassment, physical harassment and not sexist.",Before training our models we are processing the given tweets using a tweet pre-processor. The scope here is the cleaning and tokenization of the dataset.,"We present an attention-based approach for the detection of harassment language in tweets and the detection of different types of harassment as well. Our approach is based on the Recurrent Neural Networks and particularly we are using a deep, classiﬁcation specific attention mechanism. Moreover, we present a comparison between different variations of this attention-based approach and a few baseline methods. According to the results of our experiments and considering the F1 Score, the multi-attention method having a projected layer, achieved the highest performance. Also, we tackled the problem of the imbalance between the training, validation and test sets performing the technique of back-translation.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the different variations of the attention-based approach which are examined?,Sample Answer,1909.13104-Introduction-0,1909.13104-Introduction-2,1909.13104-Proposed methodology ::: Text processing-0,1909.13104-Experiments ::: Training Models-5,1909.13104-Experiments ::: Evaluation and Results-0,"In the era of social media and networking platforms, Twitter has been doomed for abuse and harassment toward users specifically women. In fact, online harassment becomes very common in Twitter and there have been a lot of critics that Twitter has become the platform for many racists, misogynists and hate groups which can express themselves openly. Online harassment is usually in the form of verbal or graphical formats and is considered harassment, because it is neither invited nor has the consent of the receipt. Monitoring the contents including sexism and sexual harassment in traditional media is easier than monitoring on the online social media platforms like Twitter. The main reason is because of the large amount of user generated content in these media. So, the research about the automated detection of content containing sexual harassment is an important issue and could be the basis for removing that content or flagging it for human evaluation. The basic goal of this automatic classification is that it will significantly improve the process of detecting these types of hate speech on social media by reducing the time and effort required by human beings.","In this paper we present our work, which is a part of the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. The topic of the competition is the classification of different types of harassment and it is divided in two tasks. The first one is the classification of the tweets in harassment and non-harassment categories, while the second one is the classification in specific harassment categories like indirect harassment, physical and sexual harassment as well. We are using the dataset of the competition, which includes text from tweets having the aforementioned categories. Our approach is based on the Recurrent Neural Networks and particularly we are using a deep, classiﬁcation specific attention mechanism. Moreover, we present a comparison between different variations of this attention-based approach like multi-attention and single attention models. The next Section includes a short description of the related work, while the third Section includes a description of the dataset. After that, we describe our methodology. Finally, we describe the experiments and we present the results and our conclusion.",Before training our models we are processing the given tweets using a tweet pre-processor. The scope here is the cleaning and tokenization of the dataset.,$BCE = -\frac{1}{n}\sum _{i=1}^{n}[y_{i}log(y^{^{\prime }}_{i}) + (1 - y_{i})log(1 - y^{^{\prime }}_{i}))]$,"Each model produces four scores and each score is the probability that a tweet includes harassment language, indirect, physical and sexual harassment language respectively. For any tweet, we first check the score of the harassment language and if it is less than a specified threshold, then the harassment label is zero, so the other three labels are zero as well. If it is greater than or equal to that threshold, then the harassment label is one and the type of harassment is the one among these three having that has the greatest score (highest probability). We set this threshold equal to 0.33.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What dataset is used for this work?,Sample Answer,1909.13104-Introduction-0,1909.13104-Introduction-2,1909.13104-Related Work-0,1909.13104-Experiments ::: Evaluation and Results-0,1909.13104-Conclusion - Future work-0,"In the era of social media and networking platforms, Twitter has been doomed for abuse and harassment toward users specifically women. In fact, online harassment becomes very common in Twitter and there have been a lot of critics that Twitter has become the platform for many racists, misogynists and hate groups which can express themselves openly. Online harassment is usually in the form of verbal or graphical formats and is considered harassment, because it is neither invited nor has the consent of the receipt. Monitoring the contents including sexism and sexual harassment in traditional media is easier than monitoring on the online social media platforms like Twitter. The main reason is because of the large amount of user generated content in these media. So, the research about the automated detection of content containing sexual harassment is an important issue and could be the basis for removing that content or flagging it for human evaluation. The basic goal of this automatic classification is that it will significantly improve the process of detecting these types of hate speech on social media by reducing the time and effort required by human beings.","In this paper we present our work, which is a part of the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. The topic of the competition is the classification of different types of harassment and it is divided in two tasks. The first one is the classification of the tweets in harassment and non-harassment categories, while the second one is the classification in specific harassment categories like indirect harassment, physical and sexual harassment as well. We are using the dataset of the competition, which includes text from tweets having the aforementioned categories. Our approach is based on the Recurrent Neural Networks and particularly we are using a deep, classiﬁcation specific attention mechanism. Moreover, we present a comparison between different variations of this attention-based approach like multi-attention and single attention models. The next Section includes a short description of the related work, while the third Section includes a description of the dataset. After that, we describe our methodology. Finally, we describe the experiments and we present the results and our conclusion.","Waseem et al. BIBREF1 were the first who collected hateful tweets and categorized them into being sexist, racist or neither. However, they did not provide specific definitions for each category. Jha and Mamidi BIBREF0 focused on just sexist tweets and proposed two categories of hostile and benevolent sexism. However, these categories were general as they ignored other types of sexism happening in social media. Sharifirad S. and Matwin S. BIBREF2 proposed complimentary categories of sexist language inspired from social science work. They categorized the sexist tweets into the categories of indirect harassment, information threat, sexual harassment and physical harassment. In the next year the same authors proposed BIBREF3 a more comprehensive categorization of online harassment in social media e.g. twitter into the following categories, indirect harassment, information threat, sexual harassment, physical harassment and not sexist.","Each model produces four scores and each score is the probability that a tweet includes harassment language, indirect, physical and sexual harassment language respectively. For any tweet, we first check the score of the harassment language and if it is less than a specified threshold, then the harassment label is zero, so the other three labels are zero as well. If it is greater than or equal to that threshold, then the harassment label is one and the type of harassment is the one among these three having that has the greatest score (highest probability). We set this threshold equal to 0.33.","We present an attention-based approach for the detection of harassment language in tweets and the detection of different types of harassment as well. Our approach is based on the Recurrent Neural Networks and particularly we are using a deep, classiﬁcation specific attention mechanism. Moreover, we present a comparison between different variations of this attention-based approach and a few baseline methods. According to the results of our experiments and considering the F1 Score, the multi-attention method having a projected layer, achieved the highest performance. Also, we tackled the problem of the imbalance between the training, validation and test sets performing the technique of back-translation.",1.0,1.0,1.0,1.0,1.0,0.33333333333333337,1.0,0.2
What were the datasets used in this paper?,Sample Answer,1909.13104-Introduction-0,1909.13104-Introduction-2,1909.13104-Related Work-0,1909.13104-Experiments ::: Evaluation and Results-0,1909.13104-Conclusion - Future work-0,"In the era of social media and networking platforms, Twitter has been doomed for abuse and harassment toward users specifically women. In fact, online harassment becomes very common in Twitter and there have been a lot of critics that Twitter has become the platform for many racists, misogynists and hate groups which can express themselves openly. Online harassment is usually in the form of verbal or graphical formats and is considered harassment, because it is neither invited nor has the consent of the receipt. Monitoring the contents including sexism and sexual harassment in traditional media is easier than monitoring on the online social media platforms like Twitter. The main reason is because of the large amount of user generated content in these media. So, the research about the automated detection of content containing sexual harassment is an important issue and could be the basis for removing that content or flagging it for human evaluation. The basic goal of this automatic classification is that it will significantly improve the process of detecting these types of hate speech on social media by reducing the time and effort required by human beings.","In this paper we present our work, which is a part of the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. The topic of the competition is the classification of different types of harassment and it is divided in two tasks. The first one is the classification of the tweets in harassment and non-harassment categories, while the second one is the classification in specific harassment categories like indirect harassment, physical and sexual harassment as well. We are using the dataset of the competition, which includes text from tweets having the aforementioned categories. Our approach is based on the Recurrent Neural Networks and particularly we are using a deep, classiﬁcation specific attention mechanism. Moreover, we present a comparison between different variations of this attention-based approach like multi-attention and single attention models. The next Section includes a short description of the related work, while the third Section includes a description of the dataset. After that, we describe our methodology. Finally, we describe the experiments and we present the results and our conclusion.","Waseem et al. BIBREF1 were the first who collected hateful tweets and categorized them into being sexist, racist or neither. However, they did not provide specific definitions for each category. Jha and Mamidi BIBREF0 focused on just sexist tweets and proposed two categories of hostile and benevolent sexism. However, these categories were general as they ignored other types of sexism happening in social media. Sharifirad S. and Matwin S. BIBREF2 proposed complimentary categories of sexist language inspired from social science work. They categorized the sexist tweets into the categories of indirect harassment, information threat, sexual harassment and physical harassment. In the next year the same authors proposed BIBREF3 a more comprehensive categorization of online harassment in social media e.g. twitter into the following categories, indirect harassment, information threat, sexual harassment, physical harassment and not sexist.","Each model produces four scores and each score is the probability that a tweet includes harassment language, indirect, physical and sexual harassment language respectively. For any tweet, we first check the score of the harassment language and if it is less than a specified threshold, then the harassment label is zero, so the other three labels are zero as well. If it is greater than or equal to that threshold, then the harassment label is one and the type of harassment is the one among these three having that has the greatest score (highest probability). We set this threshold equal to 0.33.","We present an attention-based approach for the detection of harassment language in tweets and the detection of different types of harassment as well. Our approach is based on the Recurrent Neural Networks and particularly we are using a deep, classiﬁcation specific attention mechanism. Moreover, we present a comparison between different variations of this attention-based approach and a few baseline methods. According to the results of our experiments and considering the F1 Score, the multi-attention method having a projected layer, achieved the highest performance. Also, we tackled the problem of the imbalance between the training, validation and test sets performing the technique of back-translation.",1.0,1.0,1.0,1.0,1.0,0.28571428571428575,0.5,0.2
what are the existing datasets for this task?,Sample Answer,1908.07491-Introduction-0,1908.07491-Related work-0,1908.07491-Related work-1,1908.07491-Estimating a concept's controversiality level ::: Validation ::: Leave one category out-0,1908.07491-Conclusions-0,"Indicating that a web page is controversial, or disputed - for example, in a search result - facilitates an educated consumption of the information therein, suggesting the content may not represent the “full picture”. Here, we consider the problem of estimating the level of controversiality associated with a given Wikipedia concept (title). We demonstrate that the textual contexts in which the concept is referenced can be leveraged to facilitate this.","Analysis of controversy in Wikipedia, online news and social media has attracted considerable attention in recent years. Exploiting the collaborative structure of Wikipedia, estimators of the level of controversy in a Wikipedia article were developed based on the edit-history of the article BIBREF0, BIBREF3. Along these lines, BIBREF4 detect controversy based on mutual reverts, bi-polarity in the collaboration network, and mutually-reinforced scores for editors and articles. Similarly, BIBREF1 classify whether a Wikipedia page is controversial through the combined evaluation of the topically neighboring set of pages.","Content analysis of controversial Wikipedia articles has been used to evaluate the level of controversy of other documents (e.g., web pages) by mapping them to related Wikipedia articles BIBREF5. BIBREF6 further build a language model, which enhances predictions made by existing classifiers, by inferring word probabilities from Wikipedia articles prominent in Wikipedia controversy features (mainly signals in edit history as discussed above) and from articles retrieved by manually selected query terms, believed to indicate controversy.","In a preliminary task, we looked for words which may designate sentences associated with controversial concepts. To this end, we ranked the words appearing in positive sentences according to their information gain for this task. The top of the list comprises the following: that, sexual, people, movement, religious, issues, rights.","We demonstrated that the sentence–level context in which a concept appears is indicative of its controversiality. This follows BIBREF10, who show this for concept abstractness and suggest to explore further properties identifiable in this way. Importantly, we observed that this method may pick up signals which are not directly related to the property of interest. For example, since many controversial concepts have to do with religion, part of what this method may learn is thematic relatedness to religion. However, when controlling for this effect, the drop in accuracy is fairly small.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what is the size of the introduced dataset?,Sample Answer,1908.07491-Introduction-0,1908.07491-Introduction-3,1908.07491-Introduction-4,1908.07491-Related work-0,1908.07491-Conclusions-1,"Indicating that a web page is controversial, or disputed - for example, in a search result - facilitates an educated consumption of the information therein, suggesting the content may not represent the “full picture”. Here, we consider the problem of estimating the level of controversiality associated with a given Wikipedia concept (title). We demonstrate that the textual contexts in which the concept is referenced can be leveraged to facilitate this.","Focusing here on Wikipedia concepts, we adopt as an initial “ground truth” the titles listed on the Wikipedia list of controversial issues, which is curated based on so-called “edit wars”. We then manually annotate a set of Wikipedia titles which are locked for editing, and evaluate our system on this much larger and more challenging dataset.","To estimate the level of controversy associated with a Wikipedia concept, we propose to simply examine the words in the sentences in which the concept is referenced. Because a concept can often be found in multiple contexts, the estimation can be seen as reflecting the “general opinion” about it in the corpus. This contrasts previous works, which consider this a binary problem, and employ a complex combination of features extracted from Wikipedia's article contents and inter-references, and more extensively – from the rich edit history thereof.","Analysis of controversy in Wikipedia, online news and social media has attracted considerable attention in recent years. Exploiting the collaborative structure of Wikipedia, estimators of the level of controversy in a Wikipedia article were developed based on the edit-history of the article BIBREF0, BIBREF3. Along these lines, BIBREF4 detect controversy based on mutual reverts, bi-polarity in the collaboration network, and mutually-reinforced scores for editors and articles. Similarly, BIBREF1 classify whether a Wikipedia page is controversial through the combined evaluation of the topically neighboring set of pages.","The major advantages of our estimation scheme are its simplicity and reliance on abundantly accessible features. At the same time, its accuracy is similar to state-of-the-art classifiers, which depend on complex meta-data, and rely on sophisticated - in some cases impractical - algorithmic techniques. Because the features herein are so simple, our estimators are convertible to any corpus, in any language, even of moderate size.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
how was labeling done?,Sample Answer,1908.07491-Introduction-2,1908.07491-Introduction-4,1908.07491-Related work-0,1908.07491-Related work-1,1908.07491-Estimating a concept's controversiality level ::: Controversiality Estimators-3,"Most people would agree, for example, that Global warming is a controversial concept, whereas Summer is not. However, the concept Pollution may be seen as neutral by some, yet controversial by others, who associate it with environmental debates. In other words, different people may have different opinions, potentially driven by different contexts salient in their mind. Yet, as reported in the sequel, an appreciable level of agreement can be reached, even without explicit context.","To estimate the level of controversy associated with a Wikipedia concept, we propose to simply examine the words in the sentences in which the concept is referenced. Because a concept can often be found in multiple contexts, the estimation can be seen as reflecting the “general opinion” about it in the corpus. This contrasts previous works, which consider this a binary problem, and employ a complex combination of features extracted from Wikipedia's article contents and inter-references, and more extensively – from the rich edit history thereof.","Analysis of controversy in Wikipedia, online news and social media has attracted considerable attention in recent years. Exploiting the collaborative structure of Wikipedia, estimators of the level of controversy in a Wikipedia article were developed based on the edit-history of the article BIBREF0, BIBREF3. Along these lines, BIBREF4 detect controversy based on mutual reverts, bi-polarity in the collaboration network, and mutually-reinforced scores for editors and articles. Similarly, BIBREF1 classify whether a Wikipedia page is controversial through the combined evaluation of the topically neighboring set of pages.","Content analysis of controversial Wikipedia articles has been used to evaluate the level of controversy of other documents (e.g., web pages) by mapping them to related Wikipedia articles BIBREF5. BIBREF6 further build a language model, which enhances predictions made by existing classifiers, by inferring word probabilities from Wikipedia articles prominent in Wikipedia controversy features (mainly signals in edit history as discussed above) and from articles retrieved by manually selected query terms, believed to indicate controversy.","Recurrent neural network (RNN): A bidirectional RNN using the architecture suggested in BIBREF10 was similarly trained. The network receives as input a concept and a referring sentence, and outputs a score. The controversiality score of a concept is defined, as above, to be the average of these scores.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
where does their dataset come from?,Sample Answer,1908.07491-Introduction-2,1908.07491-Related work-1,1908.07491-Estimating a concept's controversiality level ::: Validation ::: Leave one category out-0,1908.07491-Estimating a concept's controversiality level ::: Validation ::: Leave one category out-2,1908.07491-Results-1,"Most people would agree, for example, that Global warming is a controversial concept, whereas Summer is not. However, the concept Pollution may be seen as neutral by some, yet controversial by others, who associate it with environmental debates. In other words, different people may have different opinions, potentially driven by different contexts salient in their mind. Yet, as reported in the sequel, an appreciable level of agreement can be reached, even without explicit context.","Content analysis of controversial Wikipedia articles has been used to evaluate the level of controversy of other documents (e.g., web pages) by mapping them to related Wikipedia articles BIBREF5. BIBREF6 further build a language model, which enhances predictions made by existing classifiers, by inferring word probabilities from Wikipedia articles prominent in Wikipedia controversy features (mainly signals in edit history as discussed above) and from articles retrieved by manually selected query terms, believed to indicate controversy.","In a preliminary task, we looked for words which may designate sentences associated with controversial concepts. To this end, we ranked the words appearing in positive sentences according to their information gain for this task. The top of the list comprises the following: that, sexual, people, movement, religious, issues, rights.","To control for this effect, we performed a second experiment where we set the concepts from one category as the test set, and used the others for training (concepts associated with the excluded category are left out, regardless of whether they are also associated with one of the training categories). We did this for 5 categories: History, Politics and economics, Religion, Science, and Sexuality. This way, thematic relatedness observed in the training set should have little or no effect on correctly estimating the level of controversy associated of concepts in the test set, and may even “mislead” the estimator. We note that previous work on controversiality does not seem to address this issue, probably because the meta-data used is less sensitive to it.","BIBREF4 review several controversy classifiers. The most accurate one, the Structure classifier, builds, among others, collaboration networks by considering high-level behavior of editors both in their individual forms, and their pairwise interactions. A collaboration profile containing these individual and pairwise features is built for each two interacting editors and is classified based on the agreement or disagreement relation between them. This intensive computation renders that classifier impractical. Table TABREF14 therefore also includes the most accurate classifier BIBREF4 consider practical.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what are the baselines?,Sample Answer,1908.07491-Introduction-0,1908.07491-Introduction-4,1908.07491-Related work-0,1908.07491-Estimating a concept's controversiality level ::: Validation ::: Leave one category out-2,1908.07491-Conclusions-0,"Indicating that a web page is controversial, or disputed - for example, in a search result - facilitates an educated consumption of the information therein, suggesting the content may not represent the “full picture”. Here, we consider the problem of estimating the level of controversiality associated with a given Wikipedia concept (title). We demonstrate that the textual contexts in which the concept is referenced can be leveraged to facilitate this.","To estimate the level of controversy associated with a Wikipedia concept, we propose to simply examine the words in the sentences in which the concept is referenced. Because a concept can often be found in multiple contexts, the estimation can be seen as reflecting the “general opinion” about it in the corpus. This contrasts previous works, which consider this a binary problem, and employ a complex combination of features extracted from Wikipedia's article contents and inter-references, and more extensively – from the rich edit history thereof.","Analysis of controversy in Wikipedia, online news and social media has attracted considerable attention in recent years. Exploiting the collaborative structure of Wikipedia, estimators of the level of controversy in a Wikipedia article were developed based on the edit-history of the article BIBREF0, BIBREF3. Along these lines, BIBREF4 detect controversy based on mutual reverts, bi-polarity in the collaboration network, and mutually-reinforced scores for editors and articles. Similarly, BIBREF1 classify whether a Wikipedia page is controversial through the combined evaluation of the topically neighboring set of pages.","To control for this effect, we performed a second experiment where we set the concepts from one category as the test set, and used the others for training (concepts associated with the excluded category are left out, regardless of whether they are also associated with one of the training categories). We did this for 5 categories: History, Politics and economics, Religion, Science, and Sexuality. This way, thematic relatedness observed in the training set should have little or no effect on correctly estimating the level of controversy associated of concepts in the test set, and may even “mislead” the estimator. We note that previous work on controversiality does not seem to address this issue, probably because the meta-data used is less sensitive to it.","We demonstrated that the sentence–level context in which a concept appears is indicative of its controversiality. This follows BIBREF10, who show this for concept abstractness and suggest to explore further properties identifiable in this way. Importantly, we observed that this method may pick up signals which are not directly related to the property of interest. For example, since many controversial concepts have to do with religion, part of what this method may learn is thematic relatedness to religion. However, when controlling for this effect, the drop in accuracy is fairly small.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How does lattice rescoring improve inference?,Sample Answer,2004.04498-Introduction-1,2004.04498-Gender bias in machine translation-0,2004.04498-Gender bias in machine translation-11,2004.04498-Gender bias in machine translation ::: WinoMT challenge set and metrics-9,2004.04498-4-Figure1-1.png,"Gender bias is a particularly important problem for Neural Machine Translation (NMT) into gender-inflected languages. An over-prevalence of some gendered forms in the training data leads to translations with identifiable errors BIBREF0. Translations are better for sentences involving men and for sentences containing stereotypical gender roles. For example, mentions of male doctors are more reliably translated than those of male nurses BIBREF2, BIBREF4.","We focus on translating coreference sentences containing professions as a representative subset of the gender bias problem. This follows much recent work on NLP gender bias BIBREF19, BIBREF5, BIBREF6 including the release of WinoMT, a relevant challenge set for NMT BIBREF0.","This would likely be translated with a masculine entity according to the conventions of a language, unless extra-sentential context was available. As well, some languages have adopted gender-neutral singular pronouns and profession terms, both to include non-binary people and to avoid the social biases of gendered language BIBREF20, although most languages lack widely-accepted conventions BIBREF21. This paper addresses gender bias that can be resolved at the sentence level and evaluated with existing test sets, and does not address these broader challenges.","Finally, we wish to reduce gender bias without reducing translation performance. We report BLEU BIBREF22 on separate, general test sets for each language pair. WinoMT is designed to work without target language references, and so it is not possible to measure translation performance on this set by measures such as BLEU.","Figure 1: Generating counterfactual datasets for adaptation. The Original set is 1||2, a simple subset of the full dataset. FTrans original is 1||3, FTrans swapped is 4||5, and Balanced is 1,4||2,5",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
"How is the set of trusted, gender-balanced examples selected?",Sample Answer,2004.04498-Gender bias in machine translation ::: Gender debiased datasets ::: Counterfactual datasets-1,2004.04498-Experiments ::: Languages and data-6,2004.04498-Experiments ::: Results ::: Handcrafted profession set adaptation-0,2004.04498-Experiments ::: Results ::: Handcrafted profession set adaptation-1,2004.04498-Experiments ::: Results ::: Lattice rescoring with debiased models-0,"While counterfactual data augmentation is relatively simple for sentences in English, the process for inflected languages is challenging, involving identifying and updating words that are co-referent with all gendered entities in a sentence. Gender-swapping MT training data additionally requires that the same entities are swapped in the corresponding parallel sentence. A robust scheme for gender-swapping multiple entities in inflected language sentences directly, together with corresponding parallel text, is beyond the scope of this paper. Instead we suggest a rough but straightforward approach for counterfactual data augmentation for NMT which to the best of our knowledge is the first application to parallel sentences.","For en-de and en-es we learn joint 32K BPE vocabularies on the training data BIBREF33. For en-he we use separate source and target vocabularies. The Hebrew vocabulary is a 2k-merge BPE vocabulary, following the recommendations of BIBREF34 for smaller vocabularies when translating into lower-resource languages. For the en-he source vocabulary we experimented both with learning a new 32K vocabulary and with reusing the joint BPE vocabulary trained on the largest set – en-de – which lets us initialize the en-he system with the pre-trained en-de model. The latter resulted in higher BLEU and faster training.","Results for fine-tuning on the handcrafted set are given in lines 3-6 of Table TABREF40. These experiments take place in minutes on a single GPU, compared to several hours when fine-tuning on the counterfactual sets and far longer if training from scratch.","Fine-tuning on the handcrafted sets gives a much faster BLEU drop than fine-tuning on counterfactual sets. This is unsurprising since the handcrafted sets are domains of new sentences with consistent sentence length and structure. By contrast the counterfactual sets are less repetitive and close to subsets of the original training data, slowing forgetting. We believe the degradation here is limited only by the ease of fitting the small handcrafted sets.","In lines 7-9 of Table TABREF40 we consider lattice-rescoring the baseline output, using three models debiased on the handcrafted data.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the baseline method for the task?,Sample Answer,1909.02764-Introduction-0,1909.02764-Introduction-3,1909.02764-Introduction-5,1909.02764-Results ::: Facial Expressions and Audio-0,1909.02764-Summary & Future Work-1,"Automatic emotion recognition is commonly understood as the task of assigning an emotion to a predefined instance, for example an utterance (as audio signal), an image (for instance with a depicted face), or a textual unit (e.g., a transcribed utterance, a sentence, or a Tweet). The set of emotions is often following the original definition by Ekman Ekman1992, which includes anger, fear, disgust, sadness, joy, and surprise, or the extension by Plutchik Plutchik1980 who adds trust and anticipation.","Also from the application point of view, the domain is a relevant choice: Past research has shown that emotional intelligence is beneficial for human computer interaction. Properly processing emotions in interactions increases the engagement of users and can improve performance when a specific task is to be fulfilled BIBREF1, BIBREF2, BIBREF3, BIBREF4. This is mostly based on the aspect that machines communicating with humans appear to be more trustworthy when they show empathy and are perceived as being natural BIBREF3, BIBREF5, BIBREF4.","With this paper, we investigate how each of the three considered modalitites, namely facial expressions, utterances of a driver as an audio signal, and transcribed text contributes to the task of emotion recognition in in-car speech interactions. We focus on the five emotions of joy, insecurity, annoyance, relaxation, and boredom since terms corresponding to so-called fundamental emotions like fear have been shown to be associated to too strong emotional states than being appropriate for the in-car context BIBREF8. Our first contribution is the description of the experimental setup for our data collection. Aiming to provoke specific emotions with situations which can occur in real-world driving scenarios and to induce speech interactions, the study was conducted in a driving simulator. Based on the collected data, we provide baseline predictions with off-the-shelf tools for face and speech emotion recognition and compare them to a neural network-based approach for emotion recognition from text. Our second contribution is the introduction of transfer learning to adapt models trained on established out-of-domain corpora to our use case. We work on German language, therefore the transfer consists of a domain and a language transfer.","Table TABREF16 shows the confusion matrices for facial and audio emotion recognition on our complete AMMER data set and Table TABREF17 shows the results per class for each method, including facial and audio data and micro and macro averages. The classification from facial expressions yields a macro-averaged $\text{F}_1$ score of 33 % across the three emotions joy, insecurity, and annoyance (P=0.31, R=0.35). While the classification results for joy are promising (R=43 %, P=57 %), the distinction of insecurity and annoyance from the other classes appears to be more challenging.","Our results for facial expressions indicate that there is potential for the classification of joy, however, the states of annoyance and insecurity are not well recognized. Future work needs to investigate more sophisticated approaches to map frame predictions to sequence predictions. Furthermore, movements of the mouth region during speech interactions might negatively influence the classification from facial expressions. Therefore, the question remains how facial expressions can best contribute to multimodal detection in speech interactions.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What data were they used to train the multilingual encoder?,Sample Answer,1809.04686-Corpora-0,1809.04686-Corpora-1,1809.04686-Corpora-2,1809.04686-Zero-Shot Classification Results-0,1809.04686-5-Table3-1.png,"We evaluate the proposed method on three common NLP tasks: Amazon Reviews, SST and SNLI. We utilize parallel data to train our multilingual NMT system, as detailed below.","For the MT task, we use the WMT 2014 En $\leftrightarrow $ Fr parallel corpus. The dataset contains 36 million En $\rightarrow $ Fr sentence pairs. We swapped the source and target sentences to obtain parallel data for the Fr $\rightarrow $ En translation task. We use these two datasets (72 million sentence pairs) to train a single multilingual NMT model to learn both these translation directions simultaneously. We generated a shared sub-word vocabulary BIBREF37 , BIBREF38 of 32K units from all source and target training data. We use this sub-word vocabulary for all of our experiments below.","The Amazon reviews dataset BIBREF39 is a multilingual sentiment classification dataset, providing data for four languages - English (En), French (Fr), German (De), and Japanese. We use the English and French datasets in our experiments. The dataset contains 6,000 documents in the train and test portions for each language. Each review consists of a category label, a title, a review, and a star rating (5-point scale). We only use the review text in our experiments. Following BIBREF39 , we mapped the reviews with lower scores (1 and 2) to negative examples and the reviews with higher scores (4 and 5) to positive examples, thereby turning it into a binary classification problem. Reviews with score 3 are dropped. We split the training dataset into 10% for development and the rest for training, and we truncate each example and keep the first 200 words in the review. Note that, since the data for each language was obtained by crawling different product pages, the data is not aligned across languages.","In this section, we explore the zero-shot classification task in French for our systems. We assume that we do not have any French training data for all the three tasks and test how well our proposed method can generalize to the unseen French language without any further training. Specifically, we reuse the three proposed systems from Table 1 after being trained only on the English classification task and test the systems on data from an unseen language (e.g. French). A reasonable upper bound to which zero-shot performance should be compared to is bridging - translating a French test text to English and then applying the English classifier on the translated text. If we assume the translation to be perfect, we should expect this approach to perform as well as the English classifier.",Table 3: Comparison of our best zero-shot result on the French SNLI test set to other baselines. See text for details.,1.0,1.0,1.0,1.0,1.0,0.33333333333333337,1.0,0.2
How do the authors define or exemplify 'incorrect words'?,Sample Answer,2001.00137-Proposed model-3,2001.00137-Proposed model-4,2001.00137-Proposed model-8,2001.00137-Dataset ::: Intent Classification from Text with STT Error-3,2001.00137-Experiments ::: Training specifications ::: Semantic hashing with classifier-0,"The stacks of multilayer perceptrons are structured as two sets of three layers with two hidden layers each. The first set is responsible for compressing the $h_{inc}$ into a latent-space representation, extracting more abstract features into lower dimension vectors $z_1$, $z_2$ and $\mathbf {z}$ with shape $(N_{bs}, 128, 128)$, $(N_{bs}, 32, 128)$, and $(N_{bs}, 12, 128)$, respectively. This process is shown in Eq. (DISPLAY_FORM5):","where $f(\cdot )$ is the parameterized function mapping $h_{inc}$ to the hidden state $\mathbf {z}$. The second set then respectively reconstructs $z_1$, $z_2$ and $\mathbf {z}$ into $h_{rec_1}$, $h_{rec_2}$ and $h_{rec}$. This process is shown in Eq. (DISPLAY_FORM6):","Classification is done with a feedforward network and softmax activation function. Softmax $\sigma $ is a discrete probability distribution function for $N_C$ classes, with the sum of the classes probability being 1 and the maximum value being the predicted class. The predicted class can be mathematically calculated as in Eq. (DISPLAY_FORM8):","Table TABREF24 exemplifies a complete and its respective incomplete sentences with different TTS-STT combinations, thus varying rates of missing and incorrect words. The level of noise in the STT imbued sentences is denoted by a inverted BLEU (iBLEU) score ranging from 0 to 1. The inverted BLEU score is denoted in Eq. (DISPLAY_FORM23):","Trained on 3-gram, feature vector size of 768 as to match the BERT embedding size, and 13 classifiers with parameters set as specified in the authors' paper so as to allow comparison: MLP with 3 hidden layers of sizes $[300, 100, 50]$ respectively; Random Forest with 50 estimators or trees; 5-fold Grid Search with Random Forest classifier and estimator $([50, 60, 70]$; Linear Support Vector Classifier with L1 and L2 penalty and tolerance of $10^{-3}$; Regularized linear classifier with Stochastic Gradient Descent (SGD) learning with regularization term $alpha=10^{-4}$ and L1, L2 and Elastic-Net penalty; Nearest Centroid with Euclidian metric, where classification is done by representing each class with a centroid; Bernoulli Naive Bayes with smoothing parameter $alpha=10^{-2}$; K-means clustering with 2 clusters and L2 penalty; and Logistic Regression classifier with L2 penalty, tolerance of $10^{-4}$ and regularization term of $1.0$. Most often, the best performing classifier was MLP.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
By how much do they outperform other models in the sentiment in intent classification tasks?,Sample Answer,2001.00137-Introduction-4,2001.00137-Dataset ::: Intent Classification from Text with STT Error-4,2001.00137-Experiments ::: Training specifications ::: NLU service platforms-0,2001.00137-Experiments ::: Results on Intent Classification from Text with STT Error-1,2001.00137-Experiments ::: Results on Intent Classification from Text with STT Error-2,"The incomplete data problem is usually approached as a reconstruction or imputation task and is most often related to missing numbers imputation BIBREF13. Vincent et al. BIBREF14, BIBREF15 propose to reconstruct clean data from its noisy version by mapping the input to meaningful representations. This approach has also been shown to outperform other models, such as predictive mean matching, random forest, Support Vector Machine (SVM) and Multiple imputation by Chained Equations (MICE), at missing data imputation tasks BIBREF16, BIBREF17. Researchers in those two areas have shown that meaningful feature representation of data is of utter importance for high performance achieving methods. We propose a model that combines the power of BERT in the NLP domain and the strength of denoising strategies in incomplete data reconstruction to tackle the tasks of incomplete intent and sentiment classification. This enables the implementation of a novel encoding scheme, more robust to incomplete data, called Stacked Denoising BERT or Stacked DeBERT. Our approach consists of obtaining richer input representations from input tokens by stacking denoising transformers on an embedding layer with vanilla transformers. The embedding layer and vanilla transformers extract intermediate input features from the input tokens, and the denoising transformers are responsible for obtaining richer input representations from them. By improving BERT with stronger denoising abilities, we are able to reconstruct missing and incorrect words' embeddings and improve classification accuracy. To summarize, our contribution is two-fold:","where BLEU is a common metric usually used in machine translation tasks BIBREF21. We decide to showcase that instead of regular BLEU because it is more indicative to the amount of noise in the incomplete text, where the higher the iBLEU, the higher the noise.",No settable training configurations available in the online platforms.,"The table also indicates the level of noise in each dataset with the already mentioned iBLEU score, where 0 means no noise and higher values mean higher quantity of noise. As expected, the models' accuracy degrade with the increase in noise, thus F1-scores of gtts-witai are higher than macsay-witai. However, while the other models decay rapidly in the presence of noise, our model does not only outperform them but does so with a wider margin. This is shown with the increasing robustness curve in Fig. FIGREF41 and can be demonstrated by macsay-witai outperforming the baseline models by twice the gap achieved by gtts-witai.","Further analysis of the results in Table TABREF40 show that, BERT decay is almost constant with the addition of noise, with the difference between the complete data and gtts-witai being 1.88 and gtts-witai and macsay-witai being 1.89. Whereas in Stacked DeBERT, that difference is 1.89 and 0.94 respectively. This is stronger indication of our model's robustness in the presence of noise.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Is the dataset used in other work?,Sample Answer,1902.06734-Hate speech detection-0,1902.06734-Hate speech detection-1,1902.06734-Hate speech detection-2,1902.06734-Representing authors-4,1902.06734-Results-5,"Amongst the first ones to apply supervised learning to the task of hate speech detection were Yin et al. Yin09detectionof who used a linear svm classifier to identify posts containing harassment based on local (e.g., n-grams), contextual (e.g., similarity of a post to its neighboring posts) and sentiment-based (e.g., presence of expletives) features. Their best results were with all of these features combined.","Djuric et al. Djuric:2015:HSD:2740908.2742760 experimented with comments extracted from the Yahoo Finance portal and showed that distributional representations of comments learned using paragraph2vec BIBREF11 outperform simpler bag-of-words (bow) representations in a supervised classification setting for hate speech detection. Nobata et al. Nobata:2016:ALD:2872427.2883062 improved upon the results of Djuric et al. by training their classifier on a combination of features drawn from four different categories: linguistic (e.g., count of insult words), syntactic (e.g., pos tags), distributional semantic (e.g., word and comment embeddings) and bow-based (word and characters n-grams). They reported that while the best results were obtained with all features combined, character n-grams contributed more to performance than all the other features.","Waseem and Hovy c53cecce142c48628b3883d13155261c created and experimented with a dataset of racist, sexist and clean tweets. Utilizing a logistic regression (lr) classifier to distinguish amongst them, they found that character n-grams coupled with gender information of users formed the optimal feature set; on the other hand, geographic and word-length distribution features provided little to no improvement. Working with the same dataset, Badjatiya et al. Badjatiya:17 improved on their results by training a gradient-boosted decision tree (gbdt) classifier on averaged word embeddings learnt using a long short-term memory (lstm) network that they initialized with random embeddings.","In doing so, the framework learns low-dimensional embeddings for nodes in the graph. These embeddings can emphasize either their structural role or the local community they are a part of. This depends on the sampling strategies used to generate the neighborhood: if breadth-first sampling (bfs) is adopted, the model focuses on the immediate neighbors of a node; when depth-first sampling (dfs) is used, the model explores farther regions in the network, which results in embeddings that encode more information about the nodes' structural role (e.g., hub in a cluster, or peripheral node). The balance between these two ways of sampling the neighbors is directly controlled by two node2vec parameters, namely $p$ and $q$ . The default value for these is 1, which ensures a node representation that gives equal weight to both structural and community-oriented information. In our work, we use the default value for both $p$ and $q$ . Additionally, since node2vec does not produce embeddings for solitary authors, we map these to a single zero embedding.","The author profiling features on their own (auth) achieve impressive results overall and in particular on the sexism class, where their performance is typical of a community-based generalization, i.e., low precision but high recall. For the racism class on the other hand, the performance of auth on its own is quite poor. This contrast can be explained by the fact that tweets in the racism class come from only 5 unique authors who: (i) are isolated in the community graph, or (ii) have also authored several tweets in the sexism class, or (iii) are densely connected to authors from the sexism and none classes which possibly camouflages their racist nature.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the drawback to methods that rely on textual cues?,Sample Answer,1902.06734-Introduction-2,1902.06734-Introduction-3,1902.06734-Representing authors-0,1902.06734-Representing authors-4,1902.06734-Results-0,"Several approaches to hate speech detection demonstrate the effectiveness of character-level bag-of-words features in a supervised classification setting BIBREF4 , BIBREF5 , BIBREF6 . More recent approaches, and currently the best performing ones, utilize recurrent neural networks (rnns) to transform content into dense low-dimensional semantic representations that are then used for classification BIBREF1 , BIBREF7 . All of these approaches rely solely on lexical and semantic features of the text they are applied to. Waseem and Hovy c53cecce142c48628b3883d13155261c adopted a more user-centric approach based on the idea that perpetrators of hate speech are usually segregated into small demographic groups; they went on to show that gender information of authors (i.e., users who have posted content) is a helpful indicator. However, Waseem and Hovy focused only on coarse demographic features of the users, disregarding information about their communication with others. But previous research suggests that users who subscribe to particular stereotypes that promote hate speech tend to form communities online. For example, Zook zook mapped the locations of racist tweets in response to President Obama's re-election to show that such tweets were not uniformly distributed across the United States but formed clusters instead. In this paper, we present the first approach to hate speech detection that leverages author profiling information based on properties of the authors' social network and investigate its effectiveness.","Author profiling has emerged as a powerful tool for NLP applications, leading to substantial performance improvements in several downstream tasks, such as text classification, sentiment analysis and author attribute identification BIBREF8 , BIBREF9 , BIBREF10 . The relevance of information gained from it is best explained by the idea of homophily, i.e., the phenomenon that people, both in real life as well as on the Internet, tend to associate more with those who appear similar. Here, similarity can be defined along various axes, e.g., location, age, language, etc. The strength of author profiling lies in that if we have information about members of a community $c$ defined by some similarity criterion, and we know that the person $p$ belongs to $c$ , we can infer information about $p$ . This concept has a straightforward application to our task: knowing that members of a particular community are prone to creating hateful content, and knowing that the author p is connected to this community, we can leverage information beyond linguistic cues and more accurately predict the use of hateful/non-hateful language from $p$ . The questions that we seek to address here are: are some authors, and the respective communities that they belong to, more hateful than the others? And can such information be effectively utilized to improve the performance of automated hate speech detection methods?","In order to leverage community-based information for the authors whose tweets form our dataset, we create an undirected unlabeled community graph wherein nodes are the authors and edges are the connections between them. An edge is instantiated between two authors $u$ and $v$ if $u$ follows $v$ on Twitter or vice versa. There are a total of 1,836 nodes and 7,561 edges. Approximately 400 of the nodes have no edges, indicating solitary authors who neither follow any other author nor are followed by any. Other nodes have an average degree of 8, with close to 600 of them having a degree of at least 5. The graph is overall sparse with a density of 0.0075.","In doing so, the framework learns low-dimensional embeddings for nodes in the graph. These embeddings can emphasize either their structural role or the local community they are a part of. This depends on the sampling strategies used to generate the neighborhood: if breadth-first sampling (bfs) is adopted, the model focuses on the immediate neighbors of a node; when depth-first sampling (dfs) is used, the model explores farther regions in the network, which results in embeddings that encode more information about the nodes' structural role (e.g., hub in a cluster, or peripheral node). The balance between these two ways of sampling the neighbors is directly controlled by two node2vec parameters, namely $p$ and $q$ . The default value for these is 1, which ensures a node representation that gives equal weight to both structural and community-oriented information. In our work, we use the default value for both $p$ and $q$ . Additionally, since node2vec does not produce embeddings for solitary authors, we map these to a single zero embedding.","We perform 10-fold stratified cross validation (cv), as suggested by Forman and Scholz Forman:10, to evaluate all seven methods described in the previous section. Following previous research BIBREF7 , BIBREF23 , we report the average weighted precision, recall, and f $_1$ scores for all the methods. The average weighted precision is calculated as: ",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What community-based profiling features are used?,Sample Answer,1902.06734-Hate speech detection-0,1902.06734-Hate speech detection-3,1902.06734-Dataset-2,1902.06734-Classifying content-1,1902.06734-8-Figure2-1.png,"Amongst the first ones to apply supervised learning to the task of hate speech detection were Yin et al. Yin09detectionof who used a linear svm classifier to identify posts containing harassment based on local (e.g., n-grams), contextual (e.g., similarity of a post to its neighboring posts) and sentiment-based (e.g., presence of expletives) features. Their best results were with all of these features combined.","Waseem zeerakW16-5618 sampled $7k$ more tweets in the same manner as Waseem and Hovy c53cecce142c48628b3883d13155261c. They recruited expert and amateur annotators to annotate the tweets as racism, sexism, both or neither in order to study the influence of annotator knowledge on the task of hate speech detection. Combining this dataset with that of Waseem and Hovy c53cecce142c48628b3883d13155261c, Park et al. W17-3006 explored the merits of a two-step classification process. They first used a lr classifier to separate hateful and non-hateful tweets, followed by another lr classifier to distinguish between racist and sexist ones. They showed that this setup had comparable performance to a one-step classification setup built with convolutional neural networks.","We were able to extract community-based information for 1,836 out of the 1,875 unique authors who posted the $16,202$ tweets, covering a cumulative of 16,124 of them; the remaining 39 authors have either deactivated their accounts or are facing suspension. Tweets in the racism class are from 5 of the 1,875 authors, while those in the sexism class are from 527 of them.","Char n-grams (lr). As our first baseline, we adopt the method used by Waseem and Hovy c53cecce142c48628b3883d13155261c wherein they train a logistic regression (lr) classifier on the Twitter dataset using character n-gram counts. We use uni-grams, bi-grams, tri-grams and four-grams, and l $_2$ -normalize their counts. Character n-grams have been shown to be effective for the task of hate speech detection BIBREF5 .",Figure 2: Visualization of author embeddings in 2-dimensional space.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is their definition of hate speech?,Sample Answer,1908.11049-Dataset ::: Annotation Process ::: Avoiding scams-1,1908.11049-Dataset ::: Final Dataset ::: Target attribute-0,1908.11049-Experiments ::: Results and Analysis ::: STSL-0,1908.11049-Experiments ::: Results and Analysis ::: STSL-1,1908.11049-Experiments ::: Results and Analysis ::: MTSL-0,"We requested native speakers to annotate the data and chose annotators with good reputation scores (more than 0.90). We informed the annotator in the guidelines, that in case of noticeable patterns of random labeling on a substantial number of tweets, their work will be rejected and we may have to block them. Since the rejection affects the reputation of the annotators and their chances to get new tasks on Amazon Mechanical Turk, well-reputed annotators are usually reliable. We have divided our corpora into smaller batches on Amazon Mechanical Turk in order to facilitate the analysis of the annotations of the workers and, fairly identify any incoherence patterns possibly caused by the use of an automatic translation system on the tweets, or the repetition of the same annotation schema. If we reject the work of a scam, we notify them, then reassign the tasks to other annotators.","After annotating the pilot dataset, we noticed common misconceptions regarding race, ethnicity, and nationality, therefore we merged these attributes into one label origin. Then, we asked the annotators to determine whether the tweet insults or discriminates against people based on their (1) origin, (2) religious affiliation, (3) gender, (4) sexual orientation, (5) special needs or (6) other. Table TABREF20 shows there are fewer tweets targeting disability in Arabic compared to English and French and no tweets insulting people based on their sexual orientation which may be due to the fact that the labels of gender, gender identity, and sexual orientation use almost the same wording. On the other hand, French contains a small number of tweets targeting people based on their gender in comparison to English and Arabic. We have observed significant differences in terms of target attributes in the three languages. More data may help us examine the problems affecting targets of different linguistic backgrounds.","STSL performs the best among all models on the directness classification, and it is also consistent in both micro and macro-F1 scores. This is due to the fact that the directness has only two labels and multilabeling is not allowed in this task. Tasks involving imbalanced data, multiclass and multilabel annotations harm the performance of the directness in multitask settings.","Since macro-F1 is the average of all F1 scores of individual labels, all deep learning models have high macro-F1 scores in English which indicates that they are particularly good at classifying the direct class. STSL is also comparable or better than traditional BOW feature-based classifiers when performed on other tasks in terms of micro-F1 and for most of the macro-F1 scores. This shows the power of the deep learning approach.","Except for the directness, MTSL usually outperforms STSL or is comparable to it. When we jointly train each task on the three languages, the performance decreases in most cases, other than the target group classification tasks. This may be due to the difference in label distributions across languages. Yet, multilingual training of the target group classification task improves in all languages. Since the target group classification task involves 16 labels, the amount of data annotated for each label is lower than in other tasks. Hence, when aggregating annotated data in different languages, the size of the training data also increases, due to the relative regularity of identification words of different groups in all three languages in comparison to other tasks.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How big is their dataset?,Sample Answer,1908.11049-Related Work-0,1908.11049-Dataset ::: Annotation Process-0,1908.11049-Dataset ::: Final Dataset ::: Directness label-0,1908.11049-Dataset ::: Final Dataset ::: Target attribute-0,1908.11049-Experiments ::: Results and Analysis ::: MTML-0,"There is little consensus on the difference between profanity and hate speech and, how to define the latter BIBREF17. As shown in Figure FIGREF11, slurs are not an unequivocal indicator of hate speech and can be part of a non-aggressive conversation, while some of the most offensive comments may come in the form of subtle metaphors or sarcasm BIBREF18. Consequently, there is no existing human annotated vocabulary that explicitly reveals the presence of hate speech, which makes the available hate speech corpora sparse and noisy BIBREF19.","We rely on the general public opinion and common linguistic knowledge to assess how people view and react to hate speech. Given the subjectivity and difficulty of the task, we reminded the annotators not to let their personal opinions about the topics being discussed in the tweets influence their annotation decisions.","Annotators determine the explicitness of the tweet by labeling it as direct or indirect speech. This should be based on whether the target is explicitly named, or less easily discernible, especially if the tweet contains humor, metaphor, or figurative speech. Table TABREF20 shows that even when partly using equivalent keywords to search for candidate tweets, there are still significant differences in the resulting data.","After annotating the pilot dataset, we noticed common misconceptions regarding race, ethnicity, and nationality, therefore we merged these attributes into one label origin. Then, we asked the annotators to determine whether the tweet insults or discriminates against people based on their (1) origin, (2) religious affiliation, (3) gender, (4) sexual orientation, (5) special needs or (6) other. Table TABREF20 shows there are fewer tweets targeting disability in Arabic compared to English and French and no tweets insulting people based on their sexual orientation which may be due to the fact that the labels of gender, gender identity, and sexual orientation use almost the same wording. On the other hand, French contains a small number of tweets targeting people based on their gender in comparison to English and Arabic. We have observed significant differences in terms of target attributes in the three languages. More data may help us examine the problems affecting targets of different linguistic backgrounds.","MTML settings do not lead to a big improvement which may be due to the class imbalance, multilabel tasks, and the difference in the nature of the tasks. In order to inspect which tasks hurt or help one another, we trained multilingual models for pairwise tasks such as (group, target), (hostility, annotator's sentiment), (hostility, target), (hostility, group), (annotator's sentiment, target) and (annotator's sentiment, group). We noticed that when trained jointly, the target attribute slightly improves the performance of the tweet's hostility type classification by 0.03,0.05 and 0.01 better than the best reported scores in English, French, and Arabic, respectively. When target groups and attributes are trained jointly, the macro F-score of the target group classification in Arabic improves by 0.25 and when we train the tweet's hostility type within the annotator's sentiment, we improve the macro F-score of Arabic by 0.02. We believe that we can take advantage of the correlations between target attributes and groups along with other tasks, to set logic rules and develop better multilingual and multitask settings.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Do they focus on Reading Comprehension or multiple choice question answering?,Sample Answer,1912.13337-Dataset Probes and Construction ::: WordNetQA ::: Distractor Generation: @!START@$\textsc {distr}(\tau ^{\prime })$@!END@.-0,1912.13337-Probing Methodology and Modeling ::: Inoculation and Pre-training-3,1912.13337-Results and Findings ::: How well do pre-trained MCQA models do?-0,1912.13337-Results and Findings ::: Can Models Be Effectively Inoculated?-2,1912.13337-4-Figure2-1.png,"An example of how distractors are generated is shown in Figure FIGREF6, which relies on similar principles as above. For each concept $c$, we choose 4 distractor answers that are close in the WordNet semantic space. For example, when constructing hypernymy tests for $c$ from the set hops$(c,\uparrow )$, we build distractors by drawing from $\textsc {hops}(c,\downarrow )$ (and vice versa), as well as from the $\ell $-deep sister family of $c$, defined as follows. The 1-deep sister family is simply $c$'s siblings or sisters, i.e., the other children $\tilde{c} \ne c$ of the parent node $c^{\prime }$ of $c$. For $\ell > 1$, the $\ell $-deep sister family also includes all descendants of each $\tilde{c}$ up to $\ell -1$ levels deep, denoted $\textsc {hops}_{\ell -1}(\tilde{c},\downarrow )$. Formally:","Using this methodology, we can see how much exposure to new data it takes for a given model to master a new task, and whether there are phenomena that stress particular models (e.g., lead to catastrophic forgetting of the original task). Given the restrictions on the number of fine-tuning examples, our assumption is that when models are able to maintain good performance on their original task during inoculation, the quickness with which they are able to learn the inoculated task provides evidence of prior competence, which is precisely what we aim to probe. To measure past performance, we define a model's inoculation cost as the difference in the performance of this model on its original task before and after inoculation.","Science models that use non-transformer based encoders, such as the ESIM model with GloVe and ELMO, perform poorly across all probes, in many cases scoring near random chance, showing limits to how well they generalize from science to other tasks even with pre-trained GloVe and ELMO embeddings. In sharp contrast, the transformer models have mixed results, the most striking result being the RoBERTa models on the definitions and synonymy probes (achieving a test accuracy of 77% and 61%, respectively), which outperform several of the task-specific LSTM models trained directly on the probes. At first glance, this suggests that RoBERTa, which generally far outpaces even BERT across most probes, has high competence of definitions and synonyms even without explicit training on our new tasks.","As shown in Figure FIGREF28, RoBERTa is able to significantly improve performance across most categories even after inoculation with a mere 100 examples (the middle plot), which again provides strong evidence of prior competence. As an example, RoBERTa improves on 2-hop hyponymy inference with random distractors by 18% (from 59% to 77%). After 3k examples, the model has high performance on virtually all categories (the same score increases from 59% to 87%), however results still tends to degrade as a function of hop and distractor complexity, as discussed above.","Figure 2: A portion of the WordNet ISA graph (top) and an example distractor function DISTR(τ) (bottom) used to generate distractor choices {a′1, a′2, a′3} for a question q based on information in the graph.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
After how many hops does accuracy decrease?,Sample Answer,1912.13337-Introduction-4,1912.13337-Dataset Probes and Construction ::: WordNetQA-0,1912.13337-5-Table2-1.png,1912.13337-6-Table4-1.png,1912.13337-11-Table7-1.png,"In this paper, we look at systematically constructing such tests by exploiting the vast amounts of structured information contained in various types of expert knowledge such as knowledge graphs and lexical taxonomies. Our general methodology works as illustrated in Figure FIGREF1: given any MCQA model trained on a set of benchmark tasks, we systematically generate a set of synthetic dataset probes (i.e., MCQA renderings of the target information) from information in expert knowledge sources. We then use these probes to ask two empirical questions: 1) how well do models trained on benchmark tasks perform on these probing tasks and; 2) can such models be re-trained to master new challenges with minimal performance loss on their original tasks?","WordNet is an English lexical database consisting of around 117k concepts, which are organized into groups of synsets that each contain a gloss (i.e., a definition of the target concept), a set of representative English words (called lemmas), and, in around 33k synsets, example sentences. In addition, many synsets have ISA links to other synsets that express complex taxonomic relations. Figure FIGREF6 shows an example and Table TABREF4 summarizes how we formulate WordNet as a set of triples $\mathcal {T}$ of various types. These triples together represent a directed, edge-labeled graph $G$. Our main motivation for using WordNet, as opposed to a resource such as ConceptNet BIBREF36, is the availability of glosses ($\mathcal {D}$) and example sentences ($\mathcal {S}$), which allows us to construct natural language questions that contextualize the types of concepts we want to probe.","Table 2: Details of the GEN(τ) function used to construct gold question-answer pairs (q, a) from a triple graph G.",Table 4: Example dictionary entries for the word gift.,"Table 7: Example questions and answers/inferences (involving ISA reasoning) that illustrate semantic clusters, as well as model predictions (shown as # correct questions/total # questions with perturbations).",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the difficulties in modelling the ironic pattern?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How did the authors find ironic data on twitter?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
"Who judged the irony accuracy, sentiment preservation and content preservation?",Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How do they deal with unknown distribution senses?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How is knowledge stored in the memory?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How many layers does the UTCNN model have?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are the baselines?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Which data do they use as a starting point for the dialogue dataset?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How do they select instances to their hold-out test set?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are their correlation results?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What simpler models do they look at?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What linguistic quality aspects are addressed?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How do this framework facilitate demographic inference from social media?,Sample Answer,1902.06843-Introduction-0,1902.06843-Introduction-4,1902.06843-Data Modality Analysis-19,1902.06843-Multi-modal Prediction Framework-7,1902.06843-Multi-modal Prediction Framework-10,"Depression is a highly prevalent public health challenge and a major cause of disability worldwide. Depression affects 6.7% (i.e., about 16 million) Americans each year . According to the World Mental Health Survey conducted in 17 countries, on average, about 5% of people reported having an episode of depression in 2011 BIBREF0 . Untreated or under-treated clinical depression can lead to suicide and other chronic risky behaviors such as drug or alcohol addiction.","Although depression can affect anyone at any age, signs and triggers of depression vary for different age groups . Depression triggers for children include parental depression, domestic violence, and loss of a pet, friend or family member. For teenagers (ages 12-18), depression may arise from hormonal imbalance, sexuality concerns and rejection by peers. Young adults (ages 19-29) may develop depression due to life transitions, poverty, trauma, and work issues. Adult (ages 30-60) depression triggers include caring simultaneously for children and aging parents, financial burden, work and relationship issues. Senior adults develop depression from common late-life issues, social isolation, major life loses such as the death of a spouse, financial stress and other chronic health problems (e.g., cardiac disease, dementia). Therefore, inferring demographic information while studying depressive behavior from passively sensed social data, can shed better light on the population-level epidemiology of depression.","First person singular words are often seen as indicating interpersonal involvement and their high usage is associated with negative affective states implying nervousness and depression BIBREF66 . Consistent with prior studies, frequency of first person singular for depressed people is significantly higher compared to that of control class. Similarly to BIBREF66 , youngsters tend to use more first-person (e.g. I) and second person singular (e.g. you) pronouns (Figure FIGREF39 -G).","Next, for each leaf node INLINEFORM0 , deriving w.r.t INLINEFORM1 : INLINEFORM2 ",Baselines:,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How is the data annotated?,Sample Answer,1902.06843-Introduction-4,1902.06843-Data Modality Analysis-15,1902.06843-Data Modality Analysis-17,1902.06843-Data Modality Analysis-19,1902.06843-Data Modality Analysis-21,"Although depression can affect anyone at any age, signs and triggers of depression vary for different age groups . Depression triggers for children include parental depression, domestic violence, and loss of a pet, friend or family member. For teenagers (ages 12-18), depression may arise from hormonal imbalance, sexuality concerns and rejection by peers. Young adults (ages 19-29) may develop depression due to life transitions, poverty, trauma, and work issues. Adult (ages 30-60) depression triggers include caring simultaneously for children and aging parents, financial burden, work and relationship issues. Senior adults develop depression from common late-life issues, social isolation, major life loses such as the death of a spouse, financial stress and other chronic health problems (e.g., cardiac disease, dementia). Therefore, inferring demographic information while studying depressive behavior from passively sensed social data, can shed better light on the population-level epidemiology of depression.","Authenticity measures the degree of honesty. Authenticity is often assessed by measuring present tense verbs, 1st person singular pronouns (I, me, my), and by examining the linguistic manifestations of false stories BIBREF70 . Liars use fewer self-references and fewer complex words. Psychologists often see a child's first successfull lie as a mental growth. There is a decreasing trend of the Authenticity with aging (see Figure FIGREF39 -B.) Authenticity for depressed youngsters is strikingly higher than their control peers. It decreases with age (Figure FIGREF39 -B.)","People with high clout speak more confidently and with certainty, employing more social words with fewer negations (e.g., no, not) and swear words. In general, midlife is relatively stable w.r.t. relationships and work. A recent study shows that age 60 to be best for self-esteem BIBREF71 as people take on managerial roles at work and maintain a satisfying relationship with their spouse. We see the same pattern in our data (see Figure FIGREF39 -C and Table TABREF40 ). Unsurprisingly, lack of confidence (the 6th PHQ-9 symptom) is a distinguishable characteristic of vulnerable users, leading to their lower clout scores, especially among depressed users before middle age (34 years old).","First person singular words are often seen as indicating interpersonal involvement and their high usage is associated with negative affective states implying nervousness and depression BIBREF66 . Consistent with prior studies, frequency of first person singular for depressed people is significantly higher compared to that of control class. Similarly to BIBREF66 , youngsters tend to use more first-person (e.g. I) and second person singular (e.g. you) pronouns (Figure FIGREF39 -G).","Several studies highlighted the use of profanity by young adults has significantly increased over the last decade BIBREF72 . We observed the same pattern in both the depressed and the control classes (Table TABREF40 ), although it's rate is higher for depressed users BIBREF1 . Psychologists have also shown that swearing can indicate that an individual is not a fragmented member of a society. Depressed youngsters, showing higher rate of interpersonal involvement and relationships, have a higher rate of cursing (Figure FIGREF39 -E). Also, Netspeak lexicon measures the frequency of terms such as lol and thx.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Where does the information on individual-level demographics come from?,Sample Answer,1902.06843-Data Modality Analysis-5,1902.06843-Data Modality Analysis-6,1902.06843-Data Modality Analysis-12,1902.06843-Multi-modal Prediction Framework-4,1902.06843-Multi-modal Prediction Framework-5,"Following BIBREF8 's approach, we adopt Ekman's model of six emotions: anger, disgust, fear, joy, sadness and surprise, and use the Face++ API to automatically capture them from the shared images. Positive emotions are joy and surprise, and negative emotions are anger, disgust, fear, and sadness. In general, for each user u in INLINEFORM0 , we process profile/shared images for both the depressed and the control groups with at least one face from the shared images (Table TABREF23 ). For the photos that contain multiple faces, we measure the average emotion.","Figure FIGREF27 illustrates the inter-correlation of these features. Additionally, we observe that emotions gleaned from facial expressions correlated with emotional signals captured from textual content utilizing LIWC. This indicates visual imagery can be harnessed as a complementary channel for measuring online emotional signals.",Thinking Style:,"Next, we adopt an ensemble learning method that integrates the predictive power of multiple learners with two main advantages; its interpretability with respect to the contributions of each feature and its high predictive power. For prediction we have INLINEFORM0 where INLINEFORM1 is a weak learner and INLINEFORM2 denotes the final prediction.","In particular, we optimize the loss function: INLINEFORM0 where INLINEFORM1 incorporates INLINEFORM2 and INLINEFORM3 regularization. In each iteration, the new INLINEFORM4 is obtained by fitting weak learner to the negative gradient of loss function. Particularly, by estimating the loss function with Taylor expansion : INLINEFORM5 where its first expression is constant, the second and the third expressions are first ( INLINEFORM6 ) and second order derivatives ( INLINEFORM7 ) of the loss. INLINEFORM8 ",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the source of the user interaction data? ,Sample Answer,1902.06843-Introduction-0,1902.06843-Introduction-4,1902.06843-Data Modality Analysis-15,1902.06843-Multi-modal Prediction Framework-4,1902.06843-Multi-modal Prediction Framework-5,"Depression is a highly prevalent public health challenge and a major cause of disability worldwide. Depression affects 6.7% (i.e., about 16 million) Americans each year . According to the World Mental Health Survey conducted in 17 countries, on average, about 5% of people reported having an episode of depression in 2011 BIBREF0 . Untreated or under-treated clinical depression can lead to suicide and other chronic risky behaviors such as drug or alcohol addiction.","Although depression can affect anyone at any age, signs and triggers of depression vary for different age groups . Depression triggers for children include parental depression, domestic violence, and loss of a pet, friend or family member. For teenagers (ages 12-18), depression may arise from hormonal imbalance, sexuality concerns and rejection by peers. Young adults (ages 19-29) may develop depression due to life transitions, poverty, trauma, and work issues. Adult (ages 30-60) depression triggers include caring simultaneously for children and aging parents, financial burden, work and relationship issues. Senior adults develop depression from common late-life issues, social isolation, major life loses such as the death of a spouse, financial stress and other chronic health problems (e.g., cardiac disease, dementia). Therefore, inferring demographic information while studying depressive behavior from passively sensed social data, can shed better light on the population-level epidemiology of depression.","Authenticity measures the degree of honesty. Authenticity is often assessed by measuring present tense verbs, 1st person singular pronouns (I, me, my), and by examining the linguistic manifestations of false stories BIBREF70 . Liars use fewer self-references and fewer complex words. Psychologists often see a child's first successfull lie as a mental growth. There is a decreasing trend of the Authenticity with aging (see Figure FIGREF39 -B.) Authenticity for depressed youngsters is strikingly higher than their control peers. It decreases with age (Figure FIGREF39 -B.)","Next, we adopt an ensemble learning method that integrates the predictive power of multiple learners with two main advantages; its interpretability with respect to the contributions of each feature and its high predictive power. For prediction we have INLINEFORM0 where INLINEFORM1 is a weak learner and INLINEFORM2 denotes the final prediction.","In particular, we optimize the loss function: INLINEFORM0 where INLINEFORM1 incorporates INLINEFORM2 and INLINEFORM3 regularization. In each iteration, the new INLINEFORM4 is obtained by fitting weak learner to the negative gradient of loss function. Particularly, by estimating the loss function with Taylor expansion : INLINEFORM5 where its first expression is constant, the second and the third expressions are first ( INLINEFORM6 ) and second order derivatives ( INLINEFORM7 ) of the loss. INLINEFORM8 ",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the source of the textual data? ,Sample Answer,1902.06843-Introduction-0,1902.06843-Introduction-5,1902.06843-Data Modality Analysis-17,1902.06843-Multi-modal Prediction Framework-4,1902.06843-Multi-modal Prediction Framework-5,"Depression is a highly prevalent public health challenge and a major cause of disability worldwide. Depression affects 6.7% (i.e., about 16 million) Americans each year . According to the World Mental Health Survey conducted in 17 countries, on average, about 5% of people reported having an episode of depression in 2011 BIBREF0 . Untreated or under-treated clinical depression can lead to suicide and other chronic risky behaviors such as drug or alcohol addiction.","The recent advancements in deep neural networks, specifically for image analysis task, can lead to determining demographic features such as age and gender BIBREF13 . We show that by determining and integrating heterogeneous set of features from different modalities – aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement – we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users.","People with high clout speak more confidently and with certainty, employing more social words with fewer negations (e.g., no, not) and swear words. In general, midlife is relatively stable w.r.t. relationships and work. A recent study shows that age 60 to be best for self-esteem BIBREF71 as people take on managerial roles at work and maintain a satisfying relationship with their spouse. We see the same pattern in our data (see Figure FIGREF39 -C and Table TABREF40 ). Unsurprisingly, lack of confidence (the 6th PHQ-9 symptom) is a distinguishable characteristic of vulnerable users, leading to their lower clout scores, especially among depressed users before middle age (34 years old).","Next, we adopt an ensemble learning method that integrates the predictive power of multiple learners with two main advantages; its interpretability with respect to the contributions of each feature and its high predictive power. For prediction we have INLINEFORM0 where INLINEFORM1 is a weak learner and INLINEFORM2 denotes the final prediction.","In particular, we optimize the loss function: INLINEFORM0 where INLINEFORM1 incorporates INLINEFORM2 and INLINEFORM3 regularization. In each iteration, the new INLINEFORM4 is obtained by fitting weak learner to the negative gradient of loss function. Particularly, by estimating the loss function with Taylor expansion : INLINEFORM5 where its first expression is constant, the second and the third expressions are first ( INLINEFORM6 ) and second order derivatives ( INLINEFORM7 ) of the loss. INLINEFORM8 ",1.0,1.0,1.0,1.0,1.0,0.33333333333333337,1.0,0.2
What is the source of the visual data? ,Sample Answer,1902.06843-Data Modality Analysis-17,1902.06843-Data Modality Analysis-19,1902.06843-Data Modality Analysis-21,1902.06843-Multi-modal Prediction Framework-4,1902.06843-Multi-modal Prediction Framework-5,"People with high clout speak more confidently and with certainty, employing more social words with fewer negations (e.g., no, not) and swear words. In general, midlife is relatively stable w.r.t. relationships and work. A recent study shows that age 60 to be best for self-esteem BIBREF71 as people take on managerial roles at work and maintain a satisfying relationship with their spouse. We see the same pattern in our data (see Figure FIGREF39 -C and Table TABREF40 ). Unsurprisingly, lack of confidence (the 6th PHQ-9 symptom) is a distinguishable characteristic of vulnerable users, leading to their lower clout scores, especially among depressed users before middle age (34 years old).","First person singular words are often seen as indicating interpersonal involvement and their high usage is associated with negative affective states implying nervousness and depression BIBREF66 . Consistent with prior studies, frequency of first person singular for depressed people is significantly higher compared to that of control class. Similarly to BIBREF66 , youngsters tend to use more first-person (e.g. I) and second person singular (e.g. you) pronouns (Figure FIGREF39 -G).","Several studies highlighted the use of profanity by young adults has significantly increased over the last decade BIBREF72 . We observed the same pattern in both the depressed and the control classes (Table TABREF40 ), although it's rate is higher for depressed users BIBREF1 . Psychologists have also shown that swearing can indicate that an individual is not a fragmented member of a society. Depressed youngsters, showing higher rate of interpersonal involvement and relationships, have a higher rate of cursing (Figure FIGREF39 -E). Also, Netspeak lexicon measures the frequency of terms such as lol and thx.","Next, we adopt an ensemble learning method that integrates the predictive power of multiple learners with two main advantages; its interpretability with respect to the contributions of each feature and its high predictive power. For prediction we have INLINEFORM0 where INLINEFORM1 is a weak learner and INLINEFORM2 denotes the final prediction.","In particular, we optimize the loss function: INLINEFORM0 where INLINEFORM1 incorporates INLINEFORM2 and INLINEFORM3 regularization. In each iteration, the new INLINEFORM4 is obtained by fitting weak learner to the negative gradient of loss function. Particularly, by estimating the loss function with Taylor expansion : INLINEFORM5 where its first expression is constant, the second and the third expressions are first ( INLINEFORM6 ) and second order derivatives ( INLINEFORM7 ) of the loss. INLINEFORM8 ",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What modern MRC gold standards are analyzed?,Sample Answer,2003.04642-Framework for MRC Gold Standard Analysis ::: Problem definition-3,2003.04642-Framework for MRC Gold Standard Analysis ::: Dimensions of Interest ::: Required Reasoning-1,2003.04642-Framework for MRC Gold Standard Analysis ::: Dimensions of Interest ::: Required Reasoning-4,2003.04642-1-Figure1-1.png,2003.04642-18-Table8-1.png,"Span, where is a continuous subsequence of tokens from the paragraph ($A \subseteq P$). Flavours include multiple spans as the correct answer or $A \subseteq Q$.","We further extend the reasoning categories by operational logic, similar to those required in semantic parsing tasks BIBREF21, as solving those tasks typically requires “multi-hop” reasoning BIBREF14, BIBREF22. When an answer can only be obtained by combining information from different sentences joined by mentioning a common entity, concept, date, fact or event (from here on called entity), we annotate it as Bridge. We further annotate the cases, when the answer is a concrete entity that satisfies a Constraint specified in the question, when it is required to draw a Comparison of multiple entities' properties or when the expected answer is an Intersection of their properties (e.g. “What do Person A and Person B have in common?”)","An example can exhibit multiple forms of reasoning. Notably, we do not annotate any of the categories mentioned above if the expected answer is directly stated in the passage. For example, if the question asks “How many total points were scored in the game?” and the passage contains a sentence similar to “The total score of the game was 51 points”, it does not require any reasoning, in which case we annotate it as Retrieval.","Figure 1: While initially this looks like a complex question that requires the synthesis of different information across multiple documents, the keyword “2010” appears in the question and only in the sentence that answers it, considerably simplifying the search. Full example with 10 passages can be seen in Appendix D.","Table 8: Detailed reasoning results. We calculate percentages relative to the number of examples that are not unanswerable, i.e. require reasoning to obtain the answer according to our definition.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How many layers of self-attention does the model have?,Sample Answer,1909.05246-Related Work ::: Task-Oriented Chatbots Architectures-0,1909.05246-Models ::: LSTM and Bi-Directional LSTM-1,1909.05246-Experiments ::: Inference-0,1909.05246-Experiments ::: Evaluation Measures-1,1909.05246-Experiments ::: Evaluation Measures-3,"End-to-end architectures are among the most used architectures for research in the field of conversational AI. The advantage of using an end-to-end architecture is that one does not need to explicitly train different components for language understanding and dialogue management and then concatenate them together. Network-based end-to-end task-oriented chatbots as in BIBREF4, BIBREF8 try to model the learning task as a policy learning method in which the model learns to output a proper response given the current state of the dialogue. As discussed before, all encoder-decoder sequence modelling methods can be used for training end-to-end chatbots. Eric and Manning eric2017copy use the copy mechanism augmentation on simple recurrent neural sequence modelling and achieve good results in training end-to-end task-oriented chatbots BIBREF9.","Bi-directional LSTMs BIBREF21 are a variation of LSTMs which proved to give better results for some NLP tasks BIBREF22. The idea behind a Bi-directional LSTM is to give the network (while training) the ability to not only look at past tokens, like LSTM does, but to future tokens, so the model has access to information both form the past and future. In the case of a task-oriented dialogue generation systems, in some cases, the information needed so that the model learns the dependencies between the tokens, comes from the tokens that are ahead of the current index, and if the model is able to take future tokens into accounts it can learn more efficiently.","In the inference time, there are mainly two methods for decoding which are greedy and beam search BIBREF32. Beam search has been proved to be an essential part in generative NLP task such as neural machine translation BIBREF33. In the case of dialogue generation systems, beam search could help alleviate the problem of having many possible valid outputs which do not match with the target but are valid and sensible outputs. Consider the case in which a task-oriented chatbot, trained for a restaurant reservation task, in response to the user utterance “Persian food”, generates the response “what time and day would you like the reservation for?” but the target defined for the system is “would you like a fancy restaurant?”. The response generated by the chatbot is a valid response which asks the user about other possible entities but does not match with the defined target.","Per-turn Accuracy: Per-turn accuracy measures the similarity of the system generated response versus the target response. Eric and Manning eric2017copy used this metric to evaluate their systems in which they considered their response to be correct if all tokens in the system generated response matched the corresponding token in the target response. This metric is a little bit harsh, and the results may be low since all the tokens in the generated response have to be exactly in the same position as in the target response.","F1-Entity Score: Datasets used in task-oriented chores have a set of entities which represent user preferences. For example, in the restaurant domain chatbots common entities are meal, restaurant name, date, time and the number of people (these are usually the required entities which are crucial for making reservations, but there could be optional entities such as location or rating). Each target response has a set of entities which the system asks or informs the user about. Our models have to be able to discern these specific entities and inject them into the generated response. To evaluate our models we could use named-entity recognition evaluation metrics BIBREF36. The F1 score is the most commonly used metric used for the evaluation of named-entity recognition models which is the harmonic average of precision and recall of the model. We calculate this metric by micro-averaging over all the system generated responses.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the accuracy of this model compared to sota?,Sample Answer,1904.09131-Introduction-2,1904.09131-Particularities of Wikidata-1,1904.09131-Local compatibility-2,1904.09131-Local compatibility-4,1904.09131-Mapping coherence-4,"Wikidata BIBREF2 is an editable, multilingual knowledge base which has recently gained popularity as a target database for entity linking BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . As these new approaches to entity linking also introduce novel learning methods, it is hard to tell apart the benefits that come from the new models and those which come from the choice of knowledge graph and the quality of its data.","Wikidata stores information about the world in a collection of items, which are structured wiki pages. Items are identified by ther Q-id, such as Q40469, and they are made of several data fields. The label stores the preferred name for the entity. It is supported by a description, a short phrase describing the item to disambiguate it from namesakes, and aliases are alternate names for the entity. These three fields are stored separately for each language supported by Wikidata. Items also hold a collection of statements: these are RDF-style claims which have the item as subject. They can be backed by references and be made more precise with qualifiers, which all rely on a controlled vocabulary of properties (similar to RDF predicates). Finally, items can have site links, connecting them to the corresponding page for the entity in other Wikimedia projects (such as Wikipedia). Note that Wikidata items to not need to be associated with any Wikipedia page: in fact, Wikidata's policy on the notability of the subjects it covers is much more permissive than in Wikipedia. For a more detailed introduction to Wikidata's data model we refer the reader to BIBREF2 , BIBREF7 .","In Wikidata, items have labels and aliases in multiple languages. As this information is directly curated by editors, these phrases tend to be of high quality. However, they do not come with occurence counts. As items link to each other using their Wikidata identifiers only, it is not possible to compare the number of times USA was used to refer United States of America (Q30) or to United States Army (Q9212) inside Wikidata.","Manual curation of surface forms implies a fairly narrow coverage, which can be an issue for general purpose entity linking. For instance, people are commonly refered to with their given or family name only, and these names are not systematically added as aliases: at the time of writing, Trump is an alias for Donald Trump (Q22686), but Cameron is not an alias for David Cameron (Q192). As a Wikidata editor, the main incentive to add aliases to an item is to make it easier to find the item with Wikidata's auto-suggest field, so that it can be edited or linked to more easily. Aliases are not designed to offer a complete set of possible surface forms found in text: for instance, adding common mispellings of a name is discouraged.","Once a notion of semantic similarity is chosen, we need to integrate it in the inference process. Most approaches build a graph of candidate entities, where edges indicate semantic relatedness: the difference between the heuristics lie in the way this graph is used for the matching decisions. BIBREF21 use an approximate algorithm to find the densest subgraph of the semantic graph. This determines choices of entities for each mention. In other approaches, the initial evidence given by the local compatibility score is propagated along the edges of the semantic graph BIBREF14 , BIBREF22 or aggregated at a global level with a Conditional Random Field BIBREF17 .",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How does the model proposed extend ENAMEX?,Sample Answer,1912.10162-Creating a Greek POS Tagger using spaCy ::: Creation of the Tag Map with reference to Universal Dependencies-0,1912.10162-Creating a Greek POS Tagger using spaCy ::: POS Tagger training-1,1912.10162-Creating a state of the art Named Entity Recognizer using spaCy ::: Evaluation and comparison of results-3,1912.10162-Creating a state of the art Named Entity Recognizer using spaCy ::: Evaluation and comparison of results-6,1912.10162-Creating a state of the art Named Entity Recognizer using spaCy ::: Evaluation and comparison of results-7,"Different labels were found at the dataset and were matched to a label map, where for each label the part of the speech and their morphology are analyzed. In more detail, the first two characters refer to the part of speech and accordingly extend to more information about it. The label map supports 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the gender, the number, and the case BIBREF6. It must be mentioned that the extraction of morphological rules and the matching with the tags was done using the Greek version of the Universal Dependencies BIBREF7.","The main area of study for the experiments focuses on three important components. At first, we investigate the difference in results between part of speech taggers that classify morphological features and taggers that detect only the part of speech. Moreover, we explore the significance of pretrained vectors used from a model and their effect on the extraction of better results. Most importantly, the usage of subwords of tokens from a tagger as embeddings is issued. For the experiments, precision, recall and f1 score are used as evaluation metrics.","Both sources had good results in non entity tokens, which affected the F1 score. Moreover, the model did not perform well for facilities, as polyglot's Greek recognizer does not support that class and FAC entities cover a small amount of the list.","In an experiment worth mentioning the correlation of the part of speech with the performance of the recognizer was explored. In this experiment, both pipelines (part of speech, entity recognition) were used for training with 30 iterations and the model was trained twice: with and without the usage of the part of speech information for recognition.",It is evident that the recognizer did not gain knowledge from the part of speech tags of the tokens.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How much in experiments is performance improved for models trained with generated adversarial examples?,Sample Answer,1909.02560-Introduction-8,1909.02560-Methodology ::: Replaceable Position Pairs-1,1909.02560-Experiments ::: Target Models-0,1909.02560-Experiments ::: Effectiveness of Paired Common Words-0,1909.02560-6-Table2-1.png,"We reveal a new type of robustness issues in deep paraphrase identification models regarding difficult common words. Experiments show that the target models have a severe performance drop on the adversarial examples, while human annotators are much less affected and most modified sentences retain a good grammaticality.","Figure FIGREF15 shows two examples of determining replaceable positions. For the first example (matched), only common words “purpose” and “life” can be replaced. And since they are replaced simultaneously with another common words, the modified sentences are likely to talk about another same thing, e.g. changing from “purpose of life” to “measure of value”, and thereby the new sentences tend to remain matched. As for the second example (unmatched), each noun in the first sentence, “Gmail” and “account”, can form replaceable word pairs with each noun in the second sentence, “school”, “management” and “software”. The irreplaceable part determines that the modified sentences are “How can I get $\cdots $ back ? ” and “What is the best $\cdots $ ?” respectively. Sentences based on these two templates are likely to discuss about different things or different aspects, even when filled with common words, and thus they are likely to remain unmatched. In this way, the labels can be preserved in most cases.",We adopt the following typical deep models as the target models in our experiments:,"We further analyse the necessity and effectiveness of modifying sentences with paired common words. We consider another version that replaces one single word independently at each step without using paired common words, namely the unpaired version. Firstly, for matched adversarial examples that can be semantically different from original sentences, the unpaired version is inapplicable, because the matched label can be easily broken if common words from two sentences are changed into other words independently. And for the unmatched case, we show that the unpaired version is much less effective. For a more fair comparison, we double the step number limit for the unpaired version. As shown in Table TABREF41, the performance of target models on unmatched examples generated by the unpaired version, particularly that of BERT, is mostly much higher than those by our full algorithm, except for BiMPM on MRPC but its accuracies have almost reached 0 (0.0% for unpaired and 0.2% for paired). This demonstrates that our algorithm using paired common words are more effective in generating adversarial examples, on which the performance of the target model is generally much lower. An advantage of using difficult common words for unmatched examples is that such words tend to make target models over-confident about common words and distract the models on recognizing the semantic difference in the unmodified part. Our algorithm explicitly utilizes this property and thus can well reveal such a robustness issue. Moreover, although there is no such a property for the matched case, replacing existing common words with more difficult ones can still distract the target model on judging the semantic similarity in the unmodified part, due to the bias between different words learned by the model, and thus our algorithm for generating adversarial examples with difficult common words works for both matched and unmatched cases.","Table 2: Manual evaluation results, including human performance on both original and adversarial examples, and the grammaticality ratings of the generated sentences.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Does this paper propose a new task that others can try to improve performance on?,Sample Answer,1610.03112-Introduction and Related Work-1,1610.03112-Data and Annotation-0,1610.03112-Data and Annotation-1,1610.03112-Model and Experiment-0,1610.03112-Models-0,"Interesting prior work on quantifying social norm violation has taken a heavily data-driven focus BIBREF8 , BIBREF9 . For instance, BIBREF8 trained a series of bigram language models to quantify the violation of social norms in users' posts on an online community by leveraging cross-entropy value, or the deviation of word sequences predicted by the language model and their usage by the user. However, their models were trained on written-language instead of natural face-face dialog corpus. Another kind of social norm violation was examined by BIBREF10 , who developed a classifier to identify specific types of sarcasm in tweets. They utilized a bootstrapping algorithm to automatically extract lists of positive sentiment phrases and negative situation phrases from given sarcastic tweets, which were in turn leveraged to recognize sarcasm in an SVM classifier. However, no contextual information was considered in this work. BIBREF11 understood the nature of social norm violation in dialog by correlating it with associated observable verbal, vocal and visual cues. By leveraging their findings and statistical machine learning techniques, they built a computational model for automatic recognition. While they preserved short-term temporal contextual information in the model, this study avoided dealing with sparsity of the social norm violation phenomena by under-sampling the negative-class instances to make a balanced dataset.","Reciprocal peer tutoring data was collected from 12 American English-speaking dyads (6 friends and 6 strangers; 6 boys and 6 girls), with a mean age of 13 years, who interacted for 5 hourly sessions over as many weeks (a total of 60 sessions, and 5400 minutes of data), tutoring one another in algebra. Each session began with a period of getting to know one another, after which the first tutoring period started, followed by another small social interlude, a second tutoring period with role reversal between the tutor and tutee, and then the final social time.","We assessed our automatic recognition of social norm violation against this corpus annotated for those strategies. Inter-rater reliability (IRR) for the social norm violation that computed via Krippendorff's alpha was 0.75. IRR for visual behavior was 0.89 for eye gaze, 0.75 for smile count (how many smiles occur), 0.64 for smile duration and 0.99 for head nod. Table 1 shows statistics of our corpus. Below we discuss the definition of social norm violation.","In this section, our objective was to build a computational model for detecting social norm violation. Towards this end, we first took each clause, the smallest units that can express a complete proposition, as the prediction unit. Next, inspired from the thorough analysis in BIBREF11 , we extracted verbal and visual features of the speaker that were highly correlated to social norm violation clauses, with rare threshold being set to 20. Verbal features included LIWC features BIBREF12 that helped in categorization of words used during usage of social norm violation, bigrams, part of speech bigrams and word-part of speech pairs from the speaker's clauses. Visual features included head node, smile and eye gaze information of the speaker. In total there were 3782 features per clause.","We treated a dialog $D$ as a sequence of clauses $c_0, ... c_T$ , where $T$ was the number of clauses in the $D$ . Each clause $c_i$ was a tuple $([w^i_0, ...w^i_m], e_i)$ , where $[w^i_0, ...w^i_m]$ was the $m$ words in the clause $c_i$ , and $e_i$ was the corresponding meta information such as the relationship of the dyad and nonverbal behavior during the generation of the clause. The handcrafted feature of size 3782 was denoted as $c_0, ... c_T$0 , and could be viewed as a mapping function $c_0, ... c_T$1 . Meanwhile, each clause was associated with a binary label $c_0, ... c_T$2 that indicates the ground truth of whether $c_0, ... c_T$3 is a violation of social norm. Eventually, the goal was to model $c_0, ... c_T$4 , the conditional distribution over whether the latest clause was a violation of social norm, given the entire history of the dialog.",1.0,1.0,1.0,1.0,1.0,0.33333333333333337,1.0,0.2
"What kind of features are used by the HMM models, and how interpretable are those?",Sample Answer,1606.05320-Introduction-0,1606.05320-Hybrid models-1,1606.05320-Experiments-0,1606.05320-3-Table1-1.png,1606.05320-4-Figure3-1.png,"Following the recent progress in deep learning, researchers and practitioners of machine learning are recognizing the importance of understanding and interpreting what goes on inside these black box models. Recurrent neural networks have recently revolutionized speech recognition and translation, and these powerful models could be very useful in other applications involving sequential data. However, adoption has been slow in applications such as health care, where practitioners are reluctant to let an opaque expert system make crucial decisions. If we can make the inner workings of RNNs more interpretable, more applications can benefit from their power.","We also build a joint hybrid model, where the LSTM and HMM are simultaneously trained in Torch. We implemented an HMM Torch module, optimized using stochastic gradient descent rather than FFBS. Similarly to the sequential hybrid model, we concatenate the LSTM outputs with the HMM state probabilities.","We test the models on several text data sets on the character level: the Penn Tree Bank (5M characters), and two data sets used by BIBREF4 , Tiny Shakespeare (1M characters) and Linux Kernel (5M characters). We chose $k=20$ for the continuous HMM based on a PCA analysis of the LSTM states, as the first 20 components captured almost all the variance.","Table 1: Predictive loglikelihood (LL) comparison, sorted by validation set performance.",Figure 3: Decision tree predicting an individual hidden state dimension of the hybrid algorithm based on the preceding characters on the Linux data. Nodes with uninformative splits are represented with . . . .,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What kind of information do the HMMs learn that the LSTMs don't?,Sample Answer,1606.05320-Introduction-2,1606.05320-Introduction-3,1606.05320-Hidden Markov models-4,1606.05320-Hidden Markov models-6,1606.05320-4-Figure3-1.png,"Several promising approaches to interpreting RNNs have been developed recently. BIBREF3 have approached this by using gradient boosting trees to predict LSTM output probabilities and explain which features played a part in the prediction. They do not model the internal structure of the LSTM, but instead approximate the entire architecture as a black box. BIBREF4 showed that in LSTM language models, around 10% of the memory state dimensions can be interpreted with the naked eye by color-coding the text data with the state values; some of them track quotes, brackets and other clearly identifiable aspects of the text. Building on these results, we take a somewhat more systematic approach to looking for interpretable hidden state dimensions, by using decision trees to predict individual hidden state dimensions (Figure 2 ). We visualize the overall dynamics of the hidden states by coloring the training data with the k-means clusters on the state vectors (Figures 3 , 3 ).","We explore several methods for building interpretable models by combining LSTMs and HMMs. The existing body of literature mostly focuses on methods that specifically train the RNN to predict HMM states BIBREF5 or posteriors BIBREF6 , referred to as hybrid or tandem methods respectively. We first investigate an approach that does not require the RNN to be modified in order to make it understandable, as the interpretation happens after the fact. Here, we model the big picture of the state changes in the LSTM, by extracting the hidden states and approximating them with a continuous emission hidden Markov model (HMM). We then take the reverse approach where the HMM state probabilities are added to the output layer of the LSTM (see Figure 1 ). The LSTM model can then make use of the information from the HMM, and fill in the gaps when the HMM is not performing well, resulting in an LSTM with a smaller number of hidden state dimensions that could be interpreted individually (Figures 3 , 3 ).",At each iteration:,Sample transition parameters from a Multinomial-Dirichlet posterior. Let $n_{ij}$ be the number of transitions from state $i$ to state $j$ . Then the posterior distribution of the $i$ -th row of transition matrix $T$ (corresponding to transitions from state $i$ ) is: $T_i \sim \text{Mult}(n_{ij} | T_i) \text{Dir}(T_i | \alpha )$ ,Figure 3: Decision tree predicting an individual hidden state dimension of the hybrid algorithm based on the preceding characters on the Linux data. Nodes with uninformative splits are represented with . . . .,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How large is the gap in performance between the HMMs and the LSTMs?,Sample Answer,1606.05320-Introduction-1,1606.05320-Introduction-3,1606.05320-LSTM models-0,1606.05320-Hidden Markov models-6,1606.05320-4-Figure3-1.png,"There are several aspects of what makes a model or algorithm understandable to humans. One aspect is model complexity or parsimony. Another aspect is the ability to trace back from a prediction or model component to particularly influential features in the data BIBREF0 BIBREF1 . This could be useful for understanding mistakes made by neural networks, which have human-level performance most of the time, but can perform very poorly on seemingly easy cases. For instance, convolutional networks can misclassify adversarial examples with very high confidence BIBREF2 , and made headlines in 2015 when the image tagging algorithm in Google Photos mislabeled African Americans as gorillas. It's reasonable to expect recurrent networks to fail in similar ways as well. It would thus be useful to have more visibility into where these sorts of errors come from, i.e. which groups of features contribute to such flawed predictions.","We explore several methods for building interpretable models by combining LSTMs and HMMs. The existing body of literature mostly focuses on methods that specifically train the RNN to predict HMM states BIBREF5 or posteriors BIBREF6 , referred to as hybrid or tandem methods respectively. We first investigate an approach that does not require the RNN to be modified in order to make it understandable, as the interpretation happens after the fact. Here, we model the big picture of the state changes in the LSTM, by extracting the hidden states and approximating them with a continuous emission hidden Markov model (HMM). We then take the reverse approach where the HMM state probabilities are added to the output layer of the LSTM (see Figure 1 ). The LSTM model can then make use of the information from the HMM, and fill in the gaps when the HMM is not performing well, resulting in an LSTM with a smaller number of hidden state dimensions that could be interpreted individually (Figures 3 , 3 ).","We use a character-level LSTM with 1 layer and no dropout, based on the Element-Research library. We train the LSTM for 10 epochs, starting with a learning rate of 1, where the learning rate is halved whenever $\exp (-l_t) > \exp (-l_{t-1}) + 1$ , where $l_t$ is the log likelihood score at epoch $t$ . The $L_2$ -norm of the parameter gradient vector is clipped at a threshold of 5.",Sample transition parameters from a Multinomial-Dirichlet posterior. Let $n_{ij}$ be the number of transitions from state $i$ to state $j$ . Then the posterior distribution of the $i$ -th row of transition matrix $T$ (corresponding to transitions from state $i$ ) is: $T_i \sim \text{Mult}(n_{ij} | T_i) \text{Dir}(T_i | \alpha )$ ,Figure 3: Decision tree predicting an individual hidden state dimension of the hybrid algorithm based on the preceding characters on the Linux data. Nodes with uninformative splits are represented with . . . .,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
"Was the degree of offensiveness taken as how generally offensive the text was, or how personally offensive it was to the annotator?",Sample Answer,1701.08118-Introduction-0,1701.08118-Introduction-1,1701.08118-Hate Speech-0,1701.08118-Hate Speech-2,1701.08118-Methods-3,"Social media are sometimes used to disseminate hateful messages. In Europe, the current surge in hate speech has been linked to the ongoing refugee crisis. Lawmakers and social media sites are increasingly aware of the problem and are developing approaches to deal with it, for example promising to remove illegal messages within 24 hours after they are reported BIBREF0 .",This raises the question of how hate speech can be detected automatically. Such an automatic detection method could be used to scan the large amount of text generated on the internet for hateful content and report it to the relevant authorities. It would also make it easier for researchers to examine the diffusion of hateful content through social media on a large scale.,"For the purpose of building a classifier, warner2012 define hate speech as “abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation”. More recent approaches rely on lists of guidelines such as a tweet being hate speech if it “uses a sexist or racial slur” BIBREF2 . These approaches are similar in that they leave plenty of room for personal interpretation, since there may be differences in what is considered offensive. For instance, while the utterance “the refugees will live off our money” is clearly generalising and maybe unfair, it is unclear if this is already hate speech. More precise definitions from law are specific to certain jurisdictions and therefore do not capture all forms of offensive, hateful speech, see e.g. matsuda1993. In practice, social media services are using their own definitions which have been subject to adjustments over the years BIBREF3 . As of June 2016, Twitter bans hateful conduct.","Along with the presence of hate speech, its real-life consequences are also growing. It can be a precursor and incentive for hate crimes, and it can be so severe that it can even be a health issue BIBREF4 . It is also known that hate speech does not only mirror existing opinions in the reader but can also induce new negative feelings towards its targets BIBREF5 . Hate speech has recently gained some interest as a research topic on the one hand – e.g. BIBREF6 , BIBREF4 , BIBREF7 – but also as a problem to deal with in politics such as the No Hate Speech Movement by the Council of Europe.",The surveys were approved by the ethical committee of the Department of Computer Science and Applied Cognitive Science of the Faculty of Engineering at the University of Duisburg-Essen.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what was their result?,Sample Answer,1708.05521-Introduction-0,1708.05521-Introduction-1,1708.05521-Introduction-3,1708.05521-Proposed Approach-1,1708.05521-Proposed Approach-2,"Twitter is a huge micro-blogging service with more than 500 million tweets per day from different locations in the world and in different languages. This large, continuous, and dynamically updated content is considered a valuable resource for researchers. In particular, many of these messages contain emotional charge, conveying affect—emotions, feelings and attitudes, which can be studied to understand the expression of emotion in text, as well as the social phenomena associated.","While studying emotion in text it is commonly useful to characterize the emotional charge of a passage based on its words. Some words have affect as a core part of their meaning. For example, dejected and wistful denote some amount of sadness, and are thus associated with sadness. On the other hand, some words are associated with affect even though they do not denote affect. For example, failure and death describe concepts that are usually accompanied by sadness and thus they denote some amount of sadness.","The task is specially challenging since tweets contain informal language, spelling errors and text referring to external content. Given the 140 character limit of tweets, it is also possible to find some phenomena such as the intensive usage of emoticons and of other special Twitter features, such as hashtags and usernames mentions —used to call or notify other users. In this paper we describe our system designed for the WASSA-2017 Shared Task on Emotion Intensity, which we tackle based on the premise of representation learning without the usage of external information, such as lexicons. In particular, we use a Bi-LSTM model with intra-sentence attention on top of word embeddings to generate a tweet representation that is suitable for emotion intensity. Our results show that our proposed model offers interesting capabilities compared to approaches that do rely on external information sources.","EmoAtt is based on a bidirectional RNN that receives an embedded input sequence INLINEFORM0 and returns a list of hidden vectors that capture the context each input token INLINEFORM1 . To improve the capabilities of the RNN to capture short-term temporal dependencies BIBREF4 , we define the following: DISPLAYFORM0 ","Where INLINEFORM0 can be regarded as a context window of ordered word embedding vectors around position INLINEFORM1 , with a total size of INLINEFORM2 . To further complement the context-aware token representations, we concatenate each hidden vector to a vector of binary features INLINEFORM3 , extracted from each tweet token, defining an augmented hidden state INLINEFORM4 . Finally, we combine our INLINEFORM5 augmented hidden states, compressing them into a single vector, using a global intra-sentence attentional component in a fashion similar to vinyalsgrammar2015. Formally, DISPLAYFORM0 ",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
what dataset was used?,Sample Answer,1708.05521-Introduction-0,1708.05521-Introduction-1,1708.05521-Experimental Setup-3,1708.05521-4-Table2-1.png,1708.05521-4-Table3-1.png,"Twitter is a huge micro-blogging service with more than 500 million tweets per day from different locations in the world and in different languages. This large, continuous, and dynamically updated content is considered a valuable resource for researchers. In particular, many of these messages contain emotional charge, conveying affect—emotions, feelings and attitudes, which can be studied to understand the expression of emotion in text, as well as the social phenomena associated.","While studying emotion in text it is commonly useful to characterize the emotional charge of a passage based on its words. Some words have affect as a core part of their meaning. For example, dejected and wistful denote some amount of sadness, and are thus associated with sadness. On the other hand, some words are associated with affect even though they do not denote affect. For example, failure and death describe concepts that are usually accompanied by sadness and thus they denote some amount of sadness.","While the structure of our introduced model allows us to easily include more linguistic features that could potentially improve our predictive power, such as lexicons, since our focus is to study sentence representation for emotion intensity, we do not experiment adding any additional sources of information as input.",Table 2: Summary of the best results.,Table 3: Impact of adding our binary features.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the opportunities presented by the use of Semantic Web technologies in Machine Translation?,Sample Answer,1907.10676-Introduction-0,1907.10676-Introduction-2,1907.10676-Introduction-5,1907.10676-Open MT Challenges-0,1907.10676-Suggestions and Possible Directions using SW-10,"Alongside increasing globalization comes a greater need for readers to understand texts in languages foreign to them. For example, approximately 48% of the pages on the Web are not available in English. The technological progress of recent decades has made both the distribution and access to content in different languages ever simpler. Translation aims to support users who need to access content in a language in which they are not fluent BIBREF0 .","Although MT systems are now popular on the Web, they still generate a large number of incorrect translations. Recently, Popović BIBREF3 has classified five types of errors that still remain in MT systems. According to research, the two main faults that are responsible for 40% and 30% of problems respectively, are reordering errors and lexical and syntactic ambiguity. Thus, addressing these barriers is a key challenge for modern translation systems. A large number of MT approaches have been developed over the years that could potentially serve as a remedy. For instance, translators began by using methodologies based on linguistics which led to the family of RBMT. However, RBMT systems have a critical drawback in their reliance on manually crafted rules, thus making the development of new translation modules for different languages even more difficult.","According to our survey BIBREF6 , the obvious opportunity of using SWT for MT has already been studied by a number of approaches, especially w.r.t. the issue of ambiguity. In this paper, we present the challenges and opportunities in the use of SWT in MT for translating texts.","The most problematic unresolved MT challenges, from our point of view, which are still experienced by the aforementioned MT approaches are the following:","Besides C. Shi et al BIBREF11 , Arčan and Buitelaar BIBREF19 presented an approach to translate domain-specific expressions represented by English KBs in order to make the knowledge accessible for other languages. They claimed that KBs are mostly in English, therefore they cannot contribute to the problem of MT to other languages. Thus, they translated two KBs belonging to medical and financial domains, along with the English Wikipedia, to German. Once translated, the KBs were used as external resources in the translation of German-English. The results were quite appealing and the further research into this area should be undertaken. Recently, Moussallem et al BIBREF20 created THOTH, an approach which translates and enriches knowledge graphs across languages. Their approach relies on two different recurrent neural network models along with knowledge graph embeddings. The authors applied their approach on the German DBpedia with the German translation of the English DBpedia on two tasks: fact checking and entity linking. THOTH showed promising results with a translation accuracy of 88.56 while being capable of improving two NLP tasks with its enriched-German KG .",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the challenges associated with the use of Semantic Web technologies in Machine Translation?,Sample Answer,1907.10676-Introduction-0,1907.10676-Open MT Challenges-3,1907.10676-Suggestions and Possible Directions using SW-2,1907.10676-Suggestions and Possible Directions using SW-3,1907.10676-Suggestions and Possible Directions using SW-8,"Alongside increasing globalization comes a greater need for readers to understand texts in languages foreign to them. For example, approximately 48% of the pages on the Web are not available in English. The technological progress of recent decades has made both the distribution and access to content in different languages ever simpler. Translation aims to support users who need to access content in a language in which they are not fluent BIBREF0 .","The challenges above are clearly not independent, which means that addressing one of them can have an impact on the others. Since NMT has shown impressive results on reordering, the main problem turns out to be the disambiguation process (both syntactically and semantically) in SMT approaches BIBREF0 .","SW has already shown its capability for semantic disambiguation of polysemous and homonymous words. However, SWT were applied in two ways to support the semantic disambiguation in MT. First, the ambiguous words were recognized in the source text before carrying out the translation, applying a pre-editing technique. Second, SWT were applied to the output translation in the target language as a post-editing technique. Although applying one of these techniques has increased the quality of a translation, both techniques are tedious to implement when they have to translate common words instead of named entities, then be applied several times to achieve a successful translation.","The real benefit of SW comes from its capacity to provide unseen knowledge about emergent data, which appears every day. Therefore, we suggest performing the topic-modelling technique over the source text to provide a necessary context before translation. Instead of applying the topic-modeling over the entire text, we would follow the principle of communication (i.e from 3 to 5 sentences for describing an idea and define a context for each piece of text. Thus, at the execution of a translation model in a given SMT, we would focus on every word which may be a homonymous or polysemous word. For every word which has more than one translation, a SPARQL query would be required to find the best combination in the current context. Thus, at the translation phase, the disambiguation algorithm could search for an appropriate word using different SW resources such as DBpedia, in consideration of the context provided by the topic modelling. The goal is to exploit the use of more than one SW resource at once for improving the translation of ambiguous terms. The use of two or more SW resources simultaneously has not yet been investigated.","Therefore, we suggest that user characteristics can be applied as context for solving the non-standard language problem. These characteristics can be extracted from social media or user logs and stored as user properties using SWT, e.g., FOAF vocabulary. These ontologies have properties which would help identify the birth place or the interests of a given user. For instance, the properties foaf:interest and sioc:topic can be used to describe a given person's topics of interest. If the person is a computer scientist and the model contains topics such as “Information Technology"" and “Sports"", the SPARQL queries would search for terms inserted in this context which are ambiguous. Furthermore, the property foaf:based_near may support the problem of idioms. Assuming that a user is located in a certain part of Russia and he is reading an English web page which contains some idioms, this property may be used to gather appropriate translations of idioms from English to Russian using a given RDF KB. Therefore, an MT system can be adapted to a user by using specific data about him in RDF along with given KBs. Recently, Moussallem et al BIBREF16 have released a multilingual linked idioms dataset as a first part of supporting the investigation of this suggestion. The dataset contains idioms in 5 languages and are represented by knowledge graphs which facilitates the retrieval and inference of translations among the idioms.",1.0,1.0,1.0,1.0,1.0,0.20000000000000004,0.2,0.2
How do they define local variance?,Sample Answer,1910.11491-Introduction-0,1910.11491-Introduction-1,1910.11491-Proposed model ::: Model Architecture-2,1910.11491-Experiments ::: Preliminaries ::: Dataset and Metrics.-0,1910.11491-Experiments ::: Automatic Evaluation Result-0,"Abstractive document summarization BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4 attempts to produce a condensed representation of the most salient information of the document, aspects of which may not appear as parts of the original input text. One popular framework used in abstractive summarization is the sequence-to-sequence model introduced by BIBREF5. The attention mechanism BIBREF6 is proposed to enhance the sequence-to-sequence model by allowing salient features to dynamically come to the forefront as needed to make up for the incapability of memorizing the long input source.","However, when it comes to longer documents, basic attention mechanism may lead to distraction and fail to attend to the relatively salient parts. Therefore, some works focus on designing various attentions to tackle this issue BIBREF2, BIBREF7. We follow this line of research and propose an effective attention refinement unit (ARU). Consider the following case. Even with a preliminary idea of which parts of source document should be focused on (attention), sometimes people may still have trouble in deciding which exact part should be emphasized for the next word (the output of the decoder). To make a more correct decision on what to write next, people always adjust the concentrated content by reconsidering the current state of what has been summarized already. Thus, ARU is designed as an update unit based on current decoding state, aiming to retain the attention on salient parts but weaken the attention on irrelevant parts of input.","To augment the vanilla attention model, we propose the Attention Refinement Unit (ARU) module to retain the attention on the salient parts while weakening the attention on the irrelevant parts of input. As illustrated in Figure FIGREF5, the attention weight distribution $a_t$ at timestep $t$ (the first red histogram) is fed through the ARU module. In the ARU module, current decoding state $s_t$ and attention distribution $a_t$ are combined to calculate a refinement gate $r_t$:","We conduct our model on the large-scale dataset CNN/Daily Mail BIBREF19, BIBREF1, which is widely used in the task of abstractive document summarization with multi-sentences summaries. We use the scripts provided by BIBREF11 to obtain the non-anonymized version of the dataset without preprocessing to replace named entities. The dataset contains 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs in total. We use the full-length ROUGE F1 and METEOR as our main evaluation metrics.","As shown in Table TABREF13 (the performance of other models is collected from their papers), our model exceeds the PGN baseline by 3.85, 2.1 and 3.37 in terms of R-1, R-2 and R-L respectively and receives over 3.23 point boost on METEOR. FastAbs BIBREF3 regards ROUGE scores as reward signals with reinforcement learning, which brings a great performance gain. DCA BIBREF4 proposes deep communicating agents with reinforcement setting and achieves the best results on CNN/Daily Mail. Although our experimental results have not outperformed the state-of-the-art models, our model has a much simpler structure with fewer parameters. Besides, these simple methods do yield a boost in performance compared with PGN baseline and may be applied on other models with attention mechanism.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How much additional data do they manage to generate from translations?,Sample Answer,1808.10290-Introduction-1,1808.10290-Results-6,1808.10290-Results-26,1808.10290-Results-29,1808.10290-6-Table1-1.png,"Discourse relations can be marked explicitly using a discourse connective or discourse adverbial such as “because”, “but”, “however”, see example SECREF1 . Explicitly marked relations are relatively easy to classify automatically BIBREF7 . In example SECREF2 , the causal relation is not marked explicitly, and can only be inferred from the texts. This second type of case is empirically even more common than explicitly marked relations BIBREF8 , but is much harder to classify automatically.","Original English: I presided over a region crossed by heavy traffic from all over Europe...what is more, in 2002, two Member States of the European Union appealed to the European Court of Justice...","Original English: We all understand that nobody can return Russia to the path of freedom and democracy... (implicit: but) what is more, the situation in our country is not as straightforward as it might appear...",,Table 1: Performances with different sets of additional data. Average accuracy of 10 runs (5 for cross validations) are shown here with standard deviation in the brackets. Numbers in bold are significantly (p<0.05) better than the PDTB only baseline with unpaired t-test.,1.0,1.0,1.0,1.0,1.0,0.25,0.3333333333333333,0.2
How many languages do they at most attempt to use to generate discourse relation labelled data?,Sample Answer,1808.10290-Results-5,1808.10290-Results-16,1808.10290-Results-18,1808.10290-Results-23,1808.10290-Results-25,,,,,,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Which of the classifiers showed the best performance?,Sample Answer,1901.00570-Introduction-3,1901.00570-Sentiment analysis approaches-0,1901.00570-Feature Selection Methods-1,1901.00570-Feature Selection Methods-6,1901.00570-Training the model:-1,"Using text as features in Twitter is challenging because of the informal nature of the tweets, the limited length of the tweet, platform-specific language, and multilingual nature of Twitter BIBREF0 , BIBREF1 , BIBREF2 . The main challenges for text analysis in Twitter are listed below:","The third approach is to identify sentiment through the context of the post, which is another application for distributional semantics requiring a huge amount of training data to build the required understanding of the context. Sentiment analysis approaches focus on recognizing the feelings of the crowd and use the score of each feeling as a feature to calculate the probability of social events occurring. The sentiment can represent the emotion, attitude, or opinion of the user towards the subject of the post. One approach to identify sentiment is to find smiley faces such as emoticons and emojis within a tweet or a post. Another approach is to use a sentiment labelled dictionary such as SentiWordNet to assess the sentiment associated with each word.","Pearson correlation measures the linear dependency of the response variable on the independent variable with the maximum dependency of 1 and no dependency of zero. This technique needs to satisfy multiple assumptions to assess the dependency properly. These assumptions require the signals of the variables to be normally distributed, homoskedastic, stationary and have no outliers BIBREF25 , BIBREF26 . In social network and human-authored tweets, we cannot guarantee that the word-pairs signals throughout the timeframe will satisfy the required assumptions. Another drawback for Pearson correlation is that zero score does not necessarily imply no correlation, while no correlation implies zero score.","Cosine similarity metric calculates the cosine of the angle between two vectors. The cosine metric evaluates the direction similarity of the vectors rather the magnitude similarity. The cosine similarity score equals to 1 if the two vectors have the angle of zero between the directions of two vectors, and the score is set to zero when the two vectors are perpendicular BIBREF33 . if the two vectors are oriented to opposite directions, the similarity score is -1. Cosine similarity metric is usually used in the positive space, which makes the scores limited within the interval of [0,1].",The training process aims to calculate three priori probabilities to be used later in calculating the posterior probabilities: (1) the probability of each word-pair count in a specific day given the status of the day as “event” or “non-event”. (2) the priori conditional probability of each word-pair given event status INLINEFORM0 . (3) the probability of each event class as well as the probability of each word-pair as stated in equations EQREF15 and EQREF15 . DISPLAYFORM0 DISPLAYFORM1 ,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How are the keywords associated with events such as protests selected?,Sample Answer,1901.00570-Sentiment analysis approaches-0,1901.00570-Feature Selection Methods-5,1901.00570-Feature Selection Methods-7,1901.00570-Training the model:-0,1901.00570-Experiments and Results-3,"The third approach is to identify sentiment through the context of the post, which is another application for distributional semantics requiring a huge amount of training data to build the required understanding of the context. Sentiment analysis approaches focus on recognizing the feelings of the crowd and use the score of each feeling as a feature to calculate the probability of social events occurring. The sentiment can represent the emotion, attitude, or opinion of the user towards the subject of the post. One approach to identify sentiment is to find smiley faces such as emoticons and emojis within a tweet or a post. Another approach is to use a sentiment labelled dictionary such as SentiWordNet to assess the sentiment associated with each word.","Mutual information is a metric for the amount of information one variable can tell the other one. MI evaluates how similar are the joint distributions of the two variables with the product of the marginal distributions of each individual variable, which makes MI more general than correlation as it is not limited by the real cardinal values, it can also be applied to binary, ordinal and nominal values BIBREF31 . As mutual information uses the similarity of the distribution, it is not concerned with pairing the individual observations of X and Y as much as it cares about the whole statistical distribution of X and Y. This makes MI very useful for clustering purposes rather than classification purposes BIBREF32 .",Jaccard index or coefficient is a metric to evaluate the similarity of two sets by comparing their members to identify the common elements versus the distinct ones. The main advantage of Jaccard similarity is it ignores the default value or the null assumption in the two vectors and it only considers the non-default correct matches compared to the mismatches. This consideration makes the metric immune to the data imbalance. Jaccard index is similar to cosine-similarity as it retains the sparsity property and it also allows the discrimination of the collinear vectors.,"The third step is to train the model using the set of features generated in the first step. We selected the Naive Bayes classifier to be our classification technique for the following reasons: (1) the high bias of the NB classifier reduces the possibility of over-fitting, and our problem has a high probability of over-fitting due to the high number of features and the low number of observations, (2) the response variable is binary, so we do not need to regress the variable real value as much as we need to know the event-class, and (3) The counts of the word-pairs as independent variables are limited between 0 and 100 occurrences per each day, which make the probabilistic approaches more effective than distance based approaches.","Once we selected the most informative word-pairs as features, we will use the raw values to train the Naive Bayes classifier. The classifier is trained using 500 days selected randomly along the whole timeframe, then it is used to predict the other 140 days. To ensure the robustness of our experiment, We applied 10-folds cross-validation, where we performed the same experiment 10 times using 10 different folds of randomly selected training and testing data. The prediction achieved an average area under the ROC curve of 90%, which statistically significant and achieved F-score of 91%, which is immune to data imbalance as listed in table TABREF18 . Figure FIGREF25 shows the ROC curves for the results of a single fold of Naive Bayes classification that uses the features extracted by each selection methods. The classification results of the proposed method outperformed the benchmarks and state of the art developed by Cui et al. (2017), Nguyen et al. (2017), Willer et al. (2016), and Adedoyin-Olowe et al. (2016) as illustrated in the table TABREF33 BIBREF12 , BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , BIBREF42 .",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What baselines did they consider?,Sample Answer,1809.01202-Introduction-0,1809.01202-Introduction-1,1809.01202-Introduction-3,1809.01202-Exploration-0,1809.01202-1-Figure1-1.png,"Explanations of happenings in one's life, causal explanations, are an important topic of study in social, psychological, economic, and behavioral sciences. For example, psychologists have analyzed people's causal explanatory style BIBREF0 and found strong negative relationships with depression, passivity, and hostility, as well as positive relationships with life satisfaction, quality of life, and length of life BIBREF1 , BIBREF2 , BIBREF0 .","To help understand the significance of causal explanations, consider how they are applied to measuring optimism (and its converse, pessimism) BIBREF0 . For example, in “My parser failed because I always have bugs.”, the emphasized text span is considered a causal explanation which indicates pessimistic personality – a negative event where the author believes the cause is pervasive. However, in “My parser failed because I barely worked on the code.”, the explanation would be considered a signal of optimistic personality – a negative event for which the cause is believed to be short-lived.","In this paper, we introduce causal explanation analysis and its subtasks of detecting the presence of causality (causality prediction) and identifying explanatory phrases (causal explanation identification). There are many challenges to achieving these task. First, the ungrammatical texts in social media incur poor syntactic parsing results which drastically affect the performance of discourse relation parsing pipelines . Many causal relations are implicit and do not contain any discourse markers (e.g., `because'). Further, Explicit causal relations are also more difficult in social media due to the abundance of abbreviations and variations of discourse connectives (e.g., `cuz' and `bcuz').","Here, we explore the use of causal explanation analysis for downstream tasks. First we look at the relationship between use of causal explanation and one's demographics: age and gender. Then, we consider their use in sentiment analysis for extracting the causes of polarity ratings. Research involving human subjects was approved by the University of Pennsylvania Institutional Review Board.","Figure 1: A casual relation characterizes the connection between two discourse arguments, one of which is the causal explanation.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How was speed measured?,Sample Answer,1710.06700-Introduction-1,1710.06700-Background-0,1710.06700-system Description-1,1710.06700-system Description-3,1710.06700-Discussion-0,"Lemmatization is an important preprocessing step for many applications of text mining and question-answering systems, and researches in Arabic Information Retrieval (IR) systems show the need for representing Arabic words at lemma level for many applications, including keyphrase extraction BIBREF0 and machine translation BIBREF1 . In addition, lemmatization provides a productive way to generate generic keywords for search engines (SE) or labels for concept maps BIBREF2 .","Arabic is the largest Semitic language spoken by more than 400 million people. It's one of the six official languages in the United Nations, and the fifth most widely spoken language after Chinese, Spanish, English, and Hindi. Arabic has a very rich morphology, both derivational and inflectional. Generally, Arabic words are derived from a root that uses three or more consonants to define a broad meaning or concept, and they follow some templatic morphological patterns. By adding vowels, prefixes and suffixes to the root, word inflections are generated. For instance, the word ÙØ³ÙÙØªØ­ÙÙ> (wsyftHwn) “and they will open” has the triliteral root ÙØªØ­> (ftH), which has the basic meaning of opening, has prefixes ÙØ³> (ws) “and will”, suffixes ÙÙ> (wn) “they”, stem ÙÙØªØ­> (yftH) “open”, and lemma ÙØªØ­> (ftH) “the concept of opening”.","From a large diacritized corpus, we constructed a dictionary of words and their possible diacritizations ordered by number of occurrences of each diacritized form. This diacritized corpus was created by a commercial vendor and contains 9.7 million words with almost 200K unique surface words. About 73% of the corpus is in MSA and covers variety of genres like politics, economy, sports, society, etc. and the remaining part is mostly religious texts written in classical Arabic (CA). The effectiveness of using this corpus in building state-of-the-art diacritizer was proven in BIBREF10 .For example, the word ÙØ¨ÙÙØ¯> (wbnwd) “and items” is found 4 times in this corpus with two full diacritization forms ÙÙØ¨ÙÙÙÙØ¯ÙØ ÙÙØ¨ÙÙÙÙØ¯Ù> (wabunudi, wabunudK) “items, with different grammatical case endings” which appeared 3 times and once respectively. All unique undiacritized words in this corpus were analyzed using Buckwalter morphological analyzer which gives all possible word diacritizations, and their segmentation, POS tag and lemma as shown in Figure FIGREF3 .","While comparing two diacritized forms from the corpus and Buckwalter analysis, special cases were applied to solve inconsistencies between the two diacritization schemas, for example while words are fully diacritized in the corpus, Buckwalter analysis gives diacritics without case ending (i.e. without context), and removes short vowels in some cases, for example before long vowels, and after the definite article Ø§Ù> (Al) “the”, etc.","In this paper, we introduce a new dataset for Arabic lemmatization and a very fast and accurate lemmatization algorithm that performs better than state-of-the art system; MADAMIRA. Both the dataset and the code will be publicly available. We show that to build an effective IR system for complex derivational languages like Arabic, there is a a big need for very fast and accurate lemmatization algorithms, and we show that this can be achieved by considering only the most frequent diacritized form for words and matching this form with the morphological analysis with highest similarity score. We plan to study the performance if the algorithm was modified to provide diacritized lemmas which can be useful for other applications.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What were their accuracy results on the task?,Sample Answer,1710.06700-Introduction-0,1710.06700-Introduction-2,1710.06700-Background-0,1710.06700-system Description-1,1710.06700-system Description-3,"Lemmatization is the process of finding the base form (or lemma) of a word by considering its inflected forms. Lemma is also called dictionary form, or citation form, and it refers to all words having the same meaning.","Word stem is that core part of the word that never changes even with morphological inflections; the part that remains after prefix and suffix removal. Sometimes the stem of the word is different than its lemma, for example the words: believe, believed, believing, and unbelievable share the stem (believ-), and have the normalized word form (believe) standing for the infinitive of the verb (believe).","Arabic is the largest Semitic language spoken by more than 400 million people. It's one of the six official languages in the United Nations, and the fifth most widely spoken language after Chinese, Spanish, English, and Hindi. Arabic has a very rich morphology, both derivational and inflectional. Generally, Arabic words are derived from a root that uses three or more consonants to define a broad meaning or concept, and they follow some templatic morphological patterns. By adding vowels, prefixes and suffixes to the root, word inflections are generated. For instance, the word ÙØ³ÙÙØªØ­ÙÙ> (wsyftHwn) “and they will open” has the triliteral root ÙØªØ­> (ftH), which has the basic meaning of opening, has prefixes ÙØ³> (ws) “and will”, suffixes ÙÙ> (wn) “they”, stem ÙÙØªØ­> (yftH) “open”, and lemma ÙØªØ­> (ftH) “the concept of opening”.","From a large diacritized corpus, we constructed a dictionary of words and their possible diacritizations ordered by number of occurrences of each diacritized form. This diacritized corpus was created by a commercial vendor and contains 9.7 million words with almost 200K unique surface words. About 73% of the corpus is in MSA and covers variety of genres like politics, economy, sports, society, etc. and the remaining part is mostly religious texts written in classical Arabic (CA). The effectiveness of using this corpus in building state-of-the-art diacritizer was proven in BIBREF10 .For example, the word ÙØ¨ÙÙØ¯> (wbnwd) “and items” is found 4 times in this corpus with two full diacritization forms ÙÙØ¨ÙÙÙÙØ¯ÙØ ÙÙØ¨ÙÙÙÙØ¯Ù> (wabunudi, wabunudK) “items, with different grammatical case endings” which appeared 3 times and once respectively. All unique undiacritized words in this corpus were analyzed using Buckwalter morphological analyzer which gives all possible word diacritizations, and their segmentation, POS tag and lemma as shown in Figure FIGREF3 .","While comparing two diacritized forms from the corpus and Buckwalter analysis, special cases were applied to solve inconsistencies between the two diacritization schemas, for example while words are fully diacritized in the corpus, Buckwalter analysis gives diacritics without case ending (i.e. without context), and removes short vowels in some cases, for example before long vowels, and after the definite article Ø§Ù> (Al) “the”, etc.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the three datasets used in the paper?,Sample Answer,1909.04181-Data-0,1909.04181-Experiments-0,1909.04181-Experiments ::: User-Level Models-0,1909.04181-Conclusion-0,1909.04181-3-Table1-1.png,"For the purpose of our experiments, we use data released by the APDA shared task organizers. The dataset is divided into train and test by organizers. The training set is distributed with labels for the three tasks of age, dialect, and gender. Following the standard shared tasks set up, the test set is distributed without labels and participants were expected to submit their predictions on test. The shared task predictions are expected by organizers at the level of users. The distribution has 100 tweets for each user, and so each tweet is distributed with a corresponding user id. As such, in total, the distributed training data has 2,250 users, contributing a total of 225,000 tweets. The official task test set contains 720,00 tweets posted by 720 users. For our experiments, we split the training data released by organizers into 90% TRAIN set (202,500 tweets from 2,025 users) and 10% DEV set (22,500 tweets from 225 users). The age task labels come from the tagset {under-25, between-25 and 34, above-35}. For dialects, the data are labeled with 15 classes, from the set {Algeria, Egypt, Iraq, Kuwait, Lebanon-Syria, Lybia, Morocco, Oman, Palestine-Jordan, Qatar, Saudi Arabia, Sudan, Tunisia, UAE, Yemen}. The gender task involves binary labels from the set {male, female}.","As explained earlier, the shared task is set up at the user level where the age, dialect, and gender of each user are the required predictions. In our experiments, we first model the task at the tweet level and then port these predictions at the user level. For our core modelling, we fine-tune BERT on the shared task data. We also introduce an additional in-house dataset labeled with dialect and gender tags to the task as we will explain below. As a baseline, we use a small gated recurrent units (GRU) model. We now introduce our tweet-level models.","Our afore-mentioned models identify user's profiling on the tweet-level, rather than directly detecting the labels of a user. Hence, we follow the work of Zhang & Abdul-Mageed BIBREF4 to identify user-level labels. For each of the three tasks, we use tweet-level predicted labels (and associated softmax values) as a proxy for user-level labels. For each predicted label, we use the softmax value as a threshold for including only highest confidently predicted tweets. Since in some cases softmax values can be low, we try all values between 0.00 and 0.99 to take a softmax-based majority class as the user-level predicted label, fine-tuning on our DEV set. Using this method, we acquire the following results at the user level: BERT models obtain an accuracy of 55.56% for age, 96.00% for dialect, and 80.00% for gender. BERT_EXT models achieve 95.56% accuracy for dialect and 84.00% accuracy for gender.","In this work, we described our submitted models to the Arabic author profiling and deception detection shared task (APDA) BIBREF0. We focused on detecting age, dialect, and gender using BERT models under various data conditions, showing the utility of additional, in-house data on the task. We also showed that a majority vote of our models trained under different conditions outperforms single models on the official evaluation. In the future, we will investigate automatically extending training data for these tasks as well as better representation learning methods.",Table 1. Tweet level results on DEV,1.0,1.0,1.0,1.0,1.0,0.33333333333333337,1.0,0.2
How much better peformance is achieved in human evaluation when model is trained considering proposed metric?,Sample Answer,2002.05058-Introduction-2,2002.05058-Introduction-4,2002.05058-Methodology ::: Skill Rating-0,2002.05058-Methodology ::: Skill Rating-1,2002.05058-Experiments ::: Qualitative Analysis-0,"To tackle the aforementioned problems, in this paper, we propose a self-supervised approach with transfer learning to learn to compare the quality of two samples as an automated comparative Turing test. The motivation of our approach is that we can better assess the quality of generated samples or trained NLG model by comparing it with another one. Our model is a text pair classification model trained to compare the task-specific quality of two samples, which is then used to evaluate the quality of trained NLG models. As human preference annotation is generally expensive, our model is designed to be able to perform self-supervised training using only generated samples and gold reference samples without human preference annotation. When human preference annotation is available, our model can be further fine-tuned to better imitate human judgment. To evaluate the model-level quality of NLG models based on pairwise comparison in sample-level, we adopt the skill rating system similar to ELO BIBREF9 and Trueskill BIBREF10, which is a method for assigning a numerical skill to players in a player-vs-player game, given a win-loss record of games played. In our scenario, the players are NLG models to be evaluated and a higher rating indicates a better model. The skill rating system makes it possible to evaluate all n models without needing to run $n^{2}$ matches and is able to take into account the amount of new information each comparison provides.","We propose a “learning to compare” model to better assess the quality of text generated by NLG models based on pairwise comparison. Our model is able to transfer natural language understanding knowledge from BERT by fine-tuning in a self-supervised way while also able to be further fine-tuned with human preference annotation. Once trained, our model is able to perform inter-model comparison without the need for gold references, which greatly enlarges the potentially available test set and reduces the potential risk of overfitting the reference in the test set.","In player-vs-player games such as chess or tennis, skill rating systems such as Elo BIBREF9 or Glicko2 BIBREF23 evaluate players by observing a record of wins and losses of multiple players and inferring the value of a latent, unobserved skill variable for each player that explains the records of wins and losses. We adopt the skill rating system for model-level evaluation of NLG models. By taking the trained comparative evaluator as the “playground” and NLG models as “player”, the “player-vs-player” game is played by sampling one output sample from each NLG model conditioning on the same input and the game output is decided by the comparative evaluator.","Following previous work BIBREF20, in our paper, we use the Glicko2 system BIBREF23. The employed system can be summarized as follows: each player's skill rating is represented as a Gaussian distribution, with a mean and standard deviation, representing the current state of the evidence about their “true” skill rating. As we evaluate frozen snapshots of NLG models, we disabled an irrelevant feature of Glicko2 that increases uncertainty about a human player’s skill when they have not participated in a match for some time. Another difference is that conventional skill rating systems do not support the “tie” option, which is important for the system to be stable and reliable in our case because the evaluator is not perfect. To incorporate this feature, we follow the intuition that a player's skill rating should be increased when it draws with another player with a higher skill rating and vice versa. We come up with a simple rule which increases/decreases the skill rating of one player by a ratio (e.g. 0.1) of the changes in its skill rating when it wins/loses if it draws with another player with higher/lower skill rating. In our experiments, the skill rating is performed by randomly sampling two compared models, simulating a “game” between two selected models by sampling one sample from each model and comparing them with the comparative evaluator, and then updating the skill rating of selected models according to the outcome. This procedure is performed iteratively until convergence, which is defined as the order of skill ratings of compared models keeps the same after each model is selected at least 50 times. While the sampling procedure can be optimized by bayesian optimization BIBREF24 or multi-armed bandit algorithms BIBREF25, we choose to keep the method as simple as possible and use random sampling.","We present several comparison examples in the Dailydialog dataset for qualitative analysis of the proposed comparative evaluator. From the first example, we can see that the comparative evaluator is capable of identifying that generic and dull responses (i.e. “I don't know”) should be considered as of worse quality. The second example suggests that our approach handles the diversity in possible responses well, as it regards both positive response and negative response as valid responses. Hopefully, these examples may provide us with some insights about why the proposed metric correlates better with human preference.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What datasets do they use?,Sample Answer,2001.07820-Introduction-6,2001.07820-Introduction-7,2001.07820-Evaluation ::: Automatic Evaluation: Results-2,2001.07820-Evaluation ::: Human Evaluation: Design-3,2001.07820-Evaluation ::: Human Evaluation: Design-7,preserve its original label.,"These two additional criteria are generally irrelevant for images, as adding minor perturbations to an image is unlikely to: (1) create an uninterpretable image (where else changing one word in a sentence can render a sentence incoherent), or (2) change how we perceive the image, say from seeing a panda to a gibbon (but a sentence's sentiment can be reversed by simply adding a negative adverb such as not). Without considering criterion (d), generating adversarial examples in NLP would be trivial, as the model can learn to simply replace a positive adjective (amazing) with a negative one (awful) to attack a sentiment classifier.","Comparing the performance across different ACC thresholds, we observe a consistent pattern of decreasing performance over all metrics as the attacking performance increases from T0 to T2. These observations suggest that all methods are trading off fluency and content preservation as they attempt to generate stronger adversarial examples.",Is snippet B a good paraphrase of snippet A?,What is the sentiment of the text?,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are two use cases that demonstrate capability of created system?,Sample Answer,1909.08250-Background: Grammatical Framework-3,1909.08250-Method ::: GF Grammar Encoder-0,1909.08250-Method ::: GF Grammar Encoder-3,1909.08250-Method ::: GF Grammar Exporter-0,1909.08250-4-Figure1-1.png,"In these concrete syntaxes, the linearization type definition (lincat) states that Message, People, Action and Entity are type Cl (clause), NP (noun phrase), V2 (two-place verb), and NP respectively. Linearization definitions (lin) indicate what strings are assigned to each of the meanings defined in the abstract syntax. To reduce same string declaration, the operator (oper) section defines some placeholders for strings that can be used in linearization. The mkNP, mkN, mkV2, etc. are standard constructors from ConstructorsEng/Jpn library which returns an object of the type NP, N or V2 respectively.","The goal of the encoder is to identify appropriate GF rules for the construction of a GF grammar of a sentence given its structure and its components identified in the previous two modules. This is necessary since a sentence can be encoded in GF by more than one set of rules; for example, the sentence “Bill wants to play a game” can be encoded by the rules","In GF, NP, VV, V2, VP, and Cl stand for noun phrase, verb-phrase-complement verb, two-place verb, verb phrase and clause, respectively. Note that although the set of GF grammatical rules can be used to construct a constituency-based parse tree , the reverse direction is not always true. To the best of our knowledge, there exists no algorithm for converting a constituency-based parse tree to a set GF grammar rules. We therefore need to identify the GF rules for each sentence structure.","The GF Grammar Exporter has the simplest job among all modules in the system. It creates a GF program for a paragraph using the GF grammars created for the sentences of the paragraph. By taking the union of all respective elements of each grammar for each sentence, i.e., categories, functions, linearizations and operators, the Grammar Exporter will group them into the set of categories (respectively, categories, functions, linearizations, operators) of the final grammar.",Figure 1: Google translation for the Japanese sentence generated by GF. The two sentences in English and in Italian are the two representations of the meaning encoded in the abstract syntax.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is an example of a health-related tweet?,Sample Answer,1901.00439-Conventional Representations-2,1901.00439-Conventional Representations-3,1901.00439-Representation Learning-4,1901.00439-Representation Learning-5,1901.00439-Results-0,"Principal Component Analysis (PCA): PCA is used to map the word frequency representations from the original feature space to a lower dimensional feature space by an orthogonal linear transformation in such a way that the first principal component has the highest possible variance and similarly, each succeeding component has the highest variance possible while being orthogonal to the preceding components. Our PCA implementation has a time complexity of $\mathcal {O}(NP^2 + P^3)$ .","Truncated Singular Value Decomposition (t-SVD): Standard SVD and t-SVD are commonly employed dimensionality reduction techniques in which a matrix is reduced or approximated into a low-rank decomposition. Time complexity of SVD and t-SVD for $S$ components are $\mathcal {O}(min(NP^2, N^2P))$ and $\mathcal {O}(N^2S)$ , respectively (depending on the implementation). Contrary to PCA, t-SVD can be applied to sparse matrices efficiently as it does not require data normalization. When the data matrix is obtained by BoWs or tf-idf representations as in our case, the technique is also known as Latent Semantic Analysis.","The encoder in the proposed architecture consists of three 2D convolutional layers with 64, 32 and 1 filters, respectively. The decoder follows the same symmetry with three convolutional layers with 1, 32 and 64 filters, respectively and an output convolutional layer of a single filter (see Figure 1 ). All convolutional layers have a kernel size of (3 $\times $ 3) and an activation function of Rectified Linear Unit (ReLU) except the output layer which employs a linear activation function. Each convolutional layer in the encoder is followed by a 2D MaxPooling layer and similarly each convolutional layer in the decoder is followed by a 2D UpSampling layer, serving as an inverse operation (having the same parameters). The pooling sizes for pooling layers are (2 $\times $ 5), (2 $\times $ 5) and (2 $\times $ 2), respectively for the architectures when word2vec, GloVe and fastText embeddings are employed. With this configuration, an input tweet of size $32 \times 300$ (corresponding to maximum sequence length $\times $ embedding dimension, $D$ ) is downsampled to size of $4 \times 6$ out of the encoder (bottleneck layer). As BERT word embeddings have word vectors of fixed size 768, the pooling layer sizes are chosen to be (2 $\times $ 8), (2 $\times $ 8) and (2 $\times $0 2), respectively for that case. In summary, a representation of $\times $1 values is learned for each tweet through the encoder, e.g., for fastText embeddings the flow of dimensions after each encoder block is as such : $\times $2 .","In numerous NLP tasks, an Embedding Layer is employed as the first layer of the neural network which can be initialized with the word embedding matrix in order to incorporate the embedding process into the architecture itself instead of manual extraction. In our case, this was not possible because of nonexistence of an inversed embedding layer in the decoder (as in the relationship between MaxPooling layers and UpSampling layers) as an embedding layer is not differentiable.","Performance of the representations tested on 3 different clustering algorithms, i.e., CH scores, for 3 different cluster numbers can be examined from Table 2 . $L_2$ -norm constrained CAE is simply referred as $L_2$ -CAE in Table 2 . Same table shows the number of features used for each method as well. Document-term matrix extracted by BoWs and tf-idf features result in a sparse matrix of $63,326 \times 13,026$ with a sparsity of 0.9994733. Similarly, concatenation of word embeddings result in a high number of features with $32 \times 300 = 9,600$ for word2vec, GloVe and fastText, $32 \times 768 = 24,576$ for BERT embeddings. In summary, the proposed method of learning representations of tweets with CAEs outperform all of the conventional algorithms. When representations are compared with Hotelling's $T^2$ test (multivariate version of $t$ -test), every representation distribution learned by CAEs are shown to be statistically significantly different than every other conventional representation distribution with $p<0.001$ . In addition, introducing the $L_2$ -norm constraint on the learned representations during training enhances the clustering performance further (again $p<0.001$ when comparing for example fastText+CAE vs. fastText+ $L_2$0 -CAE). An example learning curve for CAE and $L_2$1 -CAE with fastText embeddings as input can also be seen in Figure 2 .",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the two PharmaCoNER subtasks?,Sample Answer,1912.09152-Development-2,1912.09152-Development-4,1912.09152-Discussion-0,1912.09152-Discussion-3,1912.09152-Acknowledgements-0,(?:antígeno )?CD[- ]?13,"With the previous regexp, the system is able to identify (and string-normalize) six different textual realizations of the same unique snomed ct term. There are more complex rules that, thus, produce many more potential strings. The important thing with this strategy is that through the generative power of these predictably-created regexps from snomed ct entities the system is able to improve its recall and overcome the limitations of traditional dictionary-based approaches.","In this section, we perform error analysis for our system run on the test dataset. We will address both recall and precision errors, but mainly concentrate on the latter type, and on a thorough revision of mismatches between system and human annotations.","In the following paragraphs, some of the most relevant inconsistencies found when performing error analysis of our system are highlighted. The list is necessarily incomplete due to space constraints, and it is geared towards the explanation of our possible errors.",We thank three anonymous reviewers of our manuscript for their careful reading and their many insightful comments and suggestions. We have made our best in providing a revised version of the manuscript that reflects their suggestions. Any remaining errors are our own responsability.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What summarization algorithms did the authors experiment with?,Sample Answer,1712.00991-Introduction-0,1712.00991-Introduction-2,1712.00991-Summarization of Peer Feedback using ILP-3,1712.00991-Summarization of Peer Feedback using ILP-7,1712.00991-Summarization of Peer Feedback using ILP-8,"Performance appraisal (PA) is an important HR process, particularly for modern organizations that crucially depend on the skills and expertise of their workforce. The PA process enables an organization to periodically measure and evaluate every employee's performance. It also provides a mechanism to link the goals established by the organization to its each employee's day-to-day activities and performance. Design and analysis of PA processes is a lively area of research within the HR community BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 .","A typical PA process involves purposeful multi-step multi-modal communication between employees, their supervisors and their peers. In most PA processes, the communication includes the following steps: (i) in self-appraisal, an employee records his/her achievements, activities, tasks handled etc.; (ii) in supervisor assessment, the supervisor provides the criticism, evaluation and suggestions for improvement of performance etc.; and (iii) in peer feedback (aka INLINEFORM0 view), the peers of the employee provide their feedback. There are several business questions that managers are interested in. Examples:",,"There is a trivial constraint INLINEFORM0 which makes sure that only INLINEFORM1 out of INLINEFORM2 candidate phrases are chosen. A suitable value of INLINEFORM3 is used for each employee depending on number of candidate phrases identified across all peers (see Algorithm SECREF6 ). Another set of constraints ( INLINEFORM4 to INLINEFORM5 ) make sure that at least one phrase is selected for each of the leadership attributes. The constraint INLINEFORM6 makes sure that multiple phrases sharing the same headword are not chosen at a time. Also, single word candidate phrases are chosen only if they are adjectives or nouns with lexical category noun.attribute. This is imposed by the constraint INLINEFORM7 . It is important to note that all the constraints except INLINEFORM8 are soft constraints, i.e. there may be feasible solutions which do not satisfy some of these constraints. But each constraint which is not satisfied, results in a penalty through the use of slack variables. These constraints are described in detail in Table TABREF36 .","The objective function maximizes the total importance score of the selected candidate phrases. At the same time, it also minimizes the sum of all slack variables so that the minimum number of constraints are broken.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What methods were used for sentence classification?,Sample Answer,1712.00991-Introduction-0,1712.00991-PA along Attributes-4,1712.00991-Summarization of Peer Feedback using ILP-0,1712.00991-Summarization of Peer Feedback using ILP-3,1712.00991-Summarization of Peer Feedback using ILP-6,"Performance appraisal (PA) is an important HR process, particularly for modern organizations that crucially depend on the skills and expertise of their workforce. The PA process enables an organization to periodically measure and evaluate every employee's performance. It also provides a mechanism to link the goals established by the organization to its each employee's day-to-day activities and performance. Design and analysis of PA processes is a lively area of research within the HR community BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 .","It can be observed that INLINEFORM0 would be undefined if INLINEFORM1 is empty and similarly INLINEFORM2 would be undefined when INLINEFORM3 is empty. Hence, overall precision and recall are computed by averaging over all the instances except where they are undefined. Instance-level F-measure can not be computed for instances where either precision or recall are undefined. Therefore, overall F-measure is computed using the overall precision and recall.","The PA system includes a set of peer feedback comments for each employee. To answer the third question in Section SECREF1 , we need to create a summary of all the peer feedback comments about a given employee. As an example, following are the feedback comments from 5 peers of an employee.",,A complete list of parameters is described in detail in Table TABREF36 .,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
By how much does their model outperform the baseline?,Sample Answer,2002.09616-Proposed Framework ::: Imaginator-0,2002.09616-Proposed Framework ::: Arbitrator-2,2002.09616-Experimental Setup ::: Datasets-2,2002.09616-Experimental Setup ::: Datasets Modification-3,2002.09616-Conclusion-0,"An imaginator is a natural language generator generating next sentence given the dialogue history. There are two imaginators in our method, agent's imaginator and user's imaginator. The goal of the two imaginators are to learn the agent’s and user’s speaking style respectively and generate possible future utterances.","We adopt several architectures like Bi-GRUs, TextCNNs and BERT as the basis of arbitrator module. We will show how to build an arbitrator by taking TextCNNs as an example.","DailyDialogue BIBREF21. DailyDialogue is a high-quality multi-turn dialogue dataset, which contains conversations about daily life. In this dataset, humans often first respond to previous context and then propose their own questions and suggestions. In this way, people show their attention others’ words and are willing to continue the conversation. Compare to the task-oriented dialogue datasets, the speaker's behavior will be more unpredictable and complex for the arbitrator.","Add Turn Tag We add turn tags, subturn tags and role tags to each split and original sentences to (1) label the speaker role and dialogue turns (2) tag the ground truth for training and testing the supervised baselines and our model.","We first address an interaction problem, whether the dialogue model should wait for the end of the utterance or reply directly in order to simulate user's real life conversation behavior, and propose a novel Imagine-then-Arbitrate (ITA) neural dialogue model to deal with it. Our model introduces the imagined future possible semantic information for prediction. We modified two popular dialogue datasets to fit in the real situation. It is reasonable that additional information is helpful for arbitrator, despite its fantasy.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What experiments are used to demonstrate the benefits of this approach?,Sample Answer,1709.10367-Model Description-0,1709.10367-Background: Exponential Family Embeddings-3,1709.10367-Background: Exponential Family Embeddings-9,1709.10367-Structured Exponential Family Embeddings-16,1709.10367-9-Table3-1.png,"In this section, we develop sefe, a model that builds on efe BIBREF10 to capture semantic variations across groups of data. In embedding models, we represent each object (e.g., a word in text, or an item in shopping data) using two sets of vectors, an embedding vector and a context vector. In this paper, we are interested in how the embeddings vary across groups of data, and for each object we want to learn a separate embedding vector for each group. Having a separate embedding for each group allows us to study how the usage of a word like 1.10intelligence varies across categories of the ArXiv, or which words are used most differently by U.S. Senators depending on which state they are from and whether they are Democrats or Republicans.","Exponential family embeddings learn the vector representation of objects based on the conditional probability of each observation, conditioned on the observations in its context. The context INLINEFORM0 gives the indices of the observations that appear in the conditional probability distribution of INLINEFORM1 . The definition of the context varies across applications. In text, it corresponds to the set of words in a fixed-size window centered at location INLINEFORM2 .","Note that maximizing the regularized conditional likelihood is not equivalent to maximum a posteriori. Rather, it is similar to maximization of the pseudo-likelihood in conditionally specified models BIBREF26 , BIBREF10 .","The context INLINEFORM0 is given at each position INLINEFORM1 by the set of surrounding words in the document, according to a fixed-size window.",Table 3: List of the three most different words for different groups for the Congressional speeches. S-EFE uncovers which words are used most differently by Republican Senators (red) and Democratic Senators (blue) from different states. The complete table is in the Appendix.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How does TP-N2F compare to LSTM-based Seq2Seq in terms of training and inference speed?,Sample Answer,1910.02339-Background: Review of Tensor-Product Representation-5,1910.02339-Appendix ::: Detailed equations of TP-N2F ::: TP-N2F encoder-13,1910.02339-Appendix ::: The tensor that is input to the decoder's Unbinding Module is a TPR-0,1910.02339-Appendix ::: The tensor that is input to the decoder's Unbinding Module is a TPR-2,1910.02339-Appendix ::: Generated programs comparison-1,"Using summation in Eq.SECREF2 to combine the vectors embedding the constituents of a structure risks non-recoverability of those constituents given the embedding $$ of the the structure as a whole. The tensor product is chosen as the binding operation in order to enable recovery of the filler of any role in a structure ${S}$ given its TPR $$. This can be done with perfect precision if the embeddings of the roles are linearly independent. In that case the role matrix $$ has a left inverse $$: $= $. Now define the unbinding (or dual) vector for role $r_j$, $_j$, to be the $j^{{\mathrm {th}}}$ column of $^\top $: $U_{:j}^\top $. Then, since $[]_{ji} = []_{ji} = _{j:} _{:i} = [^\top _{:j}]^\top _{:i} =_j^\top _i = _i^\top _j$, we have $_i^\top _j = \delta _{ji}$. This means that, to recover the filler of $r_j$ in the structure with TPR $$, we can take its tensor inner product (or matrix-vector product) with $_j$: j = [ i i i] j = i i ij = j","Similar to the Filler-LSTM, the Role-LSTM is also a standard LSTM, governed by the equations:","Here we show that, if learning is successful, the order-3 tensor $$ that each iteration of the decoder's Tuple LSTM feeds to the decoder's Unbinding Module (Figure FIGREF13) will be a TPR of the form assumed in Eq. SECREF7, repeated here: = j j rel j. The operations performed by the decoder are given in Eqs. SECREF7–SECREF7, and Eqs. SECREF12–SECREF12, rewritten here: i' = i","Treat rank-2 tensors as matrices; then unbinding is simply matrix-vector multiplication. Assume the set of unbinding vectors is linearly independent (otherwise there would in general be no way to satisfy Eq. SECREF65 exactly, contrary to assumption). Then expand the set of unbinding vectors, if necessary, into a basis $\lbrace ^{\prime }_k\rbrace _{k \in K \supseteq I}$. Find the dual basis, with $_k$ dual to $^{\prime }_k$ (so that $_l^\top _j^{\prime } = \delta _{lj}$). Because $\lbrace ^{\prime }_k\rbrace _{k \in K}$ is a basis, so is $\lbrace _k\rbrace _{k \in K}$, so any matrix $$ can be expanded as $= \sum _{k \in K} _k _k^{\top }$. Since $^{\prime }_i = _i, \forall i \in I$ are the unbinding conditions (Eq. SECREF65), we must have $_i = _i, i \in I$. Let $_{{\mathrm {TPR}}} \equiv \sum _{i \in I} _i _i^{\top }$. This is the desired TPR, with fillers $_i$ bound to the role vectors $_i$ which are the duals of the unbinding vectors $_i^{\prime }$ ($i \in I$). Then we have $= _{{\mathrm {TPR}}} + $ (Eq. SECREF65) where $\equiv \sum _{j \in K, j \notin I} _j _j^{\top }$; so $_i^{\prime } = {\mathbf {0}}, i \in I$ (Eq. SECREF65). Thus, if training is successful, the model must have learned how to feed the decoder with order-3 TPRs with the structure posited in Eq. SECREF65.",Question: A train running at the speed of 50 km per hour crosses a post in 4 seconds. What is the length of the train?,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the performance proposed model achieved on AlgoList benchmark?,Sample Answer,1910.02339-Background: Review of Tensor-Product Representation-0,1910.02339-TP-N2F Model-0,1910.02339-CONCLUSION AND FUTURE WORK-0,1910.02339-Appendix ::: Implementations of TP-N2F for experiments-0,1910.02339-Appendix ::: Detailed equations of TP-N2F ::: TP-N2F decoder-6,"The TPR mechanism is a method to create a vector space embedding of complex symbolic structures. The type of a symbol structure is defined by a set of structural positions or roles, such as the left-child-of-root position in a tree, or the second-argument-of-$R$ position of a given relation $R$. In a particular instance of a structural type, each of these roles may be occupied by a particular filler, which can be an atomic symbol or a substructure (e.g., the entire left sub-tree of a binary tree can serve as the filler of the role left-child-of-root). For now, we assume the fillers to be atomic symbols.","We propose a general TP-N2F neural network architecture operating over TPRs to solve N2F tasks under a proposed role-level description of those tasks. In this description, natural-language input is represented as a straightforward order-2 role structure, and formal-language relational representations of outputs are represented with a new order-3 recursive role structure proposed here. Figure FIGREF3 shows an overview diagram of the TP-N2F model. It depicts the following high-level description.","In this paper we propose a new scheme for neural-symbolic relational representations and a new architecture, TP-N2F, for formal-language generation from natural-language descriptions. To our knowledge, TP-N2F is the first model that combines TPR binding and TPR unbinding in the encoder-decoder fashion. TP-N2F achieves the state-of-the-art on two instances of N2F tasks, showing significant structure learning ability. The results show that both the TP-N2F encoder and the TP-N2F decoder are important for improving natural- to formal-language generation. We believe that the interpretation and symbolic structure encoding of TPRs are a promising direction for future work. We also plan to combine large-scale deep learning models such as BERT with TP-N2F to take advantage of structure learning for other generation tasks.","In this section, we present details of the experiments of TP-N2F on the two datasets. We present the implementation of TP-N2F on each dataset.","t RdH ${\mathrm {Atten}}$ is the attention mechanism used in BIBREF13, which computes the dot product between $_{\mathrm {input}}^t$ and each $_{t^{\prime }}$. Then a linear function is used on the concatenation of $_{\mathrm {input}}^t$ and the softmax scores on all dot products to generate $^t$. The following equations show the attention mechanism:",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the performance proposed model achieved on MathQA?,Sample Answer,1910.02339-TP-N2F Model-0,1910.02339-Appendix ::: Implementations of TP-N2F for experiments-0,1910.02339-Appendix ::: The tensor that is input to the decoder's Unbinding Module is a TPR-2,1910.02339-Appendix ::: Dataset samples ::: Data sample from AlgoLisp dataset-0,1910.02339-Appendix ::: Generated programs comparison-36,"We propose a general TP-N2F neural network architecture operating over TPRs to solve N2F tasks under a proposed role-level description of those tasks. In this description, natural-language input is represented as a straightforward order-2 role structure, and formal-language relational representations of outputs are represented with a new order-3 recursive role structure proposed here. Figure FIGREF3 shows an overview diagram of the TP-N2F model. It depicts the following high-level description.","In this section, we present details of the experiments of TP-N2F on the two datasets. We present the implementation of TP-N2F on each dataset.","Treat rank-2 tensors as matrices; then unbinding is simply matrix-vector multiplication. Assume the set of unbinding vectors is linearly independent (otherwise there would in general be no way to satisfy Eq. SECREF65 exactly, contrary to assumption). Then expand the set of unbinding vectors, if necessary, into a basis $\lbrace ^{\prime }_k\rbrace _{k \in K \supseteq I}$. Find the dual basis, with $_k$ dual to $^{\prime }_k$ (so that $_l^\top _j^{\prime } = \delta _{lj}$). Because $\lbrace ^{\prime }_k\rbrace _{k \in K}$ is a basis, so is $\lbrace _k\rbrace _{k \in K}$, so any matrix $$ can be expanded as $= \sum _{k \in K} _k _k^{\top }$. Since $^{\prime }_i = _i, \forall i \in I$ are the unbinding conditions (Eq. SECREF65), we must have $_i = _i, i \in I$. Let $_{{\mathrm {TPR}}} \equiv \sum _{i \in I} _i _i^{\top }$. This is the desired TPR, with fillers $_i$ bound to the role vectors $_i$ which are the duals of the unbinding vectors $_i^{\prime }$ ($i \in I$). Then we have $= _{{\mathrm {TPR}}} + $ (Eq. SECREF65) where $\equiv \sum _{j \in K, j \notin I} _j _j^{\top }$; so $_i^{\prime } = {\mathbf {0}}, i \in I$ (Eq. SECREF65). Thus, if training is successful, the model must have learned how to feed the decoder with order-3 TPRs with the structure posited in Eq. SECREF65.","Problem: Consider an array of numbers and a number, decrements each element in the given array by the given number, what is the given array?","Question: You are given an array of numbers a and numbers b , c and d , let how many times you can replace the median in a with sum of its digits before it becomes a single digit number and b be the coordinates of one end and c and d be the coordinates of another end of segment e , your task is to find the length of segment e rounded down",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What 20 domains are available for selection of source domain?,Sample Answer,2004.04478-Introduction-0,2004.04478-Similarity Metrics-2,2004.04478-Similarity Metrics ::: Metrics: Labelled Data ::: LM4: Entropy Change-4,2004.04478-Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM6: ELMo-1,2004.04478-Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM7: Universal Sentence Encoder-0,"Sentiment analysis (SA) deals with automatic detection of opinion orientation in text BIBREF0. Domain-specificity of sentiment words, and, as a result, sentiment analysis is also a well-known challenge. A popular example being `unpredictable' that is positive for a book review (as in `The plot of the book is unpredictable') but negative for an automobile review (as in `The steering of the car is unpredictable'). Therefore, a classifier that has been trained on book reviews may not perform as well for automobile reviews BIBREF1.","Labelled Data: Here, each review in the target domain data is labelled either positive or negative, and a number of such labelled reviews are insufficient in size for training an efficient model.","Note that this metric offers the advantage of asymmetricity, unlike the other three metrics for labelled data.","In ELMo, higher-level LSTM states capture the context-dependent aspects of word meaning. Therefore, we use only the topmost layer for word embeddings with 1024 dimensions. Multiple contextual embeddings of a word are averaged to obtain a single vector. We again use average Angular Similarity of word embeddings for common adjectives to compare domain-pairs along with Jaccard Similarity Coefficient. The final similarity value is obtained using equation (DISPLAY_FORM29).",One of the most recent contributions to the area of sentence embeddings is the Universal Sentence Encoder. Its transformer-based sentence encoding model constructs sentence embeddings using the encoding sub-graph of the transformer architecture BIBREF19. We leverage these embeddings and devise a metric for our work.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Do the images have multilingual annotations or monolingual ones?,Sample Answer,1905.12260-Introduction-0,1905.12260-Introduction-5,1905.12260-Related Work-2,1905.12260-Methods-2,1905.12260-Co-Occurrence Only-1,"Recent advances in learning distributed representations for words (i.e., word embeddings) have resulted in improvements across numerous natural language understanding tasks BIBREF0 , BIBREF1 . These methods use unlabeled text corpora to model the semantic content of words using their co-occurring context words. Key to this is the observation that semantically similar words have similar contexts BIBREF2 , thus leading to similar word embeddings. A limitation of these word embedding approaches is that they only produce monolingual embeddings. This is because word co-occurrences are very likely to be limited to being within language rather than across language in text corpora. Hence semantically similar words across languages are unlikely to have similar word embeddings.","There has been other recent work on reducing the amount of supervision required to learn multilingual embeddings (cf. Section ""Related Work"" ). These methods take monolingual embeddings learned using existing methods and align them post-hoc in a shared embedding space. A limitation with post-hoc alignment of monolingual embeddings, first noticed by BIBREF12 , is that doing training of monolingual embeddings and alignment separately may lead to worse results than joint training of embeddings in one step. Since the monolingual embedding objective is distinct from the multilingual embedding objective, monolingual embeddings are not required to capture all information helpful for post-hoc multilingual alignment. Post-hoc alignment loses out on some information, whereas joint training does not. BIBREF12 observe improved results using a joint training method compared to a similar post-hoc method. Thus, a joint training approach is desirable. To our knowledge, no previous method jointly learns multilingual word embeddings using weakly-supervised data available for low-resource languages.","There has been some recent work on reducing the amount of human-labeled data required to learn multilingual embeddings, enabling work on low-resource languages BIBREF19 , BIBREF20 , BIBREF21 . These methods take monolingual embeddings learned using existing methods and align them post-hoc in a shared embedding space, exploiting the structural similarity of monolingual embedding spaces first noticed by BIBREF13 . As discussed in Section ""Introduction"" , post-hoc alignment of monolingual embeddings is inherently suboptimal. For example, BIBREF19 and BIBREF20 use human-labeled data, along with shared surface forms across languages, to learn an alignment in the bilingual setting. BIBREF21 build on this for the multilingual setting, using no human-labeled data and instead using an adversarial approach to maximize alignment between monolingual embedding spaces given their structural similarities. This method (MUSE) outperforms previous approaches and represents the state-of-the-art. We compare it to our methods in Section ""Results and Conclusions"" .","where $Q_q$ is the query representation for query $q$ ; $I_i$ is the image representation corresponding to image $i$ ; $j$ ranges over all images in the corpus; and $Q_q^T I_i$ is the dot product of the vectors $Q_q$ and $I_i$ . Note that this requires that $Q_q$ and $I_j$ have identical dimensionality. If a weight $q$0 is provided for the (query, image) pair, the loss is multiplied by the weight. Observe that $q$1 and $q$2 remain unspecified for now: we detail different experiments involving different representations below.","In this setting, we keep query representations the same, and we modify image representations as follows: the image representation for an image is a randomly initialized, trainable vector (of the same dimensionality as the query representation, to ensure the cosine similarity can be calculated). The intuition for this approach is that if two queries are both associated with an image, their query representations will both be constrained to be similar to the same vector, and so the query representations themselves are constrained to be similar. This approach is a simple way to adapt our method to make use of only co-occurrence statistics.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How much important is the visual grounding in the learning of the multilingual representations?,Sample Answer,1905.12260-Methods-3,1905.12260-Leveraging Image Understanding-4,1905.12260-Co-Occurrence Only-0,1905.12260-Co-Occurrence Only-1,1905.12260-Evaluation-3,"In practice, given the size of our dataset, calculating the full denominator of the loss for a query, image pair would involve iterating through each image for each query, which is $O(n^2)$ in the number of training examples. To remedy this, we calculated the loss within each batch separately. That is, the denominator of the loss only involved summing over images in the same batch as the query. We used a batch size of 1000 for all experiments. In principle, the negative sampling approach used by BIBREF0 could be used instead to prevent quadratic time complexity.","Note that each word in each query is prefixed with the language of the query. For example, the English query “back pain” is treated as “en:back en:pain”, and the multilingual embeddings that are averaged are those for “en:back” and “en:pain”. This means that words in different languages with shared surface forms are given separate embeddings. We experiment with shared embeddings for words with shared surface forms in Section ""Discussion"" .","Another approach for generating query and image representations is treating images as a black box. Without using pixel data, how well can we do? Given the statistics of our dataset (3B query, image pairs with 220M unique queries and 900M unique images), we know that different queries co-occur with the same images. Intuitively, if a query $q_1$ co-occurs with many of the same images as query $q_2$ , then $q_1$ and $q_2$ are likely to be semantically similar, regardless of the visual content of the shared images. Thus, we can use a method that uses only co-occurrence statistics to better understand how well we can capture relationships between queries. This method serves as a baseline to our initial approach leveraging image understanding.","In this setting, we keep query representations the same, and we modify image representations as follows: the image representation for an image is a randomly initialized, trainable vector (of the same dimensionality as the query representation, to ensure the cosine similarity can be calculated). The intuition for this approach is that if two queries are both associated with an image, their query representations will both be constrained to be similar to the same vector, and so the query representations themselves are constrained to be similar. This approach is a simple way to adapt our method to make use of only co-occurrence statistics.","In this task, a classifier built on top of learned multilingual embeddings is trained on the RCV corpus of newswire text as in BIBREF15 and BIBREF4 . The corpus consists of documents in seven languages on four topics, and the classifier predicts the topic. The score on the task is test accuracy. Note that each document is monolingual, so this task measures performance within languages for multiple languages (as opposed to crosslingual performance).",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What QA system was used in this work?,Sample Answer,1908.10149-Introduction-1,1908.10149-Related Work-1,1908.10149-Question Answering System-3,1908.10149-Baseline Performance Evaluation-3,1908.10149-8-Figure4-1.png,"With this work, we want to answer the question whether a deployed QA system that is difficult to adapt and that provides a top-10 ranking of answer candidates, can be improved by an additional re-ranking step that corresponds to the operationalisation step of the augmented CRISP cycle. It is also important to know the potential gain and the limitations of such a method that works on top of an existing system. We hypothesise that our proposed re-ranking approach can effectively improve ranking-based QA systems.","For QA dialogues based on structured knowledge representations, this can be achieved by maintaining and adapting the knowledgebase BIBREF13 , BIBREF14 , BIBREF15 . In addition, BIBREF1 proposes metacognition models for building self-reflective and adaptive AI systems, e.g., dialogue systems, that improve by introspection. Buck et al. present a method for reformulating user questions: their method automatically adapts user queries with the goal to improve the answer selection of an existing QA model BIBREF16 .",Text Classification. The spacy_sklearn pipeline relies on Scikit-learn for text classification using a support vector classifier (SVC). The model confidences are used for ranking all answer candidates; the top-10 results are returned.,"Overall, the tensorflow_embedding pipelines perform considerably better than the spacy_sklearn pipeline irrespective of the remaining parameter configuration: the best performing methods are achieved by the tensorflow_embedding pipeline with spell-checking. Figure FIGREF11 sheds more light on this particular setting. It provides performance measures for all corpora and for different number of epochs used for model training. Pipelines that use 300 epochs for training range among the best for all corpora. When adding more natural user annotations, using 100 epochs achieves similar or better scores, in particular concerning the top-10 accuracy and the MRR. Re-ranking the top-10 results can only improve the performance in QA, if the correct answer is among the top-10 results. Therefore, we use the tensorflow_embedding pipeline with spellchecking, 100 epochs and the full training corpus as baseline for evaluating the re-ranking approach.","Fig. 4 Complete QA system architecture including the re-ranking model. The re-ranking model is trained using manually annotated data for generating a supervised/ideal ranking result for the top-10 answers from the QA system. Features are extracted from the user question and a particular answer candidate. At inference time, the re-ranking model is used to improve the initial top-10 ranking.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How big is the test set used for evaluating the proposed re-ranking approach?,Sample Answer,1908.10149-Question Answering System-1,1908.10149-Question Answering System-2,1908.10149-Question Answering System-3,1908.10149-Corpora-0,1908.10149-3-Table1-1.png,Spellchecker We address the problem of frequent spelling mistakes in user queries by implementing an automated spell-checking and correction module. It is based on a Python port of the SymSpell algorithm initialized with word frequencies for German. We apply the spellchecker as first component in our pipeline.,"Pre-Processing and Feature Encoding. The spacy_sklearn pipeline uses Spacy for pre-processing and feature encoding. Pre-processing includes the generation of a Spacy document and tokenization using their German language model de_core_news_sm (v2.0.0). The feature encoding is obtained via the vector function of the Spacy document that returns the mean word embedding of all tokens in a query. For German, Spacy provides only a simple dense encoding of queries (no proper word embedding model). The pre-processing step of the tensorflow_embedding pipeline uses a simple whitespace tokenizer for token extraction. The tokens are used for the feature encoding step that is based on Scikit-learn's CountVectorizer. It returns a bag of words histogram with words being the tokens (1-grams).",Text Classification. The spacy_sklearn pipeline relies on Scikit-learn for text classification using a support vector classifier (SVC). The model confidences are used for ranking all answer candidates; the top-10 results are returned.,"In this work, we include two corpora: one for training the baseline system and another for evaluating the performance of the QA pipeline and our re-ranking approach. In the following, we describe the creation of the training corpus and the structure of the test corpus. Both corpora have been anonymised.",Table 1 Sample queries with a correct and an incorrect answer option according to our test set. We report the answers’ rank of the baseline model that we used for our re-ranking experiments.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What evidence do the authors present that the model can capture some biases in data annotation and collection?,Sample Answer,1910.12574-Introduction-1,1910.12574-Previous Works-3,1910.12574-Methodology ::: Fine-Tuning Strategies-3,1910.12574-Methodology ::: Fine-Tuning Strategies-4,1910.12574-8-Table2-1.png,"People are increasingly using social networking platforms such as Twitter, Facebook, YouTube, etc. to communicate their opinions and share information. Although the interactions among users on these platforms can lead to constructive conversations, they have been increasingly exploited for the propagation of abusive language and the organization of hate-based activities BIBREF0, BIBREF1, especially due to the mobility and anonymous environment of these online platforms. Violence attributed to online hate speech has increased worldwide. For example, in the UK, there has been a significant increase in hate speech towards the immigrant and Muslim communities following the UK's leaving the EU and the Manchester and London attacks. The US also has been a marked increase in hate speech and related crime following the Trump election. Therefore, governments and social network platforms confronting the trend must have tools to detect aggressive behavior in general, and hate speech in particular, as these forms of online aggression not only poison the social climate of the online communities that experience it, but can also provoke physical violence and serious harm BIBREF1.","Transfer Learning: Pre-trained vector representations of words, embeddings, extracted from vast amounts of text data have been encountered in almost every language-based tasks with promising results. Two of the most frequently used context-independent neural embeddings are word2vec and Glove extracted from shallow neural networks. The year 2018 has been an inflection point for different NLP tasks thanks to remarkable breakthroughs: Universal Language Model Fine-Tuning (ULMFiT) BIBREF20, Embedding from Language Models (ELMO) BIBREF21, OpenAI’ s Generative Pre-trained Transformer (GPT) BIBREF22, and Google’s BERT model BIBREF11. Howard et al. BIBREF20 proposed ULMFiT which can be applied to any NLP task by pre-training a universal language model on a general-domain corpus and then fine-tuning the model on target task data using discriminative fine-tuning. Peters et al. BIBREF21 used a bi-directional LSTM trained on a specific task to present context-sensitive representations of words in word embeddings by looking at the entire sentence. Radford et al. BIBREF22 and Devlin et al. BIBREF11 generated two transformer-based language models, OpenAI GPT and BERT respectively. OpenAI GPT BIBREF22 is an unidirectional language model while BERT BIBREF11 is the first deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus. BERT has two novel prediction tasks: Masked LM and Next Sentence Prediction. The pre-trained BERT model significantly outperformed ELMo and OpenAI GPT in a series of downstream tasks in NLP BIBREF11. Identifying hate speech and offensive language is a complicated task due to the lack of undisputed labelled data BIBREF15 and the inability of surface features to capture the subtle semantics in text. To address this issue, we use the pre-trained language model BERT for hate speech classification and try to fine-tune specific task by leveraging information from different transformer encoders.","3. Insert Bi-LSTM layer: Unlike previous architectures that only use [CLS] as the input for the classifier, in this architecture all outputs of the latest transformer encoder are used in such a way that they are given as inputs to a bidirectional recurrent neural network (Bi-LSTM) as shown in Figure FIGREF8. After processing the input, the network sends the final hidden state to a fully connected network that performs classification using the softmax activation function.","4. Insert CNN layer: In this architecture shown in Figure FIGREF8, the outputs of all transformer encoders are used instead of using the output of the latest transformer encoder. So that the output vectors of each transformer encoder are concatenated, and a matrix is produced. The convolutional operation is performed with a window of size (3, hidden size of BERT which is 768 in BERTbase model) and the maximum value is generated for each transformer encoder by applying max pooling on the convolution output. By concatenating these values, a vector is generated which is given as input to a fully connected network. By applying softmax on the input, the classification operation is performed.",Table 2: Results on the trial data using pre-trained BERT model with different finetuning strategies and comparison with results in the literature.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What are the existing biases?,Sample Answer,1910.12574-Introduction-1,1910.12574-Introduction-4,1910.12574-Methodology ::: Fine-Tuning Strategies-1,1910.12574-Methodology ::: Fine-Tuning Strategies-3,1910.12574-Methodology ::: Fine-Tuning Strategies-4,"People are increasingly using social networking platforms such as Twitter, Facebook, YouTube, etc. to communicate their opinions and share information. Although the interactions among users on these platforms can lead to constructive conversations, they have been increasingly exploited for the propagation of abusive language and the organization of hate-based activities BIBREF0, BIBREF1, especially due to the mobility and anonymous environment of these online platforms. Violence attributed to online hate speech has increased worldwide. For example, in the UK, there has been a significant increase in hate speech towards the immigrant and Muslim communities following the UK's leaving the EU and the Manchester and London attacks. The US also has been a marked increase in hate speech and related crime following the Trump election. Therefore, governments and social network platforms confronting the trend must have tools to detect aggressive behavior in general, and hate speech in particular, as these forms of online aggression not only poison the social climate of the online communities that experience it, but can also provoke physical violence and serious harm BIBREF1.","Here, we propose a transfer learning approach for hate speech understanding using a combination of the unsupervised pre-trained model BERT BIBREF11 and some new supervised fine-tuning strategies. As far as we know, it is the first time that such exhaustive fine-tuning strategies are proposed along with a generative pre-trained language model to transfer learning to low-resource hate speech languages and improve performance of the task. In summary:","1. BERT based fine-tuning: In the first approach, which is shown in Figure FIGREF8, very few changes are applied to the BERTbase. In this architecture, only the [CLS] token output provided by BERT is used. The [CLS] output, which is equivalent to the [CLS] token output of the 12th transformer encoder, a vector of size 768, is given as input to a fully connected network without hidden layer. The softmax activation function is applied to the hidden layer to classify.","3. Insert Bi-LSTM layer: Unlike previous architectures that only use [CLS] as the input for the classifier, in this architecture all outputs of the latest transformer encoder are used in such a way that they are given as inputs to a bidirectional recurrent neural network (Bi-LSTM) as shown in Figure FIGREF8. After processing the input, the network sends the final hidden state to a fully connected network that performs classification using the softmax activation function.","4. Insert CNN layer: In this architecture shown in Figure FIGREF8, the outputs of all transformer encoders are used instead of using the output of the latest transformer encoder. So that the output vectors of each transformer encoder are concatenated, and a matrix is produced. The convolutional operation is performed with a window of size (3, hidden size of BERT which is 768 in BERTbase model) and the maximum value is generated for each transformer encoder by applying max pooling on the convolution output. By concatenating these values, a vector is generated which is given as input to a fully connected network. By applying softmax on the input, the classification operation is performed.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What biases does their model capture?,Sample Answer,1910.12574-Introduction-0,1910.12574-Introduction-1,1910.12574-Previous Works-0,1910.12574-Previous Works-3,1910.12574-Conclusion-1,,"People are increasingly using social networking platforms such as Twitter, Facebook, YouTube, etc. to communicate their opinions and share information. Although the interactions among users on these platforms can lead to constructive conversations, they have been increasingly exploited for the propagation of abusive language and the organization of hate-based activities BIBREF0, BIBREF1, especially due to the mobility and anonymous environment of these online platforms. Violence attributed to online hate speech has increased worldwide. For example, in the UK, there has been a significant increase in hate speech towards the immigrant and Muslim communities following the UK's leaving the EU and the Manchester and London attacks. The US also has been a marked increase in hate speech and related crime following the Trump election. Therefore, governments and social network platforms confronting the trend must have tools to detect aggressive behavior in general, and hate speech in particular, as these forms of online aggression not only poison the social climate of the online communities that experience it, but can also provoke physical violence and serious harm BIBREF1.","Here, the existing body of knowledge on online hate speech and offensive language and transfer learning is presented.","Transfer Learning: Pre-trained vector representations of words, embeddings, extracted from vast amounts of text data have been encountered in almost every language-based tasks with promising results. Two of the most frequently used context-independent neural embeddings are word2vec and Glove extracted from shallow neural networks. The year 2018 has been an inflection point for different NLP tasks thanks to remarkable breakthroughs: Universal Language Model Fine-Tuning (ULMFiT) BIBREF20, Embedding from Language Models (ELMO) BIBREF21, OpenAI’ s Generative Pre-trained Transformer (GPT) BIBREF22, and Google’s BERT model BIBREF11. Howard et al. BIBREF20 proposed ULMFiT which can be applied to any NLP task by pre-training a universal language model on a general-domain corpus and then fine-tuning the model on target task data using discriminative fine-tuning. Peters et al. BIBREF21 used a bi-directional LSTM trained on a specific task to present context-sensitive representations of words in word embeddings by looking at the entire sentence. Radford et al. BIBREF22 and Devlin et al. BIBREF11 generated two transformer-based language models, OpenAI GPT and BERT respectively. OpenAI GPT BIBREF22 is an unidirectional language model while BERT BIBREF11 is the first deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus. BERT has two novel prediction tasks: Masked LM and Next Sentence Prediction. The pre-trained BERT model significantly outperformed ELMo and OpenAI GPT in a series of downstream tasks in NLP BIBREF11. Identifying hate speech and offensive language is a complicated task due to the lack of undisputed labelled data BIBREF15 and the inability of surface features to capture the subtle semantics in text. To address this issue, we use the pre-trained language model BERT for hate speech classification and try to fine-tune specific task by leveraging information from different transformer encoders.",,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
Which are the sequence model architectures this method can be transferred across?,Sample Answer,1908.05969-Introduction-1,1908.05969-Introduction-4,1908.05969-Proposed Method-14,1908.05969-Experiments ::: Development Experiments-2,1908.05969-Experiments ::: Effectiveness Study ::: OntoNotes.-0,"Compared with NER in English, Chinese NER is more difficult since sentences in Chinese are not previously segmented. Thus, one common practice in Chinese NER is first performing word segmentation using an existing CWS system and then applying a word-level sequence labeling model to the segmented sentence BIBREF5, BIBREF6. However, it is inevitable that the CWS system will wrongly segment the query sequence. This will, in turn, result in entity boundary detection errors and even entity category prediction errors in the following NER. Take the character sequence “南京市 (Nanjing) / 长江大桥 (Yangtze River Bridge)"" as an example, where “/"" indicates the gold segmentation result. If the sequence is segmented into “南京 (Nanjing) / 市长 (mayor) / 江大桥 (Daqiao Jiang)"", the word-based NER system is definitely not able to correctly recognize “南京市 (Nanjing)"" and “长江大桥 (Yangtze River Bridge)"" as two entities of the location type. Instead, it is possible to incorrectly treat “南京 (Nanjing)"" as a location entity and predict “江大桥 (Daqiao Jiang)"" to be a person's name. Therefore, some works resort to performing Chinese NER directly on the character level, and it has been shown that this practice can achieve better performance BIBREF7, BIBREF8, BIBREF9, BIBREF0.","In this work, we aim to find a easier way to achieve the idea of Lattice-LSTM, i.e., incorporating all matched words of the sentence to the character-based NER model. The first principle of our method design is to achieve a fast inference speed. To this end, we propose to encoding the matched words, obtained from the lexicon, into the representations of characters. Compared with Lattice-LSTM, this method is more concise and easier to implement. It can avoid complicated model architecture design thus has much faster inference speed. It can also be quickly adapted to any appropriate neural architectures without redesign. Given an existing neural character-based NER model, we only have to modify its character representation layer to successfully introduce the word lexicon. In addition, experimental studies on four public Chinese NER datasets show that our method can even achieve better performance than Lattice-LSTM when applying the LSTM-CRF model. Our source code is published at https://github.com/v-mipeng/LexiconAugmentedNER.","Here, we perform weight normalization on all words of the four word sets to allow them compete with each other across sets.","Figure FIGREF32 shows the F1-score of our method against the number of training iterations when using character bigram or not. From the figure, we can see that additionally introducing character bigrams cannot bring considerable improvement to our method. A possible explanation of this phenomenon is that the introduced word information by our proposed method has covered the bichar information. Therefore, in the following experiments, we did not use bichar in our method.","Table TABREF37 shows results on OntoNotes, which has gold segmentation for both training and testing data. The methods of the “Gold seg"" and ""Auto seg"" group are word-based that build on the gold word segmentation results and the automatic segmentation results, respectively. The automatic segmentation results were generated by the segmenter trained on training data of OntoNotes. Methods of the ""No seg"" group are character-based. From the table, we can obtain several informative observations. First, by replacing the gold segmentation with the automatically generated segmentation, the F1-score of the Word-based (LSTM) + char + bichar model decreased from 75.77% to 71.70%. This shows the problem of the practice that treats the predicted word segmentation result as the true one for the word-based Chinese NER. Second, the Char-based (LSTM)+bichar+ExSoftword model achieved a 71.89% to 72.40% improvement over the Char-based (LSTM)+bichar+softword baseline on the F1-score. This indicates the feasibility of the naive extension of ExSoftword to softword. However, it still greatly underperformed Lattice-LSTM, showing its deficiency in utilizing word information. Finally, our proposed method, which is a further extension of Exsoftword, obtained a statistically significant improvement over Lattice-LSTM and even performed similarly to those word-based methods with gold segmentation, verifying its effectiveness on this data set.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
 What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?,Sample Answer,1908.05969-Introduction-0,1908.05969-Generic Character-based Neural Architecture for Chinese NER ::: Sequence Modeling Layer-0,1908.05969-Generic Character-based Neural Architecture for Chinese NER ::: Sequence Modeling Layer ::: LSTM-based-0,1908.05969-Proposed Method-3,1908.05969-Proposed Method-4,"Named Entity Recognition (NER) is concerned with identifying named entities, such as person, location, product, and organization names, in unstructured text. In languages where words are naturally separated (e.g., English), NER was conventionally formulated as a sequence labeling problem, and the state-of-the-art results have been achieved by those neural-network-based models BIBREF1, BIBREF2, BIBREF3, BIBREF4.","The sequence modeling layer models the dependency between characters built on vector representations of the characters. In this work, we explore the applicability of our method to three popular architectures of this layer: the LSTM-based, the CNN-based, and the transformer-based.","The bidirectional long-short term memory network (BiLSTM) is one of the most commonly used architectures for sequence modeling BIBREF10, BIBREF3, BIBREF11. It contains two LSTM BIBREF12 cells that model the sequence in the left-to-right (forward) and right-to-left (backward) directions with two distinct sets of parameters. Here, we precisely show the definition of the forward LSTM:","Here, $seg(c_j) \in \mathcal {Y}_{seg}$ denotes the segmentation label of the character $c_j$ predicted by the word segmentor, $\mathbf {e}^{seg}$ denotes the segmentation label embedding lookup table, and commonly $\mathcal {Y}_{seg}=\lbrace \text{B}, \text{M}, \text{E}, \text{S}\rbrace $ with B, M, E indicating that the character is the beginning, middle, and end of a word, respectively, and S indicating that the character itself forms a single-character word.","The first idea we come out based on the Softword technique is to construct a word segmenter using the lexicon and allow a character to have multiple segmentation labels. Take the sentence $s=\lbrace c_1, c_2, c_3, c_4, c_5\rbrace $ as an example. If both its sub-sequences $\lbrace c_1, c_2, c_3, c_4\rbrace $ and $\lbrace c_3, c_4\rbrace $ match a word of the lexicon, then the segmentation label sequence of $s$ using the lexicon is $segs(s)=\lbrace \lbrace \text{B}\rbrace , \lbrace \text{M}\rbrace , \lbrace \text{B}, \text{M}\rbrace , \lbrace \text{E}\rbrace , \lbrace \text{O}\rbrace \rbrace $. Here, $segs(s)_1=\lbrace \text{B}\rbrace $ indicates that there is at least one sub-sequence of $s$ matching a word of the lexicon and beginning with $c_1$, $segs(s)_3=\lbrace \text{B}, \text{M}\rbrace $ means that there is at least one sub-sequence of $s$ matching the lexicon and beginning with $c_3$ and there is also at least one lexicon matched sub-sequence in the middle of which $c_3$ occurs, and $segs(s)_5=\lbrace \text{O}\rbrace $ means that there is no sub-sequence of $s$ that matches the lexicon and contains $c_5$. The character representation is then obtained by:",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the challenge for other language except English,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How many categories of offensive language were there?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How big is their dataset?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What task do they evaluate on?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What were the sizes of the test sets?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
With how many languages do they experiment in the multilingual setup?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Which dataset do they use?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How is the intensity of the PTSD established?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How big is dataset for this challenge?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What neural machine translation models can learn in terms of transfer learning?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
On top of BERT does the RNN layer work better or the transformer layer?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How big are negative effects of proposed techniques on high-resource tasks?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
"Are this techniques used in training multilingual models, on what languages?",Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Why is improvement on OntoNotes significantly smaller compared to improvement on WNUT 2017?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
"How is ""complexity"" and ""confusability"" of entity mentions defined in this work?",Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
which neural embedding model works better?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is the degree of dimension reduction of the efficient aggregation method?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are the characteristics of the city dialect?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are the characteristics of the rural dialect?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
"What does the ""sensitivity"" quantity denote?",Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What end tasks do they evaluate on?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is a semicharacter architecture?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Why is the adversarial setting appropriate for misspelling recognition?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How do the backoff strategies work?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is the difference in size compare to the previous model?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Do they evaluate their model on datasets other than RACE?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is their model's performance on RACE?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is the model architecture used?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How is the data used for training annotated?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Do they reduce language variation of text by enhancing frequencies?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How many hand-crafted templates did they have to make?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What were their distribution results?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How did they determine fake news tweets?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is their definition of tweets going viral?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are the characteristics of the accounts that spread fake news?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How is the ground truth for fake news established?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are the baselines for this paper?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What VQA datasets are used for evaluating this task? ,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How do they model external knowledge? ,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What type of external knowledge has been used for this paper? ,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What previous methods is the proposed method compared against?,Sample Answer,2003.06044-Introduction-0,2003.06044-Introduction-2,2003.06044-Conclusions and Future Work-0,2003.06044-Conclusions and Future Work-1,2003.06044-8-Figure2-1.png,"Dialogue act (DA) characterizes the type of a speaker's intention in the course of producing an utterance and is approximately equivalent to the illocutionary act of BIBREF0 or the speech act of BIBREF1. The recognition of DA is essential for modeling and automatically detecting discourse structure, especially in developing a human-machine dialogue system. It is natural to predict the Answer acts following an utterance of type Question, and then match the Question utterance to each QA-pair in the knowledge base. The predicted DA can also guide the response generation process BIBREF2. For instance, system generates a Greeting type response to former Greeting type utterance. Moreover, DA is beneficial to other online dialogue strategies, such as conflict avoidance BIBREF3. In the offline system, DA also plays a significant role in summarizing and analyzing the collected utterances. For instance, recognizing DAs of a wholly online service record between customer and agent is beneficial to mine QA-pairs, which are selected and clustered then to expand the knowledge base. DA recognition is challenging due to the same utterance may have a different meaning in a different context. Table TABREF1 shows an example of some utterances together with their DAs from Switchboard dataset. In this example, utterance “Okay.” corresponds to two different DA labels within different semantic context.","However, previous approaches cannot make full use of the relative position relationship between utterances. It is natural that utterances in the local context always have strong dependencies in our daily dialog. In this paper, we propose a hierarchical model based on self-attention BIBREF11 and revise the attention distribution to focus on a local and contextual semantic information by a learnable Gaussian bias which represents the relative position information between utterances, inspired by BIBREF12. Further, to analyze the effect of dialog length quantitatively, we introduce a new dialog segmentation mechanism for the DA task and evaluate the performance of different dialogue length and context padding length under online and offline settings. Experiment and visualization show that our method can learn the local contextual dependency between utterances explicitly and achieve promising performance in two well-known datasets.","In the paper, we propose our hierarchical model with local contextual attention to the Dialogue Act Recognition task. Our model can explicitly capture the semantic dependencies between utterances inside the dialogue. To enhance our model with local contextual information, we revise the attention distribution by a learnable Gaussian bias to make it focus on the local neighbors. Based on our dialog segmentation mechanism, we find that local contextual attention reduces the noises through relative position information, which is essential for dialogue act recognition. And this segmentation mechanism can be applied under online and offline settings. Our model achieves promising performance in two well-known datasets, which shows that modeling local contextual information is crucial for dialogue act recognition.","There is a close relation between dialogue act recognition and discourse parsing BIBREF31. The most discourse parsing process is composed of two stages: structure construction and dependency labeling BIBREF32, BIBREF33. For future work, a promising direction is to apply our method to multi-task training with two stages jointly. Incorporating supervised information from dependency between utterances may enhance the self-attention and further improve the accuracy of dialogue act recognition.","Figure 2: Visualization of original attention and local contextual attention. Each colored grid represents the dependency score between two sentences. The deeper the color is, the higher the dependency score is.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How faster is training and decoding compared to former models?,Sample Answer,1911.08673-Introduction-0,1911.08673-The General Greedy Parsing-4,1911.08673-The General Greedy Parsing-5,1911.08673-Experiments ::: Main Results-1,1911.08673-Conclusion-0,"Dependency parsing predicts the existence and type of linguistic dependency relations between words (as shown in Figure FIGREF1), which is a critical step in accomplishing deep natural language processing. Dependency parsing has been well developed BIBREF0, BIBREF1, and it generally relies on two types of parsing models: transition-based models and graph-based models. The former BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF4 traditionally apply local and greedy transition-based algorithms, while the latter BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12 apply globally optimized graph-based algorithms.","$\bullet $ ATTACHLEFT($i$): attaches $p_{i+1}$ to $p_i$ , which results in an arc ($p_i$, $p_{i+1}$) headed by $p_i$, and removes $p_{i+1}$ from pending.","$\bullet $ ATTACHRIGHT($i$): attaches $p_i$ to $p_{i+1}$ , which results in an arc ($p_{i+1}$, $p_i$) headed by $p_{i+1}$, and removes $p_i$ from pending.","In order to explore the impact of the parsing order objective on the parsing performance, we replace the greedy inference with the traditional MST parsing algorithm (i.e., BIAF + parsing order objective), and the result is shown as “This work (MST)"", giving slight performance improvement compared to the greedy inference, which shows globally optimized decoding of graph model still takes its advantage. Besides, compared to the standard training objective for graph model based parser, the performance improvement is slight but still shows the proposed parsing order objective is indeed helpful.","This paper presents a new global greedy parser in which we enable greedy parsing inference compatible with the global arc scoring of graph-based parsing models instead of the local feature scoring of transitional parsing models. The proposed parser can perform projective parsing when only using two arc-building actions, and it also supports non-projective parsing when introducing two extra non-projective arc-building actions. Compared to graph-based and transition-based parsers, our parser achieves a better tradeoff between parsing accuracy and efficiency by taking advantages of both graph-based models' training methods and transition-based models' linear time decoding strategies. Experimental results on 28 treebanks show the effectiveness of our parser by achieving good performance on 27 treebanks, including the PTB and CTB benchmarks.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How do they split the dataset when training and evaluating their models?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How much improvement does their model yield over previous methods?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is improvement in accuracy for short Jokes in relation other types of jokes?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What was their result on Stance Sentiment Emotion Corpus?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What performance did they obtain on the SemEval dataset?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are the state-of-the-art systems?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is the difference in performance between the interpretable system (e.g. vectors and cosine distance) and LSTM with ELMo system?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
which datasets did they experiment with?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What useful information does attention capture?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
In what cases is attention different from alignment?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What metric is considered?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How are the sentence embeddings generated?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How was the audio data gathered?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is the GhostVLAD approach?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Which 7 Indian languages do they experiment with?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What other non-neural baselines do the authors compare to? ,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are simulated datasets collected?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
Does the algorithm improve on the state-of-the-art methods?,Sample Answer,1911.07555-Related Works-2,1911.07555-Related Works-4,1911.07555-Related Works-6,1911.07555-Methodology-2,1911.07555-Results and Analysis-4,"The recently published JW300 parallel corpus BIBREF2 covers over 300 languages with around 100 thousand parallel sentences per language pair on average. In South Africa, a multilingual corpus of academic texts produced by university students with different mother tongues is being developed BIBREF3. The WiLI-2018 benchmark dataset BIBREF4 for monolingual written natural language identification includes around 1000 paragraphs of 235 languages. A possibly useful link can also be made BIBREF5 between Native Language Identification (NLI) (determining the native language of the author of a text) and Language Variety Identification (LVI) (classification of different varieties of a single language) which opens up more datasets. The Leipzig Corpora Collection BIBREF6, the Universal Declaration of Human Rights and Tatoeba are also often used sources of data.","Existing NLP datasets, models and services BIBREF10 are available for South African languages. These include an LID algorithm BIBREF11 that uses a character level n-gram language model. Multiple papers have shown that 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15 and similar models work very well for doing LID. The DSL 2017 paper BIBREF1, for example, gives an overview of the solutions of all of the teams that competed on the shared task and the winning approach BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features. The winning approach for DSL 2015 used an ensemble naive Bayes classifier. The fasttext classifier BIBREF17 is perhaps one of the best known efficient 'shallow' text classifiers that have been used for LID .",Researchers have investigated deeper LID models like bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24. The latter is reported to achieve 95.12% in the DSL 2015 shared task. In these models text features can include character and word n-grams as well as informative character and word-level features learnt BIBREF25 from the training data. The neural methods seem to work well in tasks where more training data is available.,The lexicon based classifier is then used to predict the specific language within a language group. For the South African languages this is done for the Nguni and Sotho groups. If the lexicon prediction of the specific language has high confidence then its result is used as the final label else the naive Bayesian classifier's specific language prediction is used as the final result. The lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.,The execution performance of some of the LID implementations are shown in Table TABREF10. Results were generated on an early 2015 13-inch Retina MacBook Pro with a 2.9 GHz CPU (Turbo Boosted to 3.4 GHz) and 8GB RAM. The C++ implementation in BIBREF17 is the fastest. The implementation in BIBREF8 makes use of un-hashed feature representations which causes it to be slower than the proposed sklearn implementation. The execution performance of BIBREF23 might improve by a factor of five to ten when executed on a GPU.,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
How confident is the conclusion about Shakespeare vs Flectcher?,Sample Answer,1911.05652-Introduction-0,1911.05652-Attribution of Particular Scenes-4,1911.05652-Rolling attribution of the play-5,1911.05652-Conclusions-1,1911.05652-3-Table1-1.png,"In the first collection of William Shakespeare’s works published in 1623 (the so-called First Folio) a play appears entitled The Famous History of the Life of King Henry the Eight for the very first time. Nowadays it is widely recognized that along with Shakespeare, other authors were involved in the writing of this play, yet there are different opinions as to who these authors were and what the precise shares were of their authorial contributions. This article aims to contribute to the question of the play’s authorship using combined analysis of vocabulary and versification and modern machine learning techniques (as proposed in BIBREF0, BIBREF1).","Altogether there are thus 53 training samples for Shakespeare, 90 training samples for Fletcher and 46 training samples for Massinger. In order to estimate the accuracy of the model, cross-validation is performed in the following way:","For scenes 1.1 and 1.2 rhythmic types, words as well as the combined model indicate Shakespeare to be the author. All three sets of models indicate that the shift of authorship happened at the end of scene 1.2.","The rolling attribution method suggests that particular scenes are indeed mostly a work of a single author and that their contributions roughly correspond to what has been proposed by James Spedding BIBREF3. The main differences between our results and Spedding’s attribution are the ambivalent outputs of models for both scenes of act 4. However, it is worth noting that Spedding himself expressed some doubts about the authorship of these scenes. Other differences are rather marginal and usually support the modifications of Spedding’s original attribution, as proposed by Thomas Merriam BIBREF25, BIBREF26, BIBREF27.","Table 1: Selected attributions of Henry VIII. S denotes attribution of the scene to Shakespeare, F denotes Fletcher, N denotes unassigned, S* denotes Shakespeare with “mere Fletcherian interpolation”. Where the attribution gives precise division of the scene, the subscripted number indicates the last line of a given passage (Through Line Numbering as used in the Norton Facsimile of the First Folio).",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the state-of-the-art approach?,Sample Answer,1911.11161-Approach-0,1911.11161-Experiments ::: Data-0,1911.11161-Experiments ::: Metrics-0,1911.11161-Results ::: Qualitative Evaluation-4,1911.11161-Conclusion and Discussion-0,Consider the example show in Table TABREF1 that shows a snippet of the conversation between a speaker and a listener that is grounded in a situation representing a type of emotion. Our goal is to produce responses to conversation that are emotionally appropriate to the situation and emotion portrayed.,"In our experiments we use the Empathetic Dialogues dataset made available by Rashkin et al. BIBREF3. Empathetic dialogues is crowdsourced dataset that contains dialogue grounded in a emotional situation. The dataset comprises of 32 emotion labels including surprised, excited, angry, proud, grateful. The speaker initiates the conversation using the grounded emotional situation and the listener responds in an appropriate manner.Table TABREF4 provides the basic statistics of the corpus.","Evaluating the quality of responses in open domain situations where the goal is not defined is an important area of research. Researchers have used methods such as BLEU , METEOR BIBREF17, ROUGE BIBREF18 from machine translation and text summarization BIBREF19 tasks. BLEU and METEOR are based on word overlap between the proposed and ground truth responses; they do not adequately account for the diversity of responses that are possible for a given input utterance and show little to no correlation with human judgments BIBREF19. We report on the BLEU BIBREF20 and Perplexity (PPL) metric to provide a comparison with the current state-of-the-art methods. We also report our performance using other metrics such as length of responses produced by the model. Following, Mei et al BIBREF21, we also report the diversity metric that helps us measure the ability of the model to promote diversity in responses BIBREF22. Diversity is calculated as the as the number of distinct unigrams in the generation scaled by the total number of generated tokens BIBREF21, BIBREF1. We report on two additional automated metrics of readability and coherence. Readability quantifies the linguistic quality of text and the difficulty of the reader in understanding the text BIBREF23. We measure readability through the Flesch Reading Ease (FRE) BIBREF24 which computes the number of words, syllables and sentences in the text. Higher readability scores indicate that utterance is easier to read and comprehend. Similarly, coherence measures the ability of the dialogue system to produce responses consistent with the topic of conversation. To calculate coherence, we use the method proposed by Dziri et al. BIBREF25.","Table TABREF15 shows the results obtained from the human evaluation comparing the performance of our fine-tuned, emotion pre-pend model to the ground-truth response. We find that our fine-tuned model outperforms the emo-prepend on all three metrics from the ratings provided by the human ratings.","In this work, we study how pre-trained language models can be adopted for conditional language generation on smaller datasets. Specifically, we look at conditioning the pre-trained model on the emotion of the situation produce more affective responses that are appropriate for a particular situation. We notice that our fine-tuned and emo-prepend models outperform the current state of the art approach relative to the automated metrics such as BLEU and perplexity on the validation set. We also notice that the emo-prepend approach does not out perform a simple fine tuning approach on the dataset. We plan to investigate the cause of this in future work from the perspective of better experiment design for evaluation BIBREF34 and analyzing the models focus when emotion is prepended to the sequence BIBREF35. Along with this, we also notice other drawbacks in our work such as not having an emotional classifier to predict the outcome of the generated sentence, which we plan to address in future work.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
What is the seed lexicon?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What are the results?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How are relations used to propagate polarity?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How big is the Japanese data?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How does their model learn using mostly raw data?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How big is seed lexicon used for training?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
What is the performance improvement of their method over state-of-the-art models on the used datasets? ,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How does the proposed training framework mitigate the bias pattern?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
How does this model overcome the assumption that all words in a document are generated from a single event?,Sample Answer,sample_id,,,,,Sample Passage,,,,,1.0,,,,,0.0,0.0,0.0
For which languages do they build word embeddings for?,Sample Answer,1805.03710-Subword LexVec-4,1805.03710-Subword LexVec-5,1805.03710-Subword LexVec-6,1805.03710-Materials-6,1805.03710-3-Table1-1.png,"With the PPMI matrix calculated, the sliding window process is repeated and the following loss functions are minimized for every observed $(w,c)$ pair and target word $w$ : ","$$L_{wc} &= \frac{1}{2} (u_w^\top v_c - PPMI_{wc})^2 \\

L_{w} &= \frac{1}{2} \sum \limits _{i=1}^k{\mathbf {E}_{c_i \sim P_n(c)} (u_w^\top v_{c_i} - PPMI_{wc_i})^2 }$$   (Eq. 4) "," where $u_w$ and $v_c$ are $d$ -dimensional word and context vectors. The second loss function describes how, for each target word, $k$ negative samples BIBREF2 are drawn from the smoothed context unigram distribution.","We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings.","Table 1: Word similarity (Spearman’s rho), analogy (% accuracy), and downstream task (% accuracy) results. In downstream tasks, for the same model accuracy varies over different runs, so we report the mean over 20 runs, in which the only significantly (p < .05 under a random permutation test) different result is in chunking.",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0
